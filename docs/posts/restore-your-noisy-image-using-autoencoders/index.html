<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Restore your Noisy Image Using Autoencoders | fahrizain</title>
<meta name=keywords content="deep learning,image reconstruction,autoencoder"><meta name=description content="Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration."><meta name=author content="Affandy Fahrizain"><link rel=canonical href=https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/><link crossorigin=anonymous href=/assets/css/stylesheet.f0568d4df87da526a07cdd5f492b4a146e3fa93d5ee950200eaafb2bb50d6fd8.css integrity="sha256-8FaNTfh9pSagfN1fSStKFG4/qT1e6VAgDqr7K7UNb9g=" rel="preload stylesheet" as=style><link rel=icon href=https://fhrzn.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fhrzn.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fhrzn.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fhrzn.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fhrzn.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:title" content="Restore your Noisy Image Using Autoencoders"><meta property="og:description" content="Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration."><meta property="og:type" content="article"><meta property="og:url" content="https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/"><meta property="og:image" content="https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-07T19:19:52+07:00"><meta property="article:modified_time" content="2024-01-07T19:19:52+07:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg"><meta name=twitter:title content="Restore your Noisy Image Using Autoencoders"><meta name=twitter:description content="Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration."><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fhrzn.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Restore your Noisy Image Using Autoencoders","item":"https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Restore your Noisy Image Using Autoencoders","name":"Restore your Noisy Image Using Autoencoders","description":"Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration.","keywords":["deep learning","image reconstruction","autoencoder"],"articleBody":" This is the next series of my notes while exploring autoencoders. You may also interested to read my other notes on Autoencoder network:\n1. Autoencoders: Your First Step into Generative AI\n2. Restore your Noisy Image using Autoencoders\nConvolutional Autoencoder (CAE) In the previous article, we implemented the image compression model using Linear Autoencoder. However, when it comes to larger image and having more colors (RGB, not grayscale or just black and white) it is a good idea to try incorporating Convolutional layer instead of simple Linear layer. You may do your mini research by yourself, but this time let’s just modify our network using Convolutional layer.\nBefore we deep dive to code, let’s cover some basic theory so we can completely understand what is happening in our model.\nConvolutional Network ","wordCount":"2905","inLanguage":"en","image":"https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg","datePublished":"2024-01-07T19:19:52+07:00","dateModified":"2024-01-07T19:19:52+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/"},"publisher":{"@type":"Organization","name":"fahrizain","logo":{"@type":"ImageObject","url":"https://fhrzn.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fhrzn.github.io/ accesskey=h title="fahrizain (Alt + H)"><img src=https://fhrzn.github.io/favicon-32x32.png alt aria-label=logo height=30>fahrizain</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Restore your Noisy Image Using Autoencoders</h1><div class=post-description>Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration.</div><div class=post-meta><span title='2024-01-07 19:19:52 +0700 WIB'>January 7, 2024</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2905 words&nbsp;·&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager srcset="https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_72515_360x0_resize_q75_box.jpg 360w ,https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_72515_480x0_resize_q75_box.jpg 480w ,https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg 640w" sizes="(min-width: 768px) 720px, 100vw" src=https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg alt="Cover Post" width=640 height=427><p>Photo by <a href=https://unsplash.com/@perotto>Alexandre Perotto</a> on <a href=https://unsplash.com/photos/low-angle-photography-of-building-zCevd81eJDU>Unsplash</a></p></figure><div class=post-content><blockquote><p><em>This is the next series of my notes while exploring autoencoders.</em>
<em>You may also interested to read my other notes on Autoencoder network:</em></p><p><em>1. <a href=https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/>Autoencoders: Your First Step into Generative AI</a></em></p><p><em>2. <strong>Restore your Noisy Image using Autoencoders</strong></em></p></blockquote><h2 id=convolutional-autoencoder-cae>Convolutional Autoencoder (CAE)<a hidden class=anchor aria-hidden=true href=#convolutional-autoencoder-cae>#</a></h2><p>In the previous article, we implemented the image compression model using Linear Autoencoder. However, when it comes to larger image and having more colors (RGB, not grayscale or just black and white) it is a good idea to try incorporating Convolutional layer instead of simple Linear layer. You may do your mini research by yourself, but this time let’s just modify our network using Convolutional layer.</p><p>Before we deep dive to code, let’s cover some basic theory so we can completely understand what is happening in our model.</p><h3 id=convolutional-network>Convolutional Network<a hidden class=anchor aria-hidden=true href=#convolutional-network>#</a></h3><p><img loading=lazy src=images/cnn.jpeg#center alt='Convolutional Neural Network diagram. source: <a href="https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699">https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699</a>'>
source: <a href=https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699>https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699</a></p><p>Let’s do quick review how Convolutional Neural Network (CNN) works.</p><p><em>What will happen if we feed our original image with size 512x512 pixel into our model?</em>
Well, if our image is grayscale it would produce <code>512 * 512 = 262,144 data</code>. While if our image is RGB, it would produce <code>512 * 512 * 3 = 786,432 data</code>. And that’s a lot for our computer memory!</p><p>This is where the implementation of CNN is certainly needed.</p><p>Basically, CNN used a so-called kernel to “scan” each part of the image. This kernel is actually 2-dimensional matrix within size $M\times N$. It travels through whole image by moving within $S$ stride on each step. And this kernel is duplicated as much as image’s channels size, for example colorful image will have 3 channels for each Red Green Blue colors. Therefore, there will be 3 for such image.</p><p>For easier interpretation you may see the illustration below.</p><p><img loading=lazy src=images/cnn_kernel.gif#center alt='CNN Kernel. source: <a href="https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel">https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel</a>'>
source: <a href=https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel>https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel</a></p><p>The above step will produce a “projection layer” which contains the observation of each kernels as shown in the previous illustration. Then, from this projection layer we will try to reduce the pixel size by applying a Pooling layer.</p><p>There are two popular kind of Pooling layer, Max Pooling and Average Pooling. And you can guess by its name, the former one reduce the data dimension by taking the maximum on each observation space, while the latter perform it by taking the average.</p><p><img loading=lazy src=images/pooling_layers.png#center alt='Pooling layers. source: <a href="https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b">https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b</a>'></p><p>source: <a href=https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b>https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b</a></p><p>Finally, the reduced data will be passed to the next layer and will be going through the same process every time it passed to CNN layers.</p><blockquote><p><em>Note that you may look for other references for further explanation of CNNs as we won’t cover it too much here.</em></p></blockquote><h3 id=what-cnns-actually-do>What CNN’s Actually do?<a hidden class=anchor aria-hidden=true href=#what-cnns-actually-do>#</a></h3><p>Overall, as in my understanding, I could say that CNN operation is trying to extract features and reduce the data dimension at the same time. By applying Max pooling or Average pooling, it expected to keeps the relevant information while reducing the dimension.</p><h2 id=cae-network-design>CAE Network Design<a hidden class=anchor aria-hidden=true href=#cae-network-design>#</a></h2><p>The network design for Convolutional based autoencoder was basically same with the Linear one. There are Encoder, Decoder, and Latent space. However, the key differences is located on how we implement Encoder and Decoder layers.</p><h3 id=encoder-layer>Encoder Layer<a hidden class=anchor aria-hidden=true href=#encoder-layer>#</a></h3><p>The Encoder layer design is not complicated as it is very similar to common CNN implementation for classification. The layers, of course, designed to shrink as it closer to the latent space, expecting it to extract key features of our images and map it to $C\times W\times H$ of latent space.</p><p>where:</p><p>$C:$ channels</p><p>$W:$ image width</p><p>$H:$ image height</p><p><img loading=lazy src=images/convolutional_encoder.png#center alt="convolutional encoder.png"></p><p>In the last layer of encoder output, you may choose to flatten it then feed to Linear layer or leave it as is for the latent space.</p><p><em>(P.S. if you have opinions or best practices, please don’t hesitate to leave it in comment section)</em></p><h3 id=decoder-layer>Decoder Layer<a hidden class=anchor aria-hidden=true href=#decoder-layer>#</a></h3><p>The Decoder layer design in general still the same to regular autoencoder network, which is need to expand over the time (the opposite shape of encoder layer). However, there are some adjustment to the convolutional layer for the decoder.</p><p>While convolutional layer is intended to reduce the data dimension size, we need to make it do the opposite for the implementation in Decoder layer. Instead of shrink the dimension, it needs to upsample it. Fortunately, in PyTorch we can achieve it easily with <code>Conv2dTranspose</code> layer.</p><p><img loading=lazy src=images/convtranspose.gif#center alt='Convolutional Transpose. source: <a href="https://stackoverflow.com/a/55119869">https://stackoverflow.com/a/55119869</a>'></p><p>source: <a href=https://stackoverflow.com/a/55119869>https://stackoverflow.com/a/55119869</a></p><p>In short, we can say it do the opposite operation of regular CNN layer. And we will use it to upsample our latent space back to its original size image.</p><p><img loading=lazy src=images/convolutional_decoder.png#center alt="convolutional decoder.png"></p><p>If you are looking for more explanation on this upsample operation, I recommend you to watch <a href="https://www.youtube.com/watch?v=345wRyqKkQ0&amp;list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&amp;index=139">Intro to Deep Learning and Generative Models Course</a> by <a href=https://www.youtube.com/@SebastianRaschka>Sebastian Raschka</a>.</p><h2 id=coding-time>Coding Time!<a hidden class=anchor aria-hidden=true href=#coding-time>#</a></h2><p>This time we will try to denoise flowers image using Convolutional Autoencoders. We will use 16 different <a href=https://www.kaggle.com/datasets/l3llff/flowers>Flowers dataset which available in Kaggle</a>.</p><p>If you wants to jump ahead into the notebook, <a href=https://www.kaggle.com/code/affand20/denoising-autoencoder/notebook>please visit this link</a>.</p><h3 id=dataset-preparation>Dataset Preparation<a hidden class=anchor aria-hidden=true href=#dataset-preparation>#</a></h3><p>Let’s import the necessary libraries and setup our base config.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># data manipulation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># utils</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> gzip
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> string
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tqdm.auto <span style=color:#f92672>import</span> tqdm
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># sklearn</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision.datasets <span style=color:#f92672>import</span> ImageFolder
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchvision.transforms <span style=color:#66d9ef>as</span> transforms
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader, Subset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>config</span>:
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>    device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cuda&#39;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;cpu&#39;</span>
</span></span><span style=display:flex><span>    epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>    learning_rate <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-3</span>
</span></span><span style=display:flex><span>    log_step <span style=color:#f92672>=</span> <span style=color:#ae81ff>50</span>
</span></span><span style=display:flex><span>    seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>
</span></span><span style=display:flex><span>    latent_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>    inp_out_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>784</span>
</span></span><span style=display:flex><span>    hidden_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span></code></pre></div><p>We wants to make sure that all images must be in size 256 x 256. Therefore, we can add a transformation operation to crop and resize the image.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># transform data</span>
</span></span><span style=display:flex><span>transform <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>RandomResizedCrop(<span style=color:#ae81ff>256</span>),    <span style=color:#75715e># crop and/or resize image to 256x256</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>ToTensor()
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><p>Now, since our dataset is only images stored in folders, we may load it to pytorch using ImageFolder utils from torchvision like this. Don’t forget to pass our previously defined transformation operations.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># load data from folder</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> ImageFolder(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/kaggle/input/flowers/flowers&#34;</span>, transform<span style=color:#f92672>=</span>transform)
</span></span><span style=display:flex><span>target <span style=color:#f92672>=</span> dataset<span style=color:#f92672>.</span>targets
</span></span></code></pre></div><p>Then, let’s make train–test split and setup our dataloaders.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># make train test split</span>
</span></span><span style=display:flex><span>train_idx, test_idx <span style=color:#f92672>=</span> train_test_split(np<span style=color:#f92672>.</span>arange(len(dataset)), test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, stratify<span style=color:#f92672>=</span>target, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>train_subset <span style=color:#f92672>=</span> Subset(dataset, indices<span style=color:#f92672>=</span>train_idx)
</span></span><span style=display:flex><span>test_subset <span style=color:#f92672>=</span> Subset(dataset, indices<span style=color:#f92672>=</span>test_idx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># make trainloader</span>
</span></span><span style=display:flex><span>trainloader <span style=color:#f92672>=</span> DataLoader(train_subset, batch_size<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>batch_size, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>testloader <span style=color:#f92672>=</span> DataLoader(test_subset, batch_size<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>batch_size, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><h3 id=designing-model-architecture>Designing Model Architecture<a hidden class=anchor aria-hidden=true href=#designing-model-architecture>#</a></h3><p>It’s time to implement our previous discussion on the convolutional autoencoder network!</p><p>For the encoder part, let’s stack 4 Convolutional layers with shrinking output to its end.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># encoder</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Here, I am trying to reduce the data dimension by using only <code>stride</code> parameter instead of employing the Max Pooling layer. You may try it by yourself to incorporate the Max Pooling layer and leave the <code>stride</code> parameter as 1.</p><blockquote><p>💡 <em>There is also discussion whether to use stride or max pooling layer for reducing data dimension. You can find it here: <a href=https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling>https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling</a></em></p></blockquote><p>During the process, I found difficulties to guess what is the output size of each convolution layers. Then I found the full explanation and guide from Stanford which I find very useful. Here is the link: <a href=https://cs231n.github.io/convolutional-networks/#layers>https://cs231n.github.io/convolutional-networks/#layers</a>.</p><p>Or simply, you can follow this formula which I obtained from the link above.</p><p>$$
(W−F+2P)/S+1
$$</p><p>where:</p><p>$W:$ input volume size</p><p>$F:$ kernel (filter) size</p><p>$P:$ padding size</p><p>$S:$ stride size</p><p>Now, for the decoder part, as the opposite of the encoder layer, let’s stack 4 Transpose Convolutional layers with expanding output to its end.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># decoder</span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>decoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),   <span style=color:#75715e># for the final layer I used kernel size 2, as I found it empirically match the original input image size</span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Sigmoid()
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Honestly, specific on this part I still had a difficulties to tell the reason why the <code>padding</code> need to set as 0 and the final <code>stride</code> to 1. What I did was actually following the tutorial <a href=https://github.com/udacity/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/Denoising_Autoencoder_Solution.ipynb>here</a> and <a href=https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L16/conv-autoencoder_mnist.ipynb>here</a> and do my observation through try and error by feeding random tensor in equal shape with the latent space to this network.</p><p>While to calculate the output of each Transpose Convolutional layer, I follow its formula defined on PyTorch page here: <a href=https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html>https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html</a>.</p><p>Finally, the forward function is defined as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>		x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(x)
</span></span><span style=display:flex><span>		x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decoder(x)
</span></span><span style=display:flex><span>		
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>The full code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># define our network</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ConvDenoiseAutoencoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(ConvDenoiseAutoencoder, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># encoder</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># here we downsample the image using only stride instead of maxpool.</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># you may try maxpool approach as it is only my experiment using stride.</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># see discussion here: https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># decoder</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>decoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),   <span style=color:#75715e># for the final layer I used kernel size 2, as I found it empirically match the original input image size</span>
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Sigmoid()
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decoder(x)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>Let’s define our model and see its architecture.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># define model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> ConvDenoiseAutoencoder()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># move to GPU device</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(config<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>print(model)
</span></span></code></pre></div><pre tabindex=0><code>ConvDenoiseAutoencoder(
  (encoder): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
  (decoder): Sequential(
    (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ReLU()
    (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2))
    (3): ReLU()
    (4): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(1, 1))
    (7): Sigmoid()
  )
)
</code></pre><h3 id=add-noise-to-images>Add Noise to Images<a hidden class=anchor aria-hidden=true href=#add-noise-to-images>#</a></h3><p>In order to make our model understand to clean the noise from images, we first need to introduce a random noise to the original images.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># get one batch from trainloader</span>
</span></span><span style=display:flex><span>features, _ <span style=color:#f92672>=</span> next(iter(trainloader))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># define random noise and how much its influence</span>
</span></span><span style=display:flex><span>noise_factor <span style=color:#f92672>=</span> <span style=color:#ae81ff>.5</span>
</span></span><span style=display:flex><span>noise <span style=color:#f92672>=</span> noise_factor <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#f92672>*</span>features<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>noisy_img <span style=color:#f92672>=</span> features <span style=color:#f92672>+</span> noise
</span></span><span style=display:flex><span><span style=color:#75715e># keep image colors stay in scale between 0 to 1</span>
</span></span><span style=display:flex><span>features <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>clamp(noisy_img, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>1.</span>)
</span></span></code></pre></div><p><img loading=lazy src=images/ori_vs_noisy.png#center alt="ori vs noisy.png"></p><p>Note that the denoising autoencoder learn the representation by measuring the distance between the generated image and the original one. The generated image here is the attempt output of denoising autoencoder network.</p><p>By minimizing the distance between noisy and original one, it means the model will also learn to remove the noise or at least reduce it.</p><h3 id=noise-probability>Noise Probability<a hidden class=anchor aria-hidden=true href=#noise-probability>#</a></h3><p>During training phase, I found its challenging for our model to learn image representation if the whole training data are corrupted. Therefore, as part of my experiment, I tried to add threshold probability to determine whether batch of images need to be corrupted or not.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># add noise to image</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>random() <span style=color:#f92672>&gt;</span> p:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> noise_factor:
</span></span><span style=display:flex><span>        noise <span style=color:#f92672>=</span> noise_factor <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#f92672>*</span>features<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>        noisy_img <span style=color:#f92672>=</span> features <span style=color:#f92672>+</span> noise
</span></span><span style=display:flex><span>        features <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>clamp(noisy_img, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>1.</span>)
</span></span></code></pre></div><p>By adding threshold probability, we let some non-corrupted dataset to be learned as well. And by doing so, we hope the model will be able to learn underlying images representation and have a little insight on difference between the corrupted and non-corrupted images.</p><h2 id=training-model>Training Model<a hidden class=anchor aria-hidden=true href=#training-model>#</a></h2><p>Let’s now define our training loop. Here we would like to wrap our code inside a function, so we can reproduce easily later and the code itself become cleaner.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(model, noise_factor: float <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>, p_threshold: float <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># loss and optimizer</span>
</span></span><span style=display:flex><span>    criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>learning_rate)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># loss logging</span>
</span></span><span style=display:flex><span>    history <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;train_loss&#39;</span>: []
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># progressbar</span>
</span></span><span style=display:flex><span>    num_train_steps <span style=color:#f92672>=</span> len(trainloader) <span style=color:#f92672>*</span> config<span style=color:#f92672>.</span>epochs
</span></span><span style=display:flex><span>    progressbar <span style=color:#f92672>=</span> tqdm(range(num_train_steps))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    epochtime <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(config<span style=color:#f92672>.</span>epochs):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        trainloss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        batchtime <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> idx, batch <span style=color:#f92672>in</span> enumerate(trainloader):
</span></span><span style=display:flex><span>            <span style=color:#75715e># unpack data</span>
</span></span><span style=display:flex><span>            features, _ <span style=color:#f92672>=</span> batch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>						<span style=color:#f92672>**</span><span style=color:#75715e># add noise to image</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> random<span style=color:#f92672>.</span>random() <span style=color:#f92672>&gt;</span> p:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> noise_factor:
</span></span><span style=display:flex><span>                    noise <span style=color:#f92672>=</span> noise_factor <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#f92672>*</span>features<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>                    noisy_img <span style=color:#f92672>=</span> features <span style=color:#f92672>+</span> noise
</span></span><span style=display:flex><span>                    features <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>clamp(noisy_img, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>1.</span>)<span style=color:#f92672>**</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            features <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>to(config<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># clear gradient</span>
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># forward pass</span>
</span></span><span style=display:flex><span>            output <span style=color:#f92672>=</span> model(features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># calculate loss</span>
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> criterion(output, features)
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># optimize</span>
</span></span><span style=display:flex><span>            optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e>#  update running training loss</span>
</span></span><span style=display:flex><span>            trainloss <span style=color:#f92672>+=</span> loss<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># update progressbar</span>
</span></span><span style=display:flex><span>            progressbar<span style=color:#f92672>.</span>update(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            progressbar<span style=color:#f92672>.</span>set_postfix_str(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># log step</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> idx <span style=color:#f92672>%</span> config<span style=color:#f92672>.</span>log_step <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;Epoch: </span><span style=color:#e6db74>%03d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%03d</span><span style=color:#e6db74> | Batch: </span><span style=color:#e6db74>%04d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%04d</span><span style=color:#e6db74> | Loss: </span><span style=color:#e6db74>%.4f</span><span style=color:#e6db74>&#34;</span> \
</span></span><span style=display:flex><span>                      <span style=color:#f92672>%</span> ((epoch<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>), config<span style=color:#f92672>.</span>epochs, idx, \
</span></span><span style=display:flex><span>                         len(trainloader), trainloss <span style=color:#f92672>/</span> (idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># log epoch</span>
</span></span><span style=display:flex><span>        history[<span style=color:#e6db74>&#39;train_loss&#39;</span>]<span style=color:#f92672>.</span>append(trainloss <span style=color:#f92672>/</span> len(trainloader))
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;***Epoch: </span><span style=color:#e6db74>%03d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%03d</span><span style=color:#e6db74> | Loss: </span><span style=color:#e6db74>%.3f</span><span style=color:#e6db74>&#34;</span> \
</span></span><span style=display:flex><span>              <span style=color:#f92672>%</span> ((epoch<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>), config<span style=color:#f92672>.</span>epochs, loss<span style=color:#f92672>.</span>item()))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># log time</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#39;Time elapsed: </span><span style=color:#e6db74>%.2f</span><span style=color:#e6db74> min&#39;</span> <span style=color:#f92672>%</span> ((time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> batchtime) <span style=color:#f92672>/</span> <span style=color:#ae81ff>60</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;Total Training Time: </span><span style=color:#e6db74>%.2f</span><span style=color:#e6db74> min&#39;</span> <span style=color:#f92672>%</span> ((time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> epochtime) <span style=color:#f92672>/</span> <span style=color:#ae81ff>60</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model, history
</span></span></code></pre></div><p>And the function call itself.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model, history <span style=color:#f92672>=</span> train(model, noise_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>0.15</span>, p_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span></code></pre></div><p>Here we are training our model with probability of data corruption 0.5 and the noise factor itself 0.15.</p><p>After training for a while, let’s now plot our train loss history.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>7</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(range(len(history[<span style=color:#e6db74>&#39;train_loss&#39;</span>])), history[<span style=color:#e6db74>&#39;train_loss&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Train Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epochs&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;MSE Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img loading=lazy src=images/stride_w_noise_proba_losses.png#center alt="stride w noise proba losses.png"></p><h2 id=testing-time>Testing time!<a hidden class=anchor aria-hidden=true href=#testing-time>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>testloss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>testtime <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>noise_factor <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.25</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> tqdm(testloader):
</span></span><span style=display:flex><span>    <span style=color:#75715e># unpack data</span>
</span></span><span style=display:flex><span>    test_feats, _ <span style=color:#f92672>=</span> batch
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#f92672>**</span><span style=color:#75715e># add noise to image</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> noise_factor:
</span></span><span style=display:flex><span>        noise <span style=color:#f92672>=</span> noise_factor <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#f92672>*</span>test_feats<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>        noisy_img <span style=color:#f92672>=</span> test_feats <span style=color:#f92672>+</span> noise
</span></span><span style=display:flex><span>        test_feats <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>clamp(noisy_img, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>1.</span>)<span style=color:#f92672>**</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    test_feats <span style=color:#f92672>=</span> test_feats<span style=color:#f92672>.</span>to(config<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    <span style=color:#75715e># forward pass</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        test_out <span style=color:#f92672>=</span> model(test_feats)
</span></span><span style=display:flex><span>    <span style=color:#75715e># compute loss</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> criterion(test_out, test_feats)
</span></span><span style=display:flex><span>    testloss <span style=color:#f92672>+=</span> loss<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Test Loss: </span><span style=color:#e6db74>%.4f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (testloss <span style=color:#f92672>/</span> len(testloader)))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Total Testing Time: </span><span style=color:#e6db74>%.2f</span><span style=color:#e6db74> min&#39;</span> <span style=color:#f92672>%</span> ((time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> testtime) <span style=color:#f92672>/</span> <span style=color:#ae81ff>60</span>))
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled.png#center alt=Untitled></p><h3 id=visualize-our-test-result>Visualize our Test Result<a hidden class=anchor aria-hidden=true href=#visualize-our-test-result>#</a></h3><p>Let’s take the first 5 original and reconstructed image from latest batch in test set.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>orig <span style=color:#f92672>=</span> test_feats[:<span style=color:#ae81ff>5</span>]<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()
</span></span><span style=display:flex><span>recon <span style=color:#f92672>=</span> test_out[:<span style=color:#ae81ff>5</span>]<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>5</span>, sharex<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, sharey<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>8</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> images, row <span style=color:#f92672>in</span> zip([orig, recon], axes):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> img, ax <span style=color:#f92672>in</span> zip(images, row):
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>imshow(img<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>get_xaxis()<span style=color:#f92672>.</span>set_visible(<span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>get_yaxis()<span style=color:#f92672>.</span>set_visible(<span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p><img loading=lazy src=images/stride_w_noise_p_proba_inference.png alt="stride w noise p proba inference.png"></p><p>Great! Based on the result, although the image not completely denoised, our model seems to be able reduce it a little bit. You may also try to inference your denoised image several time and see if the noise will be more reduced.</p><h2 id=further-exploration>Further Exploration<a hidden class=anchor aria-hidden=true href=#further-exploration>#</a></h2><h3 id=experiment-with-different-model-architecture>Experiment with Different Model Architecture<a hidden class=anchor aria-hidden=true href=#experiment-with-different-model-architecture>#</a></h3><p>To fulfill my curiosity, I tried incorporate the Max Pooling layer. So I changed the model architecture to be like this.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ConvDenoiseAutoencoderV3</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(ConvDenoiseAutoencoderV3, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># encoder</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># decoder</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>decoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),   <span style=color:#75715e># for the final layer I used kernel size 2, as I found it empirically match the original input image size</span>
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Sigmoid()
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decoder(x)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><pre tabindex=0><code>ConvDenoiseAutoencoderV3(
  (encoder): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): ReLU()
    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (decoder): Sequential(
    (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
    (1): ReLU()
    (2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))
    (3): ReLU()
    (4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    (5): ReLU()
    (6): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))
    (7): ReLU()
    (8): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): Sigmoid()
  )
)
</code></pre><h3 id=experimental-result>Experimental Result<a hidden class=anchor aria-hidden=true href=#experimental-result>#</a></h3><p>We keep the training setup same to our previous setup. The only difference is now we are setting noise probability to 0 and noise_factor to 0.5, ensuring that all images are quite badly corrupted.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model, history <span style=color:#f92672>=</span> train(model, noise_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, p_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>)
</span></span></code></pre></div><p>With that settings, here is our training loss looks like.</p><p><img loading=lazy src=images/maxpool_losses.png#center alt="maxpool losses.png"></p><p>And finally, this is our denoised images using such architecture.</p><p><img loading=lazy src=images/maxpool_inference.png#center alt="maxpool inference.png"></p><p>From the output we could tell that the model might be able to remove noise better than our previous model architecture. However, the generated result seems to be blurred.</p><p>Again, our model architecture is very simple compared to the latest image reconstruction architecture. Of course the performance and reconstruction quality is differs by large margin. You may try to improve the model architecture or implement published paper’s architecture.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>Now we know more advanced Autoencoder architecture using Convolutional layers. The key difference is how our image will be processed through each layer. It works by reducing the dimension of images and projected to specific size of dimension.</p><p>We also discovered how to corrupt image simply by creating random noise and add it to our data. By learning the distance between corrupted images and the original one, it is expected that our model is able to remove (or at least reduce) the noise from image, leaving better image quality.</p><p>If you have any inquiries, comments, suggestions, or critics please don’t hesitate to reach me out:</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul><p>Until next time! 👋</p><hr><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ol><li><a href="https://www.youtube.com/watch?v=345wRyqKkQ0&amp;list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&amp;index=139">Intro to Deep Learning and Generative Models Course</a> by <a href=https://www.youtube.com/@SebastianRaschka>Sebastian Raschka</a>.</li><li><a href=https://github.com/udacity/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/Denoising_Autoencoder_Solution.ipynb>Udacity Intro to Deep Learning with Pytorch Github repos</a></li><li><a href=https://cs231n.github.io/convolutional-networks/#layers>CS231n Convolutional Neural Networks for Visual Recognition</a> by Stanford</li><li><a href=https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling>Pooling vs Stride for downsampling</a></li><li><a href=https://www.v7labs.com/blog/autoencoders-guide>Autoencoders in Deep Learning</a> by V7</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://fhrzn.github.io/tags/autoencoder/>autoencoder</a></li><li><a href=https://fhrzn.github.io/tags/deeplearning/>deeplearning</a></li></ul><nav class=paginav><a class=prev href=https://fhrzn.github.io/posts/pain-free-python-fastapi-rmq-integration/><span class=title>« Prev</span><br><span>Pain-free Python Fastapi RabbitMQ Integration</span>
</a><a class=next href=https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/><span class=title>Next »</span><br><span>Autoencoders: Your First Step into Generative AI</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://fhrzn.github.io/>fahrizain</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>