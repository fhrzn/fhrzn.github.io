<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots | fahrizain</title><meta name=keywords content="chatbot,conversational ai,context aware,langchain,rag"><meta name=description content="We moved from in-house training model to hosted models and ready-to-use APIs. With the existence of free LLM APIs, let&rsquo;s explore how to create our own free chatbot!"><meta name=author content="Affandy Fahrizain"><link rel=canonical href=http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:url" content="http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/"><meta property="og:site_name" content="fahrizain"><meta property="og:title" content="Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots"><meta property="og:description" content="We moved from in-house training model to hosted models and ready-to-use APIs. With the existence of free LLM APIs, let‚Äôs explore how to create our own free chatbot!"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-01T20:00:00+07:00"><meta property="article:modified_time" content="2024-06-01T20:00:00+07:00"><meta property="article:tag" content="Chatbot"><meta property="article:tag" content="Conversational Ai"><meta property="article:tag" content="Context Aware"><meta property="article:tag" content="Langchain"><meta property="article:tag" content="Rag"><meta property="og:image" content="http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/cover.jpg"><meta name=twitter:title content="Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots"><meta name=twitter:description content="We moved from in-house training model to hosted models and ready-to-use APIs. With the existence of free LLM APIs, let&rsquo;s explore how to create our own free chatbot!"><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots","item":"http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots","name":"Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots","description":"We moved from in-house training model to hosted models and ready-to-use APIs. With the existence of free LLM APIs, let\u0026rsquo;s explore how to create our own free chatbot!","keywords":["chatbot","conversational ai","context aware","langchain","rag"],"articleBody":"Since the rise of LLMs era such as ChatGPT, Bard, Gemini, Claude, etc. the development of AI based application has been drastically changed. We moved from the conventional in-house training (which require high machine spec) to ready-to-use APIs.\nFortunately, there are free APIs curated on OpenRouter platform. So we can build our simple chatbot without spending a penny!\nLet‚Äôs get started! üî•\n‚ö°Ô∏è If you want to jump directly to the repository, you can access it here.\nProject Brief Before we start working, let‚Äôs take a look on high level concept below of how it will works. For those who are just started learning LLM, this brief information can help you to understand more how LLM-based application works.\nBasically, LLM works by taking user input and answer them based on its internal knowledge. If we want our LLM to do specific task such as brainstorming, making travel plan, calculating our expenses, or etc. we will need more advanced and structured user input. This is what we called Prompt.\nHere are very simple illustrations of the difference in user input structure when adding a prompt and not.\nHuman: Assistant: System Prompt: Human: Assistant: At this point, our LLM should be able to do specific and more advanced task. However, the LLM doesn‚Äôt remember the prior conversation and every time we invoke LLM call it assume that current user input as initial conversation with the user. To solve this problem, we need to add our previous conversation to the prompt. So that everytime we send user input, the LLM has knowledge of prior conversation which makes it remember previous conversation contexes. At the end of conversation, right after the LLM given its responses, we need to save both user input and LLM response.\nHere is the simple illustration of our system prompt which incorporating previous conversation.\nYou are professional travel planner. You are able to convert different timezone to the desired timezone quick and precisely. ... ... --------------------- Chat history: {chat_history} Pay attention that we are giving clear separation between system prompt (act as basic command) and previous chat history (as additional knowledge/context). And we put chat_history variable in curly braces that intended to be replaced with our retrieved previous conversation later. We will talk about it more in technical implementation.\nIn short, our chatbot will combine both user input and previous chat history in a prompt. Then, it will passed to LLM as the input. The LLM then responsible to generate answer based on given input. Finally, we save current conversation (user input and LLM response) as chat history which will be consumed again later.\nSetup OpenRouter API Key Mostly, each LLM has different APIs to the others. That makes switching between models become less straightforward. OpenRouter lift those problem for us by providing a unified interface which allow us to change between models easily with very minimal code changes.\nNow, to get started make sure you already created an account in https://openrouter.ai/. Then, go to Keys page and create your API Key. Remember to save it somewhere save as it won‚Äôt show twice. Start Coding! üî• Setup Environment In python, it is advised to create individual virtualenv to isolate our libraries. This can reduce the possibility of error due to conflict on library versions. We will use default python‚Äôs virtualenv to make one.\nRun the commands below on terminal\nmkdir simple-rag-openrouter \u0026\u0026 cd simple-rag-openrouter python -m venv venv source .venv/bin/activate After running the commands above, a new folder named venv should be appeared in our project directory. All of our installed library will be saved there.\nInstall Libraries Make sure you already activate the virtualenv. Then, run this command to install required libraries.\npip install langchain langchain_openai gradio langchain_community uuid httpx Create Project Environment Variables Now, create a new file named .env. We will store our openrouter api key and base url there. So we can ensure our published codes didn‚Äôt contains any confidential information.\nYou also may create the file using terminal\ntouch .env Then, open the .env file we just created and put our credentials there.\nOPENROUTER_BASE = \"https://openrouter.ai/api/v1\" OPENROUTER_API_KEY = \"\" To make our code reproducable by other person, let‚Äôs dump our installed python libraries to a file called requirements.txt.\npip freeze \u003e requirements.txt Building our Chatbot Interface There are a various options to build chat UI, but here we will use gradio‚Äôs ChatInterface which very handy to use.\nLet‚Äôs create a python file called main.py and put the codes below.\nimport gradio as gr from dotenv import load_dotenv load_dotenv() def predict_chat(): # TODO: we will put our LLM call here later pass with gr.Blocks(fill_height=True) as demo: chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) if __name__ == \"__main__\": demo.queue() demo.launch() To run our script, run the following command in terminal.\ngradio main.py And we can already see the chat interface provided by gradio. Now, since openrouter provide a lot of LLM models we can choose and switch between them every time we want to send message. Think of the LLM here as replacable module that we can change and set to any model we want. Let‚Äôs add all available openrouter models, but limited to the free version only as we dont want to spend any money. To get the full list of available models, we can perform API request to openrouter‚Äôs endpoint here https://openrouter.ai/api/v1/models. Then we can put the available models as dropdown options above the chat interface.\nFirst, create a new function to get all available free models.\nimport httpx def get_free_models(): res = httpx.get(\"https://openrouter.ai/api/v1/models\") if res: res = res.json() # filter only free models models = [item[\"id\"] for item in res[\"data\"] if \"free\" in item[\"id\"]] return sorted(models) Then, inside add dropdown component and populate the model names.\nwith gr.Blocks(fill_height=True) as demo: models = get_free_models() # get model names model_choice = gr.Dropdown( choices=models, # populate model names show_label=True, label=\"Model Choice\", interactive=True, value=models[0] ) Finally, add the newly added component as Chat Interface additional inputs.\nchat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) Our main.py file should be look like this.\nimport gradio as gr from dotenv import load_dotenv import httpx load_dotenv() def predict_chat(): # TODO: we will put our LLM call here later pass def get_free_models(): res = httpx.get(\"https://openrouter.ai/api/v1/models\") if res: res = res.json() # filter only free models models = [item[\"id\"] for item in res[\"data\"] if \"free\" in item[\"id\"]] return sorted(models) with gr.Blocks(fill_height=True) as demo: models = get_free_models() model_choice = gr.Dropdown( choices=models, # populate model names show_label=True, label=\"Model Choice\", interactive=True, value=models[0] ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) if __name__ == \"__main__\": demo.queue() demo.launch() The code above should add dropdown list with free models name available in OpenRouter. Now we have all necessary components for our UI. However, we still can‚Äôt play around with the LLMs as we haven‚Äôt put logic to handle user-chatbot interactions yet.\nLet‚Äôs add some logic there!\nAdd Chatbot Logic Remember that we‚Äôve created a function called predict_chat() earlier? Now, to make the code cleaner we will move it to a new file named chatbot.py. We will put everything related to chatbot logic there including manage the conversation history.\nWe will use the prompt below to give specific task to the LLM.\nSystem prompt:\nYou are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\nLet‚Äôs write some codes to file chatbot.py. Don‚Äôt forget to create one if you don‚Äôt have it yet.\nFirst, let‚Äôs create a prompt template to put our prompt and user input together.\nfrom langchain.prompts import ChatPromptTemplate def predict_chat(message: str, history: list, model_name: str): prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\"), (\"human\", \"{input}\") ]) Here, we are using ChatPromptTemplate as we want to format the prompt in the conversation style. There are only 3 roles available, system, human, and AI.\nThe predict_chat() function takes 3 input, namely message, history, and model_name. Both message and history is required by default as we used gradio‚Äôs ChatInterface. While the model_name came from the model names dropdown in the main.py file.\nüí° If you‚Äôre curious how in the world those dropdown input automatically required in the predict_chat function, it is because we put that component into additional_inputs in Chat Interface parameter.\nNext, let‚Äôs initiate our LLM instance. Then chain our prompt and LLM together.\nllm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"), openai_api_base=os.getenv(\"OPENROUTER_BASE\") ) # chain prompt and LLM instance using LCEL chain = prompt | llm Notice that we chain prompt and LLM together using pipe (|) symbol. Thanks to LangChain Expression Language (LCEL) we can write this handy shorthand.\nFinally, we invoke the chain in stream mode so we can see the progressive output while it generates the full response.\npartial_msg = \"\" # for chunk in history_runnable.stream({\"input\": message}, config={\"configurable\": {\"session_id\": user_id}, \"callbacks\": [ConsoleCallbackHandler()]}): for chunk in chain.stream({\"input\": message}): partial_msg = partial_msg + chunk.content yield partial_msg Our chatbot.py should be look like this now.\nfrom langchain.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI import os def predict_chat(message: str, history: list, model_name: str): prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\"), (\"human\", \"{input}\") ]) llm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"), openai_api_base=os.getenv(\"OPENROUTER_BASE\") ) chain = prompt | llm partial_msg = \"\" # for chunk in history_runnable.stream({\"input\": message}, config={\"configurable\": {\"session_id\": user_id}, \"callbacks\": [ConsoleCallbackHandler()]}): for chunk in chain.stream({\"input\": message}): partial_msg = partial_msg + chunk.content yield partial_msg Now, go back for a while to main.py and add this line on import section.\nfrom chatbot import predict_chat Cool! At this point, we can start playing around with our chatbot and it will respond to our chats!\nContext-Aware Response Generation Our chatbot can respond to our question already. However, it lack of previous conversation contexes. Take a look on the captured conversation below. The previous context was talking about travel plan, but when I tried to continue the conversation it gave me an answer that doesn‚Äôt have correlation with previous context. To work on this issue, we need to put the chat history to our prompt. Here we will use SQLite as our database to save the whole chat history. Since we will only have one database for all users, we need a session_id between each user conversation history to avoid retrieving wrong user‚Äôs conversation.\nWe will first add a hidden input in chat interface that generate a unique UUID which will act as our session_id. So, everytime we refresh the page, it will generate new session_id as well.\nimport uuid with gr.Blocks(fill_height=True) as demo: user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) Next, add the hidden input component as Chat Interface additional_inputs as well. So, now Chat Interface additional inputs should contains model_choice and user_ids. Otherwise, we cannot pass the value to the function predict_chat() behind.\nchat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) Our final main.py should be look like this.\nimport gradio as gr from dotenv import load_dotenv import httpx from chat import predict_chat import uuid load_dotenv() def get_free_models(): res = httpx.get(\"https://openrouter.ai/api/v1/models\") if res: res = res.json() models = [item[\"id\"] for item in res[\"data\"] if \"free\" in item[\"id\"]] return sorted(models) with gr.Blocks(fill_height=True) as demo: user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) models = get_free_models() model_choice = gr.Dropdown( choices=models, show_label=True, label=\"Model Choice\", interactive=True, value=models[0] ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) if __name__ == \"__main__\": demo.queue() demo.launch() Now, we‚Äôre done with main.py. Let‚Äôs move further to chatbot.py file.\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory def get_chat_history(session_id: str): chat_history = SQLChatMessageHistory( session_id=session_id, connection_string=\"sqlite:///memory.db\") return chat_history Then, we‚Äôll a new variable named history in our prompt using MessagesPlaceholder. The rest of the prompt attribute stays the same. Also, don‚Äôt forget to add user_id to our predict_chat function parameter\ndef predict_chat(message: str, history: list, model_name: str, user_id: str): prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\"), MessagesPlaceholder(\"history\"), (\"human\", \"{input}\") ]) Next, instead of we call invoke directly on chained prompt and LLM instance, we will use a new instance called RunnableWithMessageHistory.\nfrom langchain_core.runnables.history import RunnableWithMessageHistory history_runnable = RunnableWithMessageHistory( chain, get_session_history=get_chat_history, input_messages_key=\"input\", history_messages_key=\"history\" ) Remember that we should always save our current conversation to database so we can use it in future interaction with chatbot? Gratefully, all those logic already handled by RunnableWithMessageHistory so we don‚Äôt have to handle it by ourselves.\nNote that we also put input and history as input and history message key respectively. Keep in mind that this key should match with variables key that already defined in prompt.\nFinally, rather than calling stream from chain, we call it from history_runnable instead. So our streaming code will look like this.\npartial_msg = \"\" for chunk in history_runnable.stream({\"input\": message}, config={\"configurable\": {\"session_id\": user_id}}): partial_msg = partial_msg + chunk.content yield partial_msg Our final main.py should be look like this.\nfrom langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.runnables.history import RunnableWithMessageHistory from langchain_community.chat_message_histories import SQLChatMessageHistory def get_chat_history(session_id: str): chat_history = SQLChatMessageHistory(session_id=session_id, connection_string=\"sqlite:///memory.db\") return chat_history def predict_chat(message: str, history: list, model_name: str, user_id: str): prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\"), MessagesPlaceholder(\"history\"), (\"human\", \"{input}\") ]) llm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"), openai_api_base=os.getenv(\"OPENROUTER_BASE\") ) chain = prompt | llm history_runnable = RunnableWithMessageHistory( chain, get_session_history=get_chat_history, input_messages_key=\"input\", history_messages_key=\"history\" ) partial_msg = \"\" for chunk in history_runnable.stream({\"input\": message}, config={\"configurable\": {\"session_id\": user_id}}): partial_msg = partial_msg + chunk.content yield partial_msg That‚Äôs a wrap! Our chatbot now can understand previous conversation context. Super! ‚ö°Ô∏è Full Project https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter\nConclusion Throughout this article, we already covered how to build a context-aware chatbot ‚Äì a chatbot that understand previous conversation contexes.\nIn the upcoming article I will demonstrate how we can extend this chatbot to be able interact with external files as well such as financial reports, product catalogs, or even company profile website.\nStay tune! üëã\nReferences LangChain getting started Add message history (memory) LCEL (LangChain Expression Language) OpenRouter docs Let‚Äôs get Connected üôå If you have any inquiries, comments, suggestions, or critics please don‚Äôt hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","wordCount":"2441","inLanguage":"en","image":"http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/cover.jpg","datePublished":"2024-06-01T20:00:00+07:00","dateModified":"2024-06-01T20:00:00+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/"},"publisher":{"@type":"Organization","name":"fahrizain","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="fahrizain (Alt + H)"><img src=http://localhost:1313/favicon-32x32.png alt aria-label=logo height=30>fahrizain</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/profile/ title=Profile><span>Profile</span></a></li><li><a href=https://www.linkedin.com/in/fahrizainn/ title=LinkedIn><span>LinkedIn</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots</h1><div class=post-meta><span title='2024-06-01 20:00:00 +0700 WIB'>June 1, 2024</span>&nbsp;¬∑&nbsp;12 min&nbsp;¬∑&nbsp;2441 words&nbsp;¬∑&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager srcset='http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/cover_hu_36b01b963d7b2c9f.jpg 360w,http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/cover_hu_aa25bc84dbd5c0e6.jpg 480w,http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/cover.jpg 640w' src=http://localhost:1313/posts/building-conversational-ai-context-aware-chatbot/cover.jpg sizes="(min-width: 768px) 720px, 100vw" width=640 height=427 alt="Cover Post"><figcaption>Photo by <a href="https://unsplash.com/@ajonesyyyyy?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Aaron Jones</a> on <a href="https://unsplash.com/photos/person-riding-bicycle-on-road-during-daytime-i2MYu4AElsE?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a></figcaption></figure><div class=post-content><p>Since the rise of LLMs era such as ChatGPT, Bard, Gemini, Claude, etc. the development of AI based application has been drastically changed. We moved from the conventional in-house training (which require high machine spec) to ready-to-use APIs.</p><p>Fortunately, there are free APIs curated on OpenRouter platform. So we can build our simple chatbot without spending a penny!</p><p>Let&rsquo;s get started! üî•</p><blockquote><p>‚ö°Ô∏è If you want to jump directly to the repository, you can access it <a href=https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter/contextual-conversation>here</a>.</p></blockquote><h2 id=project-brief>Project Brief<a hidden class=anchor aria-hidden=true href=#project-brief>#</a></h2><p>Before we start working, let&rsquo;s take a look on high level concept below of how it will works.
<img alt="Conversational AI Architecture" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/convai_arch.png#center></p><p>For those who are just started learning LLM, this brief information can help you to understand more how LLM-based application works.</p><p>Basically, LLM works by taking user input and answer them based on its internal knowledge. If we want our LLM to do specific task such as brainstorming, making travel plan, calculating our expenses, or etc. we will need more advanced and structured user input. This is what we called <strong>Prompt</strong>.</p><p>Here are very simple illustrations of the difference in user input structure when adding a prompt and not.</p><pre tabindex=0><code>Human: &lt;user query/input here&gt;
Assistant: &lt;chatbot answer here&gt;
</code></pre><pre tabindex=0><code>System Prompt: &lt;system prompt here&gt;
Human: &lt;user query/input here&gt;
Assistant: &lt;chatbot answer here&gt;
</code></pre><p>At this point, our LLM should be able to do specific and more advanced task. However, the LLM doesn&rsquo;t remember the prior conversation and every time we invoke LLM call it assume that current user input as initial conversation with the user. To solve this problem, we need to add our previous conversation to the prompt. So that everytime we send user input, the LLM has knowledge of prior conversation which makes it <em>remember</em> previous conversation contexes. At the end of conversation, right after the LLM given its responses, we need to save both user input and LLM response.</p><p>Here is the simple illustration of our system prompt which incorporating previous conversation.</p><pre tabindex=0><code>You are professional travel planner. You are able to convert different timezone to the desired timezone quick and precisely.
...
...
---------------------
Chat history:
{chat_history}
</code></pre><p>Pay attention that we are giving clear separation between system prompt (act as basic command) and previous chat history (as additional knowledge/context). And we put <code>chat_history</code> variable in curly braces that intended to be replaced with our retrieved previous conversation later. We will talk about it more in technical implementation.</p><p>In short, our chatbot will combine both user input and previous chat history in a prompt. Then, it will passed to LLM as the input. The LLM then responsible to generate answer based on given input. Finally, we save current conversation (user input and LLM response) as chat history which will be consumed again later.</p><h2 id=setup-openrouter-api-key>Setup OpenRouter API Key<a hidden class=anchor aria-hidden=true href=#setup-openrouter-api-key>#</a></h2><p>Mostly, each LLM has different APIs to the others. That makes switching between models become less straightforward. OpenRouter lift those problem for us by providing a unified interface which allow us to change between models easily with very minimal code changes.</p><p>Now, to get started make sure you already created an account in <a href=https://openrouter.ai/>https://openrouter.ai/</a>. Then, go to <a href=https://openrouter.ai/keys>Keys page</a> and create your API Key. Remember to save it somewhere save as it won&rsquo;t show twice.
<img alt="OpenRouter API Key page" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/api_page.png#center></p><h2 id=start-coding->Start Coding! üî•<a hidden class=anchor aria-hidden=true href=#start-coding->#</a></h2><h3 id=setup-environment>Setup Environment<a hidden class=anchor aria-hidden=true href=#setup-environment>#</a></h3><p>In python, it is advised to create individual virtualenv to isolate our libraries. This can reduce the possibility of error due to conflict on library versions. We will use default python&rsquo;s <code>virtualenv</code> to make one.</p><p>Run the commands below on terminal</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir simple-rag-openrouter <span style=color:#f92672>&amp;&amp;</span> cd simple-rag-openrouter
</span></span><span style=display:flex><span>python -m venv venv
</span></span><span style=display:flex><span>source .venv/bin/activate
</span></span></code></pre></div><p>After running the commands above, a new folder named <code>venv</code> should be appeared in our project directory. All of our installed library will be saved there.</p><h3 id=install-libraries>Install Libraries<a hidden class=anchor aria-hidden=true href=#install-libraries>#</a></h3><p>Make sure you already activate the virtualenv. Then, run this command to install required libraries.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install langchain langchain_openai gradio langchain_community uuid httpx
</span></span></code></pre></div><h3 id=create-project-environment-variables>Create Project Environment Variables<a hidden class=anchor aria-hidden=true href=#create-project-environment-variables>#</a></h3><p>Now, create a new file named <code>.env</code>. We will store our openrouter api key and base url there. So we can ensure our published codes didn&rsquo;t contains any confidential information.</p><p>You also may create the file using terminal</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>touch .env
</span></span></code></pre></div><p>Then, open the <code>.env</code> file we just created and put our credentials there.</p><pre tabindex=0><code>OPENROUTER_BASE = &#34;https://openrouter.ai/api/v1&#34;
OPENROUTER_API_KEY = &#34;&lt;your-openrouter-api-key&gt;&#34;
</code></pre><p>To make our code reproducable by other person, let&rsquo;s dump our installed python libraries to a file called <code>requirements.txt</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip freeze &gt; requirements.txt
</span></span></code></pre></div><h3 id=building-our-chatbot-interface>Building our Chatbot Interface<a hidden class=anchor aria-hidden=true href=#building-our-chatbot-interface>#</a></h3><p>There are a various options to build chat UI, but here we will use gradio&rsquo;s <code>ChatInterface</code> which very handy to use.</p><p>Let&rsquo;s create a python file called <code>main.py</code> and put the codes below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> gradio <span style=color:#66d9ef>as</span> gr
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>load_dotenv()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>():
</span></span><span style=display:flex><span>    <span style=color:#75715e># TODO: we will put our LLM call here later</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Blocks(fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#66d9ef>as</span> demo:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat_window <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Chatbot(bubble_full_width<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, render<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>ChatInterface(
</span></span><span style=display:flex><span>        predict_chat,
</span></span><span style=display:flex><span>        chatbot<span style=color:#f92672>=</span>chat_window,
</span></span><span style=display:flex><span>        fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        retry_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        undo_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        clear_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>queue()
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>launch()
</span></span></code></pre></div><p>To run our script, run the following command in terminal.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gradio main.py
</span></span></code></pre></div><p>And we can already see the chat interface provided by gradio.
<img alt="Gradio default ChatInterface" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/gradio_1.png#center></p><p>Now, since openrouter provide a lot of LLM models we can choose and switch between them every time we want to send message. Think of the LLM here as replacable module that we can change and set to any model we want.
<img alt="Replacable LLM module" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/modular_llm.png#center></p><p>Let&rsquo;s add all available openrouter models, but limited to the free version only as we dont want to spend any money. To get the full list of available models, we can perform API request to openrouter&rsquo;s endpoint here <a href=https://openrouter.ai/api/v1/models>https://openrouter.ai/api/v1/models</a>. Then we can put the available models as dropdown options above the chat interface.</p><p>First, create a new function to get all available free models.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>import</span> httpx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_free_models</span>():
</span></span><span style=display:flex><span>    res <span style=color:#f92672>=</span> httpx<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;https://openrouter.ai/api/v1/models&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> res:
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>        <span style=color:#75715e># filter only free models</span>
</span></span><span style=display:flex><span>        models <span style=color:#f92672>=</span> [item[<span style=color:#e6db74>&#34;id&#34;</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> res[<span style=color:#e6db74>&#34;data&#34;</span>] <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;free&#34;</span> <span style=color:#f92672>in</span> item[<span style=color:#e6db74>&#34;id&#34;</span>]]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> sorted(models)
</span></span></code></pre></div><p>Then, inside add dropdown component and populate the model names.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Blocks(fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#66d9ef>as</span> demo:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    models <span style=color:#f92672>=</span> get_free_models()      <span style=color:#75715e># get model names</span>
</span></span><span style=display:flex><span>    model_choice <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Dropdown(
</span></span><span style=display:flex><span>        choices<span style=color:#f92672>=</span>models,             <span style=color:#75715e># populate model names</span>
</span></span><span style=display:flex><span>        show_label<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Model Choice&#34;</span>,
</span></span><span style=display:flex><span>        interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        value<span style=color:#f92672>=</span>models[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Finally, add the newly added component as Chat Interface additional inputs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span>chat <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>ChatInterface(
</span></span><span style=display:flex><span>    predict_chat,
</span></span><span style=display:flex><span>    chatbot<span style=color:#f92672>=</span>chat_window,
</span></span><span style=display:flex><span>    additional_inputs<span style=color:#f92672>=</span>[model_choice],
</span></span><span style=display:flex><span>    fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    retry_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>    undo_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>    clear_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Our <code>main.py</code> file should be look like this.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>import</span> gradio <span style=color:#66d9ef>as</span> gr
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> httpx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>load_dotenv()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>():
</span></span><span style=display:flex><span>    <span style=color:#75715e># TODO: we will put our LLM call here later</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_free_models</span>():
</span></span><span style=display:flex><span>    res <span style=color:#f92672>=</span> httpx<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;https://openrouter.ai/api/v1/models&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> res:
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>        <span style=color:#75715e># filter only free models</span>
</span></span><span style=display:flex><span>        models <span style=color:#f92672>=</span> [item[<span style=color:#e6db74>&#34;id&#34;</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> res[<span style=color:#e6db74>&#34;data&#34;</span>] <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;free&#34;</span> <span style=color:#f92672>in</span> item[<span style=color:#e6db74>&#34;id&#34;</span>]]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> sorted(models)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Blocks(fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#66d9ef>as</span> demo:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    models <span style=color:#f92672>=</span> get_free_models()
</span></span><span style=display:flex><span>    model_choice <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Dropdown(
</span></span><span style=display:flex><span>        choices<span style=color:#f92672>=</span>models,             <span style=color:#75715e># populate model names</span>
</span></span><span style=display:flex><span>        show_label<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Model Choice&#34;</span>,
</span></span><span style=display:flex><span>        interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        value<span style=color:#f92672>=</span>models[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat_window <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Chatbot(bubble_full_width<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, render<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>ChatInterface(
</span></span><span style=display:flex><span>        predict_chat,
</span></span><span style=display:flex><span>        chatbot<span style=color:#f92672>=</span>chat_window,
</span></span><span style=display:flex><span>        additional_inputs<span style=color:#f92672>=</span>[model_choice],
</span></span><span style=display:flex><span>        fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        retry_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        undo_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        clear_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>queue()
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>launch()
</span></span></code></pre></div><p>The code above should add dropdown list with free models name available in OpenRouter.
<img alt="Chat UI with model choices" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/chatui_w_choice.png#center></p><p>Now we have all necessary components for our UI. However, we still can&rsquo;t play around with the LLMs as we haven&rsquo;t put logic to handle user-chatbot interactions yet.</p><p>Let&rsquo;s add some logic there!</p><h3 id=add-chatbot-logic>Add Chatbot Logic<a hidden class=anchor aria-hidden=true href=#add-chatbot-logic>#</a></h3><p>Remember that we&rsquo;ve created a function called <code>predict_chat()</code> earlier? Now, to make the code cleaner we will move it to a new file named <code>chatbot.py</code>. We will put everything related to chatbot logic there including manage the conversation history.</p><p>We will use the prompt below to give specific task to the LLM.</p><blockquote><p>System prompt:</p><p><em>You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.</em></p></blockquote><p>Let&rsquo;s write some codes to file <code>chatbot.py</code>. Don&rsquo;t forget to create one if you don&rsquo;t have it yet.</p><p>First, let&rsquo;s create a prompt template to put our prompt and user input together.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>(message: str, history: list, model_name: str):
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_messages([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;human&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{input}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span></code></pre></div><p>Here, we are using <code>ChatPromptTemplate</code> as we want to format the prompt in the conversation style. There are only 3 roles available, system, human, and AI.</p><p>The <code>predict_chat()</code> function takes 3 input, namely message, history, and model_name. Both message and history is required by default as we used gradio&rsquo;s <code>ChatInterface</code>. While the model_name came from the model names dropdown in the <code>main.py</code> file.</p><blockquote><p>üí° If you&rsquo;re curious how in the world those dropdown input automatically required in the <code>predict_chat</code> function, it is because we put that component into <code>additional_inputs</code> in Chat Interface parameter.</p></blockquote><p>Next, let&rsquo;s initiate our LLM instance. Then chain our prompt and LLM together.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span>llm <span style=color:#f92672>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model_name,
</span></span><span style=display:flex><span>    openai_api_key<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_API_KEY&#34;</span>),
</span></span><span style=display:flex><span>    openai_api_base<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_BASE&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># chain prompt and LLM instance using LCEL</span>
</span></span><span style=display:flex><span>chain <span style=color:#f92672>=</span> prompt <span style=color:#f92672>|</span> llm
</span></span></code></pre></div><p>Notice that we chain prompt and LLM together using pipe (<code>|</code>) symbol. Thanks to LangChain Expression Language (LCEL) we can write this handy shorthand.</p><p>Finally, we invoke the chain in stream mode so we can see the progressive output while it generates the full response.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span>partial_msg <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># for chunk in history_runnable.stream({&#34;input&#34;: message}, config={&#34;configurable&#34;: {&#34;session_id&#34;: user_id}, &#34;callbacks&#34;: [ConsoleCallbackHandler()]}):</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> chain<span style=color:#f92672>.</span>stream({<span style=color:#e6db74>&#34;input&#34;</span>: message}):
</span></span><span style=display:flex><span>    partial_msg <span style=color:#f92672>=</span> partial_msg <span style=color:#f92672>+</span> chunk<span style=color:#f92672>.</span>content
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> partial_msg
</span></span></code></pre></div><p>Our <code>chatbot.py</code> should be look like this now.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_openai <span style=color:#f92672>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>(message: str, history: list, model_name: str):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_messages([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;human&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{input}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#f92672>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model_name,
</span></span><span style=display:flex><span>        openai_api_key<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_API_KEY&#34;</span>),
</span></span><span style=display:flex><span>        openai_api_base<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_BASE&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    chain <span style=color:#f92672>=</span> prompt <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    partial_msg <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># for chunk in history_runnable.stream({&#34;input&#34;: message}, config={&#34;configurable&#34;: {&#34;session_id&#34;: user_id}, &#34;callbacks&#34;: [ConsoleCallbackHandler()]}):</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> chain<span style=color:#f92672>.</span>stream({<span style=color:#e6db74>&#34;input&#34;</span>: message}):
</span></span><span style=display:flex><span>        partial_msg <span style=color:#f92672>=</span> partial_msg <span style=color:#f92672>+</span> chunk<span style=color:#f92672>.</span>content
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> partial_msg
</span></span></code></pre></div><p>Now, go back for a while to <code>main.py</code> and add this line on import section.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> chatbot <span style=color:#f92672>import</span> predict_chat
</span></span></code></pre></div><p>Cool! At this point, we can start playing around with our chatbot and it will respond to our chats!</p><p><img alt="Playing around with our chatbot" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/chatui_demo.gif#center></p><h3 id=context-aware-response-generation>Context-Aware Response Generation<a hidden class=anchor aria-hidden=true href=#context-aware-response-generation>#</a></h3><p>Our chatbot can respond to our question already. However, it lack of previous conversation contexes. Take a look on the captured conversation below. The previous context was talking about <strong>travel plan</strong>, but when I tried to continue the conversation it gave me an answer that doesn&rsquo;t have correlation with previous context.
<img alt="Chatbot failed to understand prevoius context" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/chatui_fail_context.png#center></p><p>To work on this issue, we need to put the chat history to our prompt. Here we will use SQLite as our database to save the whole chat history. Since we will only have one database for all users, we need a <code>session_id</code> between each user conversation history to avoid retrieving wrong user&rsquo;s conversation.</p><p>We will first add a hidden input in chat interface that generate a unique UUID which will act as our session_id. So, everytime we refresh the page, it will generate new session_id as well.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>import</span> uuid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Blocks(fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#66d9ef>as</span> demo:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    user_ids <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Textbox(visible<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, value<span style=color:#f92672>=</span>uuid<span style=color:#f92672>.</span>uuid4())
</span></span></code></pre></div><p>Next, add the hidden input component as Chat Interface additional_inputs as well. So, now Chat Interface additional inputs should contains model_choice and user_ids. Otherwise, we cannot pass the value to the function <code>predict_chat()</code> behind.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span>chat <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>ChatInterface(
</span></span><span style=display:flex><span>    predict_chat,
</span></span><span style=display:flex><span>    chatbot<span style=color:#f92672>=</span>chat_window,
</span></span><span style=display:flex><span>    additional_inputs<span style=color:#f92672>=</span>[model_choice, user_ids],
</span></span><span style=display:flex><span>    fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    retry_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>    undo_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>    clear_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Our final <code>main.py</code> should be look like this.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>import</span> gradio <span style=color:#66d9ef>as</span> gr
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> httpx
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> chat <span style=color:#f92672>import</span> predict_chat
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> uuid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>load_dotenv()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_free_models</span>():
</span></span><span style=display:flex><span>    res <span style=color:#f92672>=</span> httpx<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;https://openrouter.ai/api/v1/models&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> res:
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>        models <span style=color:#f92672>=</span> [item[<span style=color:#e6db74>&#34;id&#34;</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> res[<span style=color:#e6db74>&#34;data&#34;</span>] <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;free&#34;</span> <span style=color:#f92672>in</span> item[<span style=color:#e6db74>&#34;id&#34;</span>]]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> sorted(models)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Blocks(fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#66d9ef>as</span> demo:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    user_ids <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Textbox(visible<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, value<span style=color:#f92672>=</span>uuid<span style=color:#f92672>.</span>uuid4())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    models <span style=color:#f92672>=</span> get_free_models()
</span></span><span style=display:flex><span>    model_choice <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Dropdown(
</span></span><span style=display:flex><span>        choices<span style=color:#f92672>=</span>models,
</span></span><span style=display:flex><span>        show_label<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Model Choice&#34;</span>,
</span></span><span style=display:flex><span>        interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        value<span style=color:#f92672>=</span>models[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat_window <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Chatbot(bubble_full_width<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, render<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>ChatInterface(
</span></span><span style=display:flex><span>        predict_chat,
</span></span><span style=display:flex><span>        chatbot<span style=color:#f92672>=</span>chat_window,
</span></span><span style=display:flex><span>        additional_inputs<span style=color:#f92672>=</span>[model_choice, user_ids],
</span></span><span style=display:flex><span>        fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        retry_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        undo_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        clear_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>queue()
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>launch()
</span></span></code></pre></div><p>Now, we&rsquo;re done with <code>main.py</code>. Let&rsquo;s move further to <code>chatbot.py</code> file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_message_histories <span style=color:#f92672>import</span> SQLChatMessageHistory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_chat_history</span>(session_id: str):
</span></span><span style=display:flex><span>    chat_history <span style=color:#f92672>=</span> SQLChatMessageHistory(
</span></span><span style=display:flex><span>        session_id<span style=color:#f92672>=</span>session_id, connection_string<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sqlite:///memory.db&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> chat_history
</span></span></code></pre></div><p>Then, we&rsquo;ll a new variable named <code>history</code> in our prompt using <code>MessagesPlaceholder</code>. The rest of the prompt attribute stays the same. Also, don&rsquo;t forget to add user_id to our predict_chat function parameter</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>(message: str, history: list, model_name: str, user_id: str):
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_messages([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.&#34;</span>),
</span></span><span style=display:flex><span>        MessagesPlaceholder(<span style=color:#e6db74>&#34;history&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;human&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{input}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span></code></pre></div><p>Next, instead of we call invoke directly on chained prompt and LLM instance, we will use a new instance called <code>RunnableWithMessageHistory</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables.history <span style=color:#f92672>import</span> RunnableWithMessageHistory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>history_runnable <span style=color:#f92672>=</span> RunnableWithMessageHistory(
</span></span><span style=display:flex><span>    chain,
</span></span><span style=display:flex><span>    get_session_history<span style=color:#f92672>=</span>get_chat_history,
</span></span><span style=display:flex><span>    input_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;input&#34;</span>,
</span></span><span style=display:flex><span>    history_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;history&#34;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Remember that we should always save our current conversation to database so we can use it in future interaction with chatbot? Gratefully, all those logic already handled by <code>RunnableWithMessageHistory</code> so we don&rsquo;t have to handle it by ourselves.</p><p>Note that we also put <code>input</code> and <code>history</code> as input and history message key respectively. Keep in mind that this key should match with variables key that already defined in prompt.</p><p>Finally, rather than calling stream from <code>chain</code>, we call it from <code>history_runnable</code> instead. So our streaming code will look like this.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span>partial_msg <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> history_runnable<span style=color:#f92672>.</span>stream({<span style=color:#e6db74>&#34;input&#34;</span>: message}, config<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;configurable&#34;</span>: {<span style=color:#e6db74>&#34;session_id&#34;</span>: user_id}}):
</span></span><span style=display:flex><span>    partial_msg <span style=color:#f92672>=</span> partial_msg <span style=color:#f92672>+</span> chunk<span style=color:#f92672>.</span>content
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> partial_msg
</span></span></code></pre></div><p>Our final <code>main.py</code> should be look like this.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> langchain_openai <span style=color:#f92672>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate, MessagesPlaceholder
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables.history <span style=color:#f92672>import</span> RunnableWithMessageHistory
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_message_histories <span style=color:#f92672>import</span> SQLChatMessageHistory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_chat_history</span>(session_id: str):
</span></span><span style=display:flex><span>    chat_history <span style=color:#f92672>=</span> SQLChatMessageHistory(session_id<span style=color:#f92672>=</span>session_id, connection_string<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sqlite:///memory.db&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> chat_history
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>(message: str, history: list, model_name: str, user_id: str):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_messages([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.&#34;</span>),
</span></span><span style=display:flex><span>        MessagesPlaceholder(<span style=color:#e6db74>&#34;history&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;human&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{input}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#f92672>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model_name,
</span></span><span style=display:flex><span>        openai_api_key<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_API_KEY&#34;</span>),
</span></span><span style=display:flex><span>        openai_api_base<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_BASE&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chain <span style=color:#f92672>=</span> prompt <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    history_runnable <span style=color:#f92672>=</span> RunnableWithMessageHistory(
</span></span><span style=display:flex><span>        chain,
</span></span><span style=display:flex><span>        get_session_history<span style=color:#f92672>=</span>get_chat_history,
</span></span><span style=display:flex><span>        input_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;input&#34;</span>,
</span></span><span style=display:flex><span>        history_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;history&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    partial_msg <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> history_runnable<span style=color:#f92672>.</span>stream({<span style=color:#e6db74>&#34;input&#34;</span>: message}, config<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;configurable&#34;</span>: {<span style=color:#e6db74>&#34;session_id&#34;</span>: user_id}}):
</span></span><span style=display:flex><span>        partial_msg <span style=color:#f92672>=</span> partial_msg <span style=color:#f92672>+</span> chunk<span style=color:#f92672>.</span>content
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> partial_msg
</span></span></code></pre></div><p>That&rsquo;s a wrap! Our chatbot now can understand previous conversation context. Super! ‚ö°Ô∏è
<img alt="Final chatbot that understand context from previous conversation" loading=lazy src=/posts/building-conversational-ai-context-aware-chatbot/images/chatui_demo_final.gif#center></p><h3 id=full-project>Full Project<a hidden class=anchor aria-hidden=true href=#full-project>#</a></h3><p><a href=https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter>https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter</a></p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Throughout this article, we already covered how to build a context-aware chatbot &ndash; a chatbot that understand previous conversation contexes.</p><p>In the upcoming article I will demonstrate how we can extend this chatbot to be able interact with external files as well such as financial reports, product catalogs, or even company profile website.</p><p>Stay tune! üëã</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://python.langchain.com/v0.1/docs/expression_language/get_started/>LangChain getting started</a></li><li><a href=https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/>Add message history (memory)</a></li><li><a href=https://python.langchain.com/v0.1/docs/expression_language/why/>LCEL (LangChain Expression Language)</a></li><li><a href=https://openrouter.ai/docs/quick-start>OpenRouter docs</a></li></ol><hr><h2 id=lets-get-connected->Let&rsquo;s get Connected üôå<a hidden class=anchor aria-hidden=true href=#lets-get-connected->#</a></h2><p>If you have any inquiries, comments, suggestions, or critics please don‚Äôt hesitate to reach me out:</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/chatbot/>Chatbot</a></li><li><a href=http://localhost:1313/tags/conversational-ai/>Conversational Ai</a></li><li><a href=http://localhost:1313/tags/context-aware/>Context Aware</a></li><li><a href=http://localhost:1313/tags/langchain/>Langchain</a></li><li><a href=http://localhost:1313/tags/rag/>Rag</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/building-conversational-ai-chat-with-your-data/><span class=title>¬´ Prev</span><br><span>Building Conversational AI with LangChain Part 2: Chat with Your Data</span>
</a><a class=next href=http://localhost:1313/posts/pain-free-python-fastapi-rmq-integration/><span class=title>Next ¬ª</span><br><span>Pain-free Python Fastapi RabbitMQ Integration</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>fahrizain</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>