<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know | fahrizain</title><meta name=keywords content="jina ai,document parsing,llm understanding,pdf,website"><meta name=description content="TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding."><meta name=author content="Affandy Fahrizain"><link rel=canonical href=https://fhrzn.github.io/posts/llm-document-parsing-tips/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://fhrzn.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fhrzn.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fhrzn.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fhrzn.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fhrzn.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://fhrzn.github.io/posts/llm-document-parsing-tips/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:url" content="https://fhrzn.github.io/posts/llm-document-parsing-tips/"><meta property="og:site_name" content="fahrizain"><meta property="og:title" content="Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know"><meta property="og:description" content="TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-26T19:56:54+07:00"><meta property="article:modified_time" content="2024-10-26T19:56:54+07:00"><meta property="og:image" content="https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png"><meta name=twitter:title content="Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know"><meta name=twitter:description content="TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding."><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fhrzn.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know","item":"https://fhrzn.github.io/posts/llm-document-parsing-tips/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know","name":"Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know","description":"TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding.","keywords":["jina ai","document parsing","llm understanding","pdf","website"],"articleBody":"TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding.\nDocument Parsing In RAG, it is very common to upload our documents (i.e. PDF, spreadsheet, docs, or website) to the vector database so we can augment the LLM knowledge from it. One integral step is parsing the document itself, which the result must represent the document in the best way.\nThe easiest way to do that, is just read the file and get the contents. However, it is very possible to make the content losing it structures. Gratefully, we can mitigate this by parsing the document into structured format â€“ Markdown. To the best of my experience, most LLMs are work best with Markdown format â€“ especially OpenAIâ€™s family.\nPDF To parse PDF file into Markdown, we can leverage the extended version of PyMuPDF library, called PyMuPDF4LLM.\nThis library is designed specifically to produce the best output that works well with LLM. Without further ado, letâ€™s see how we can use it to parse PDF document.\nFirst, we need to install the library itself.\npip install pymupdf4llm We will use these sample PDFs from https://www.princexml.com/samples/. Optionally, you may also use your own PDF file.\nFortunately, it is very easy to convert it into markdown using pymupdf4llm. In this article we will only work with one of them. You can find the rest implementation on GitHub repo.\nimport pymupdf4llm # these files are downloaded from link above filepath = { \"invoice1\": \"assets/invoice.pdf\", \"invoice2\": \"assets/invoice2.pdf\", \"brochure\": \"assets/brochure.pdf\", \"newsletter\": \"assets/newsletter.pdf\", \"textbook\": \"assets/textbook.pdf\", } newsletter = pymupdf4llm.to_markdown(filepath[\"newsletter\"], show_progress=False) print(newsletter) Here is the parsed document output:\n# DrylabNews #### for investors \u0026 friends Â· May 2017 Welcome to our first newsletter of 2017! It's been a while since the last one, and a lot has happened. We promise to keep them coming every two months hereafter, and permit ourselves to make this one rather long. The big news is the beginnings of our launch in the American market, but there are also interesting updates on sales, development, mentors and (of course) the investment round that closed in January. **New capital: The investment round was** successful. We raised 2.13 MNOK to match the 2.05 MNOK loan from Innovation Norway. Including the development agreement with Filmlance International, the total new capital is 5 MNOK, partly tied to the successful completion of milestones. All formalities associated with this process are now finalized. **New owners: We would especially like to** warmly welcome our new owners to the Drylab family: Unni Jacobsen, Torstein Jahr, Suzanne Bolstad, Eivind Bergene, Turid Brun, Vigdis Trondsen, Lea Blindheim, Kristine ## 34 ### meetingsmeetings NY Â· SFNY Â· SF LA Â· LLA Â· LVV Academy of Motion Picture Arts and Sciences Â· Alesha \u0026 Jamie Metzger Â· Amazon AWS Â· Apple Â· Caitlin Burns, PGA Â· Carlos Melcer Â· Chimney L.A. Â· Dado Valentic Â· Dave Stump Â· DIT WIT Â· ERA NYC Â· Facebook Â· Fancy Film Â· FilmLight Â· Geo Labelle Â· Google Â· IBM Â· Innovation Norway (NYC) Â· Innovation Norway (SF) Â· International Cinematographers Guild Â· NBC Â· Local 871 Â· Netflix Â· Pomfort Â· Radiant Images Â· Screening Room Â· Signiant Â· Moods of Norway Â· Tapad Â· Team Downey ----- Holmsen, Torstein Hansen, and Jostein Aanensen. We look forward to working with you! **Sales: Return customer rate is now 80%,** proving value and willingness to pay. Film Factory Montreal is our first customer in Canada. Lumiere Numeriques have started using us in France. We also have new customers in Norway, and high-profile users such as Gareth Unwin, producer of Oscar[winning The King's Speech. Revenue for the](http://www.imdb.com/title/tt1504320/) first four months is 200 kNOK, compared to 339 kNOK for all of 2016. We are working on a partnership to safeguard sales in Norway while beginning to focus more on the US. ... (We trimmed the output) From the result above, we can see there is ----- token denoting different pages.\nAlthough, the parsed result is not perfect, the outputâ€™s structure is good enough as it also maintain separation of each document parts.\nAs comparison, we shall look into Langchain PyPDFLoader implementation.\nðŸ’¡ Make sure you already install langchain on your machine.\nfrom langchain_community.document_loaders import PyPDFLoader lc_newsletter = \"\\n==================\\n\".join(doc.page_content for doc in list(PyPDFLoader(filepath[\"newsletter\"]).lazy_load())) print(lc_newsletter) ðŸ’¡ Note that langchain document loaderâ€™s implementation always return list of langchain Document object â€“ each page represented by an object. Therefore we join them with separator token ================== to denote different pages.\nHere is the result of Langchain PyPDFLoader:\nDrylab Newsfor in vestors \u0026 friends Â· Ma y 2017 Welcome to our first newsletter of 2017! It's been a while since the last one, and a lot has happened. W e promise to k eep them coming every two months hereafter , and permit ourselv es to mak e this one r ather long. The big news is the beginnings of our launch in the American mark et, but there are also interesting updates on sales, de velopment, mentors and ( of course ) the in vestment round that closed in January . New c apital: The in vestment round was successful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation Norwa y. Including the de velopment agreement with Filmlance International, the total new capital is 5 MNOK, partly tied to the successful completion of milestones. All formalities associated with this process are now finalized. New o wners: We would especially lik e to warmly welcome our new owners to the Drylab family: Unni Jacobsen, T orstein Jahr , Suzanne Bolstad, Eivind Bergene, T urid Brun, Vigdis T rondsen, L ea Blindheim, Kristine 34meetingsmeetings NY Â· SFNY Â· SF LA Â· LLA Â· L VVAcadem yofMotion Picture Arts and Sciences Â·Alesha \u0026Jamie Metzger Â·Amazon AWS Â·Apple Â·Caitlin Burns, PGA Â·Carlos Melcer Â·Chimne yL.A.Â·Dado Valentic Â· DaveStump Â·DIT WIT Â·ERA NYCÂ·Facebook Â·Fancy Film Â·FilmLight Â·Geo Labelle Â· Google Â·IBM Â·Inno vation Norwa y(NY C)Â·Inno vation Norwa y(SF) Â·International Cinematogr aphers Guild Â·NBC Â·Local 871 Â·Netflix Â·Pomfort Â·Radiant Images Â· Screening Room Â· Signiant Â· Moods of Norwa yÂ· Tapad Â· Team Downe y ================== Holmsen, T orstein Hansen, and Jostein Aanensen. W e look forward to working with you! Sales: Return customer r ate is now 80%, pro ving value and willingness to pa y. Film Factory Montreal is our first customer in Canada. Lumiere Numeriques ha ve started using us in F rance. W e also ha ve new customers in Norwa y, and high-profile users such as Gareth Un win, producer of Oscar- winning The King's Speech . Re venue for the first four months is 200 kNOK, compared to 339 kNOK for all of 2016. W e are working on a partnership to safeguard sales in Norwa y while beginning to focus more on the US. Pay attention the output documents structure are not preserved, making its hard to identify which part originally belongs to. In addition, if we take a closer look, some captured words are strangely separated by random whitespace.\nAlthough the LLM may still can understand it, I believe giving better input representation will produce better output as well. Therefore, parsing into markdown format is a good choice to enhance LLM understanding of our document.\nNow we already know how to parse PDF into markdown format. What if I am telling you that you can do the same to webpages?\nLetâ€™s look how to do it.\nWebpage To parse webpage into Markdown, we can utilize Jina AI Reader API.\nJina AI give an API key we can use for the first 1M token processed. Once it reaches the limit, we need to top up if we are intended to use the API key.\nAfraid not, we can still use their service (for now) even without the API key. Simply do the request with omitting the API key. In this example, we will try to omit the API key.\nðŸš§ Please note the rate limiter is very tight when we are not using the API key. Make sure we are doing graceful requests, otherwise we will get Error 429 â€“ Too many requests\nTo use it, we only need to perform GET request to their endpoint.\nSkip it if you have installed it already\npip install requests We will try to parse one of my article Exploring Vision Transformers (ViT) with ðŸ¤— Huggingface. You may also change it into any other websites.\nimport requests import os BASE_URL = \"https://r.jina.ai\" site = \"https://fhrzn.github.io/posts/building-conversational-ai-context-aware-chatbot/\" url = os.path.join(BASE_URL, site) resp = requests.get(url) print(resp.text) And here is the parsed result:\nTitle: Vision Transformers (ViT) with ðŸ¤— Huggingface | Data Folks Indonesia URL Source: https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0 Published Time: 2022-10-14T11:00:46.983Z Markdown Content: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021) ----------------------------------------------------------------------------------------------------- [![Image 1: Affandy Fahrizain](https://miro.medium.com/v2/resize:fill:88:88/1*JCueIcAZjfbCE_ro4ZB8Og.jpeg)](https://medium.com/@fahrizain?source=post_page-----8cdda82920a0--------------------------------)[![Image 2: Data Folks Indonesia](https://miro.medium.com/v2/resize:fill:48:48/1*s8T4-0fscxMhh6V8adR4mQ.png)](https://medium.com/data-folks-indonesia?source=post_page-----8cdda82920a0--------------------------------) Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration! ![Image 3](https://miro.medium.com/v2/resize:fit:700/0*et8V-t6bjFm1w6ds) Photo by [Alex Litvin](https://unsplash.com/@alexlitvin?utm_source=medium\u0026utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium\u0026utm_medium=referral) Background ---------- As self-attention based model like Transformers has successfully become a _standard_ in NLP area, it triggers researchers to adapt attention-based models in Computer Vision too. There were different evidences, such as combine CNN with self-attention and completely replace Convolutions. While this selected paper belongs to the latter aproach. The application of attention mechanism in images requires each pixel attends to every other pixel, which indeed requires expensive computation. Hence, several techniques have been applied such as self-attention only in local neighborhoods \\[1\\], using local multihead dot product self-attention blocks to completely replace convolutions \\[2\\]\\[3\\]\\[4\\], postprocessing CNN outputs using self- attention \\[5\\]\\[6\\], etc. Although shown promising results, these techniques quite hard to be scaled and requires complex engineering to be implemented efficiently on hardware accelerators. On the other hand, Transformers model is based on MLP networks, it has more computational efficiency and scalability, making its possible to train big models with over 100B parameters. Methods ------- ![Image 4](https://miro.medium.com/v2/resize:fit:700/1*-HQPfbnebarylP543i58_Q.png) General architecture of ViT. Taken from the original paper (Dosovitskiy et al., 2021) The original Transformers model treat its input as sequences which very different approach with CNN, hence the inputted images need to be extracted into fixed-size patches and flattened. Similar to BERT \\[CLS\\] token, the so-called _classification token_ will be added into the beginning of the sequences, which will serve as image representation and later will be fed into classification head. Finally, to retain the positional information of the sequences, positional embedding will be added to each patch. The authors designed model following the original Transformers as close as possible. The proposed model then called as Vision Transfomers (ViT). ... (We trimmed the output) Please also pay attention, for some website this technique might not works very well. That probably caused by firewall or cloudflare protection. You may use the proxy to mitigate it.\nAlso, there are a lot more options provided by Jina AI Reader. You may find it out here https://jina.ai/reader/#apiform.\nConclusion Maintaining document structures can ensure the quality of LLM response when it getting asked about our documents. Therefore, choosing the right tools is essential. PyMuPDF4LLM ensure the parsed document output is given in markdown format, which is great in maintaining the document structures.\nOn the other side, Langchain implementation is easy to use. Unfortunately, it lack of ability to preserve document structure. One may extend Langchain API to create a PyMuPDF4LLM integration. So that can take benefit from both sides.\nShould you have other opinions or feedbacks, please never hesitate to comment below!\nLetâ€™s get Connected ðŸ™Œ If you have any inquiries, comments, suggestions, or critics please donâ€™t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","wordCount":"1973","inLanguage":"en","image":"https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png","datePublished":"2024-10-26T19:56:54+07:00","dateModified":"2024-10-26T19:56:54+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fhrzn.github.io/posts/llm-document-parsing-tips/"},"publisher":{"@type":"Organization","name":"fahrizain","logo":{"@type":"ImageObject","url":"https://fhrzn.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fhrzn.github.io/ accesskey=h title="fahrizain (Alt + H)"><img src=https://fhrzn.github.io/favicon-32x32.png alt aria-label=logo height=30>fahrizain</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://fhrzn.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://fhrzn.github.io/profile/ title=Profile><span>Profile</span></a></li><li><a href=https://www.linkedin.com/in/fahrizainn/ title=LinkedIn><span>LinkedIn</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know</h1><div class=post-meta><span title='2024-10-26 19:56:54 +0700 WIB'>October 26, 2024</span>&nbsp;Â·&nbsp;10 min&nbsp;Â·&nbsp;1973 words&nbsp;Â·&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager srcset='https://fhrzn.github.io/posts/llm-document-parsing-tips/cover_hu_7cfd04f14619c7ad.png 360w,https://fhrzn.github.io/posts/llm-document-parsing-tips/cover_hu_b88874618a778630.png 480w,https://fhrzn.github.io/posts/llm-document-parsing-tips/cover_hu_3f04e7dfc10afbc3.png 720w,https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png 1024w' src=https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png sizes="(min-width: 768px) 720px, 100vw" width=1024 height=1024 alt="a newspaper representing RAG's document"><figcaption>This image was generated using Microsoft Copilot</figcaption></figure><div class=post-content><p>TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding.</p><h3 id=document-parsing>Document Parsing<a hidden class=anchor aria-hidden=true href=#document-parsing>#</a></h3><p>In RAG, it is very common to upload our documents (i.e. PDF, spreadsheet, docs, or website) to the vector database so we can augment the LLM knowledge from it. One integral step is parsing the document itself, which the result must represent the document in the best way.</p><p>The easiest way to do that, is just read the file and get the contents. However, it is very possible to make the content losing it structures. Gratefully, we can mitigate this by parsing the document into structured format â€“ Markdown. To the best of my experience, most LLMs are work best with Markdown format â€“ especially OpenAI&rsquo;s family.</p><h2 id=pdf>PDF<a hidden class=anchor aria-hidden=true href=#pdf>#</a></h2><p>To parse PDF file into Markdown, we can leverage the extended version of PyMuPDF library, called <strong>PyMuPDF4LLM</strong>.</p><p>This library is designed specifically to produce the best output that works well with LLM. Without further ado, let&rsquo;s see how we can use it to parse PDF document.</p><p>First, we need to install the library itself.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install pymupdf4llm
</span></span></code></pre></div><p>We will use these sample PDFs from <a href=https://www.princexml.com/samples/>https://www.princexml.com/samples/</a>. Optionally, you may also use your own PDF file.</p><p>Fortunately, it is very easy to convert it into markdown using pymupdf4llm. In this article we will only work with one of them. You can find the rest implementation on <a href=https://github.com/fhrzn/rags-archive>GitHub repo</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pymupdf4llm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># these files are downloaded from link above</span>
</span></span><span class=line><span class=cl><span class=n>filepath</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;invoice1&#34;</span><span class=p>:</span> <span class=s2>&#34;assets/invoice.pdf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;invoice2&#34;</span><span class=p>:</span> <span class=s2>&#34;assets/invoice2.pdf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;brochure&#34;</span><span class=p>:</span> <span class=s2>&#34;assets/brochure.pdf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;newsletter&#34;</span><span class=p>:</span> <span class=s2>&#34;assets/newsletter.pdf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;textbook&#34;</span><span class=p>:</span> <span class=s2>&#34;assets/textbook.pdf&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>newsletter</span> <span class=o>=</span> <span class=n>pymupdf4llm</span><span class=o>.</span><span class=n>to_markdown</span><span class=p>(</span><span class=n>filepath</span><span class=p>[</span><span class=s2>&#34;newsletter&#34;</span><span class=p>],</span> <span class=n>show_progress</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>newsletter</span><span class=p>)</span>
</span></span></code></pre></div><p>Here is the parsed document output:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-markdown data-lang=markdown><span class=line><span class=cl><span class=gh># DrylabNews
</span></span></span><span class=line><span class=cl><span class=gh></span>
</span></span><span class=line><span class=cl><span class=gu>#### for investors &amp; friends Â· May 2017
</span></span></span><span class=line><span class=cl><span class=gu></span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Welcome to our first newsletter of 2017! It&#39;s
</span></span><span class=line><span class=cl>been a while since the last one, and a lot has
</span></span><span class=line><span class=cl>happened. We promise to keep them coming
</span></span><span class=line><span class=cl>every two months hereafter, and permit
</span></span><span class=line><span class=cl>ourselves to make this one rather long. The
</span></span><span class=line><span class=cl>big news is the beginnings of our launch in
</span></span><span class=line><span class=cl>the American market, but there are also
</span></span><span class=line><span class=cl>interesting updates on sales, development,
</span></span><span class=line><span class=cl>mentors and (of course) the investment
</span></span><span class=line><span class=cl>round that closed in January.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=gs>**New capital: The investment round was**</span>
</span></span><span class=line><span class=cl>successful. We raised 2.13 MNOK to match
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>the 2.05 MNOK loan from Innovation
</span></span><span class=line><span class=cl>Norway. Including the development
</span></span><span class=line><span class=cl>agreement with Filmlance International, the
</span></span><span class=line><span class=cl>total new capital is 5 MNOK, partly tied to
</span></span><span class=line><span class=cl>the successful completion of milestones. All
</span></span><span class=line><span class=cl>formalities associated with this process are
</span></span><span class=line><span class=cl>now finalized.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=gs>**New owners: We would especially like to**</span>
</span></span><span class=line><span class=cl>warmly welcome our new owners to the
</span></span><span class=line><span class=cl>Drylab family: Unni Jacobsen, Torstein Jahr,
</span></span><span class=line><span class=cl>Suzanne Bolstad, Eivind Bergene, Turid Brun,
</span></span><span class=line><span class=cl>Vigdis Trondsen, Lea Blindheim, Kristine
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=gu>## 34
</span></span></span><span class=line><span class=cl><span class=gu></span>
</span></span><span class=line><span class=cl><span class=gu>### meetingsmeetings
</span></span></span><span class=line><span class=cl><span class=gu></span> NY Â· SFNY Â· SF
</span></span><span class=line><span class=cl> LA Â· LLA Â· LVV
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Academy of Motion Picture Arts and Sciences Â· Alesha <span class=err>&amp;</span> Jamie Metzger Â· Amazon
</span></span><span class=line><span class=cl>AWS Â· Apple Â· Caitlin Burns, PGA Â· Carlos Melcer Â· Chimney L.A. Â· Dado Valentic Â·
</span></span><span class=line><span class=cl>Dave Stump Â· DIT WIT Â· ERA NYC Â· Facebook Â· Fancy Film Â· FilmLight Â· Geo Labelle Â·
</span></span><span class=line><span class=cl>Google Â· IBM Â· Innovation Norway (NYC) Â· Innovation Norway (SF) Â· International
</span></span><span class=line><span class=cl>Cinematographers Guild Â· NBC Â· Local 871 Â· Netflix Â· Pomfort Â· Radiant Images Â·
</span></span><span class=line><span class=cl>Screening Room Â· Signiant Â· Moods of Norway Â· Tapad Â· Team Downey
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>-----
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Holmsen, Torstein Hansen, and Jostein
</span></span><span class=line><span class=cl>Aanensen. We look forward to working with
</span></span><span class=line><span class=cl>you!
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=gs>**Sales: Return customer rate is now 80%,**</span>
</span></span><span class=line><span class=cl>proving value and willingness to pay. Film
</span></span><span class=line><span class=cl>Factory Montreal is our first customer in
</span></span><span class=line><span class=cl>Canada. Lumiere Numeriques have started
</span></span><span class=line><span class=cl>using us in France. We also have new
</span></span><span class=line><span class=cl>customers in Norway, and high-profile users
</span></span><span class=line><span class=cl>such as Gareth Unwin, producer of Oscar[<span class=nt>winning The King&#39;s Speech. Revenue for the</span>](<span class=na>http://www.imdb.com/title/tt1504320/</span>)
</span></span><span class=line><span class=cl>first four months is 200 kNOK, compared to
</span></span><span class=line><span class=cl>339 kNOK for all of 2016. We are working
</span></span><span class=line><span class=cl>on a partnership to safeguard sales in
</span></span><span class=line><span class=cl>Norway while beginning to focus more on
</span></span><span class=line><span class=cl>the US.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>... (We trimmed the output)
</span></span></code></pre></div><p>From the result above, we can see there is <code>-----</code> token denoting different pages.</p><p>Although, the parsed result is not perfect, the output&rsquo;s structure is good enough as it also maintain separation of each document parts.</p><h1></h1><h1></h1><p>As comparison, we shall look into <strong>Langchain PyPDFLoader</strong> implementation.</p><blockquote><p>ðŸ’¡ Make sure you already install langchain on your machine.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_community.document_loaders</span> <span class=kn>import</span> <span class=n>PyPDFLoader</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>lc_newsletter</span> <span class=o>=</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>==================</span><span class=se>\n</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=nb>list</span><span class=p>(</span><span class=n>PyPDFLoader</span><span class=p>(</span><span class=n>filepath</span><span class=p>[</span><span class=s2>&#34;newsletter&#34;</span><span class=p>])</span><span class=o>.</span><span class=n>lazy_load</span><span class=p>()))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lc_newsletter</span><span class=p>)</span>
</span></span></code></pre></div><blockquote><p>ðŸ’¡ Note that langchain document loader&rsquo;s implementation always return list of langchain <code>Document</code> object â€“ each page represented by an object. Therefore we join them with separator token <code>==================</code> to denote different pages.</p></blockquote><p>Here is the result of Langchain PyPDFLoader:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-markdown data-lang=markdown><span class=line><span class=cl>Drylab Newsfor in vestors <span class=err>&amp;</span> friends Â· Ma y 2017
</span></span><span class=line><span class=cl>Welcome to our first newsletter of 2017! It&#39;s
</span></span><span class=line><span class=cl>been a while since the last one, and a lot has
</span></span><span class=line><span class=cl>happened. W e promise to k eep them coming
</span></span><span class=line><span class=cl>every two months hereafter , and permit
</span></span><span class=line><span class=cl>ourselv es to mak e this one r ather long. The
</span></span><span class=line><span class=cl>big news is the beginnings of our launch in
</span></span><span class=line><span class=cl>the American mark et, but there are also
</span></span><span class=line><span class=cl>interesting updates on sales, de velopment,
</span></span><span class=line><span class=cl>mentors and ( of course ) the in vestment
</span></span><span class=line><span class=cl>round that closed in January .
</span></span><span class=line><span class=cl>New c apital: The in vestment round was
</span></span><span class=line><span class=cl>successful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation
</span></span><span class=line><span class=cl>Norwa y. Including the de velopment
</span></span><span class=line><span class=cl>agreement with Filmlance International, the
</span></span><span class=line><span class=cl>total new capital is 5 MNOK, partly tied to
</span></span><span class=line><span class=cl>the successful completion of milestones. All
</span></span><span class=line><span class=cl>formalities associated with this process are
</span></span><span class=line><span class=cl>now finalized.
</span></span><span class=line><span class=cl>New o wners: We would especially lik e to
</span></span><span class=line><span class=cl>warmly welcome our new owners to the
</span></span><span class=line><span class=cl>Drylab family: Unni Jacobsen, T orstein Jahr ,
</span></span><span class=line><span class=cl>Suzanne Bolstad, Eivind Bergene, T urid Brun,
</span></span><span class=line><span class=cl>Vigdis T rondsen, L ea Blindheim, Kristine
</span></span><span class=line><span class=cl>34meetingsmeetings
</span></span><span class=line><span class=cl>NY Â· SFNY Â· SF
</span></span><span class=line><span class=cl>LA Â· LLA Â· L VVAcadem yofMotion Picture Arts and Sciences Â·Alesha <span class=err>&amp;</span>Jamie Metzger Â·Amazon
</span></span><span class=line><span class=cl>AWS Â·Apple Â·Caitlin Burns, PGA Â·Carlos Melcer Â·Chimne yL.A.Â·Dado Valentic Â·
</span></span><span class=line><span class=cl>DaveStump Â·DIT WIT Â·ERA NYCÂ·Facebook Â·Fancy Film Â·FilmLight Â·Geo Labelle Â·
</span></span><span class=line><span class=cl>Google Â·IBM Â·Inno vation Norwa y(NY C)Â·Inno vation Norwa y(SF) Â·International
</span></span><span class=line><span class=cl>Cinematogr aphers Guild Â·NBC Â·Local 871 Â·Netflix Â·Pomfort Â·Radiant Images Â·
</span></span><span class=line><span class=cl>Screening Room Â· Signiant Â· Moods of Norwa yÂ· Tapad Â· Team Downe y
</span></span><span class=line><span class=cl>==================
</span></span><span class=line><span class=cl>Holmsen, T orstein Hansen, and Jostein
</span></span><span class=line><span class=cl>Aanensen. W e look forward to working with
</span></span><span class=line><span class=cl>you!
</span></span><span class=line><span class=cl>Sales: Return customer r ate is now 80%,
</span></span><span class=line><span class=cl>pro ving value and willingness to pa y. Film
</span></span><span class=line><span class=cl>Factory Montreal is our first customer in
</span></span><span class=line><span class=cl>Canada. Lumiere Numeriques ha ve started
</span></span><span class=line><span class=cl>using us in F rance. W e also ha ve new
</span></span><span class=line><span class=cl>customers in Norwa y, and high-profile users
</span></span><span class=line><span class=cl>such as Gareth Un win, producer of Oscar-
</span></span><span class=line><span class=cl>winning The King&#39;s Speech . Re venue for the
</span></span><span class=line><span class=cl>first four months is 200 kNOK, compared to
</span></span><span class=line><span class=cl>339 kNOK for all of 2016. W e are working
</span></span><span class=line><span class=cl>on a partnership to safeguard sales in
</span></span><span class=line><span class=cl>Norwa y while beginning to focus more on
</span></span><span class=line><span class=cl>the US.
</span></span></code></pre></div><p>Pay attention the output documents structure are not preserved, making its hard to identify which part originally belongs to. In addition, if we take a closer look, some captured words are strangely separated by random whitespace.</p><p>Although the LLM may still can understand it, I believe giving better input representation will produce better output as well. Therefore, parsing into markdown format is a good choice to enhance LLM understanding of our document.</p><p>Now we already know how to parse PDF into markdown format. What if I am telling you that you can do the same to webpages?</p><p>Let&rsquo;s look how to do it.</p><h2 id=webpage>Webpage<a hidden class=anchor aria-hidden=true href=#webpage>#</a></h2><p>To parse webpage into Markdown, we can utilize <a href=https://jina.ai/reader/>Jina AI Reader API</a>.</p><p><input type=checkbox id=zoomCheck-a3a73 hidden>
<label for=zoomCheck-a3a73><img class=zoomCheck loading=lazy decoding=async src=images/jina.png#center alt="Jina AI Reader API"></label></p><p>Jina AI give an API key we can use for the first 1M token processed. Once it reaches the limit, we need to top up if we are intended to use the API key.</p><p>Afraid not, we can still use their service (for now) even without the API key. Simply do the request with omitting the API key. In this example, we will try to omit the API key.</p><blockquote><p>ðŸš§ Please note the rate limiter is very tight when we are not using the API key. Make sure we are doing graceful requests, otherwise we will get <strong>Error 429 â€“ Too many requests</strong></p></blockquote><p>To use it, we only need to perform GET request to their endpoint.</p><p>Skip it if you have installed it already</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install requests
</span></span></code></pre></div><p>We will try to parse one of my article <em><a href=https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0>Exploring Vision Transformers (ViT) with ðŸ¤— Huggingface</a></em>. You may also change it into any other websites.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>BASE_URL</span> <span class=o>=</span> <span class=s2>&#34;https://r.jina.ai&#34;</span>
</span></span><span class=line><span class=cl><span class=n>site</span> <span class=o>=</span> <span class=s2>&#34;https://fhrzn.github.io/posts/building-conversational-ai-context-aware-chatbot/&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>BASE_URL</span><span class=p>,</span> <span class=n>site</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>resp</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>resp</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span></span></code></pre></div><p>And here is the parsed result:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-markdown data-lang=markdown><span class=line><span class=cl>Title: Vision Transformers (ViT) with ðŸ¤— Huggingface | Data Folks Indonesia
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>URL Source: https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Published Time: 2022-10-14T11:00:46.983Z
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Markdown Content:
</span></span><span class=line><span class=cl>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021)
</span></span><span class=line><span class=cl>-----------------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[<span class=nt>![Image 1: Affandy Fahrizain</span>](<span class=na>https://miro.medium.com/v2/resize:fill:88:88/1*JCueIcAZjfbCE_ro4ZB8Og.jpeg</span>)](https://medium.com/<span class=ni>@fahrizain</span>?source=post_page-----8cdda82920a0--------------------------------)[<span class=nt>![Image 2: Data Folks Indonesia</span>](<span class=na>https://miro.medium.com/v2/resize:fill:48:48/1*s8T4-0fscxMhh6V8adR4mQ.png</span>)](https://medium.com/data-folks-indonesia?source=post_page-----8cdda82920a0--------------------------------)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration!
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>![<span class=nt>Image 3</span>](<span class=na>https://miro.medium.com/v2/resize:fit:700/0*et8V-t6bjFm1w6ds</span>)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Photo by [<span class=nt>Alex Litvin</span>](<span class=na>https://unsplash.com/@alexlitvin?utm_source=medium&amp;utm_medium=referral</span>) on [<span class=nt>Unsplash</span>](<span class=na>https://unsplash.com/?utm_source=medium&amp;utm_medium=referral</span>)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Background
</span></span><span class=line><span class=cl>----------
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>As self-attention based model like Transformers has successfully become a <span class=ge>_standard_</span> in NLP area, it triggers researchers to adapt attention-based models in Computer Vision too. There were different evidences, such as combine CNN with self-attention and completely replace Convolutions. While this selected paper belongs to the latter aproach.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>The application of attention mechanism in images requires each pixel attends to every other pixel, which indeed requires expensive computation. Hence, several techniques have been applied such as self-attention only in local neighborhoods \[1\], using local multihead dot product self-attention blocks to completely replace convolutions \[2\]\[3\]\[4\], postprocessing CNN outputs using self- attention \[5\]\[6\], etc. Although shown promising results, these techniques quite hard to be scaled and requires complex engineering to be implemented efficiently on hardware accelerators.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>On the other hand, Transformers model is based on MLP networks, it has more computational efficiency and scalability, making its possible to train big models with over 100B parameters.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Methods
</span></span><span class=line><span class=cl>-------
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>![<span class=nt>Image 4</span>](<span class=na>https://miro.medium.com/v2/resize:fit:700/1*-HQPfbnebarylP543i58_Q.png</span>)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>General architecture of ViT. Taken from the original paper (Dosovitskiy et al., 2021)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>The original Transformers model treat its input as sequences which very different approach with CNN, hence the inputted images need to be extracted into fixed-size patches and flattened. Similar to BERT \[CLS\] token, the so-called <span class=ge>_classification token_</span> will be added into the beginning of the sequences, which will serve as image representation and later will be fed into classification head. Finally, to retain the positional information of the sequences, positional embedding will be added to each patch.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>The authors designed model following the original Transformers as close as possible. The proposed model then called as Vision Transfomers (ViT).
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>... (We trimmed the output)
</span></span></code></pre></div><p>Please also pay attention, for some website this technique might not works very well. That probably caused by firewall or cloudflare protection. You may use the proxy to mitigate it.</p><p>Also, there are a lot more options provided by Jina AI Reader. You may find it out here <a href=https://jina.ai/reader/#apiform>https://jina.ai/reader/#apiform</a>.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Maintaining document structures can ensure the quality of LLM response when it getting asked about our documents. Therefore, choosing the right tools is essential. PyMuPDF4LLM ensure the parsed document output is given in markdown format, which is great in maintaining the document structures.</p><p>On the other side, Langchain implementation is easy to use. Unfortunately, it lack of ability to preserve document structure. One may extend Langchain API to create a PyMuPDF4LLM integration. So that can take benefit from both sides.</p><h1></h1><h1></h1><p>Should you have other opinions or feedbacks, please never hesitate to comment below!</p><hr><h2 id=lets-get-connected->Letâ€™s get Connected ðŸ™Œ<a hidden class=anchor aria-hidden=true href=#lets-get-connected->#</a></h2><p>If you have any inquiries, comments, suggestions, or critics please donâ€™t hesitate to reach me out:</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://fhrzn.github.io/posts/the-secret-sauce-behind-better-product-recommendation-mmr-explained/><span class=title>Â« Prev</span><br><span>The Secret Sauce Behind Better Product Recommendation: MMR Explained</span>
</a><a class=next href=https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/><span class=title>Next Â»</span><br><span>Building Conversational AI with LangChain Part 2: Chat with Your Data</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know on x" href="https://x.com/intent/tweet/?text=Enhance%20Your%20LLM%e2%80%99s%20Understanding%3a%20Document%20Parsing%20Tips%20You%20Need%20to%20Know&amp;url=https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f&amp;title=Enhance%20Your%20LLM%e2%80%99s%20Understanding%3a%20Document%20Parsing%20Tips%20You%20Need%20to%20Know&amp;summary=Enhance%20Your%20LLM%e2%80%99s%20Understanding%3a%20Document%20Parsing%20Tips%20You%20Need%20to%20Know&amp;source=https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f&title=Enhance%20Your%20LLM%e2%80%99s%20Understanding%3a%20Document%20Parsing%20Tips%20You%20Need%20to%20Know"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know on whatsapp" href="https://api.whatsapp.com/send?text=Enhance%20Your%20LLM%e2%80%99s%20Understanding%3a%20Document%20Parsing%20Tips%20You%20Need%20to%20Know%20-%20https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know on telegram" href="https://telegram.me/share/url?text=Enhance%20Your%20LLM%e2%80%99s%20Understanding%3a%20Document%20Parsing%20Tips%20You%20Need%20to%20Know&amp;url=https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know on ycombinator" href="https://news.ycombinator.com/submitlink?t=Enhance%20Your%20LLM%e2%80%99s%20Understanding%3a%20Document%20Parsing%20Tips%20You%20Need%20to%20Know&u=https%3a%2f%2ffhrzn.github.io%2fposts%2fllm-document-parsing-tips%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://fhrzn.github.io/>fahrizain</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>