<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know | fahrizain</title>
<meta name=keywords content="jina ai,document parsing,llm understanding,pdf,website"><meta name=description content="TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding."><meta name=author content="Affandy Fahrizain"><link rel=canonical href=https://fhrzn.github.io/posts/llm-document-parsing-tips/><link crossorigin=anonymous href=/assets/css/stylesheet.f0568d4df87da526a07cdd5f492b4a146e3fa93d5ee950200eaafb2bb50d6fd8.css integrity="sha256-8FaNTfh9pSagfN1fSStKFG4/qT1e6VAgDqr7K7UNb9g=" rel="preload stylesheet" as=style><link rel=icon href=https://fhrzn.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fhrzn.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fhrzn.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fhrzn.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fhrzn.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:title" content="Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know"><meta property="og:description" content="TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding."><meta property="og:type" content="article"><meta property="og:url" content="https://fhrzn.github.io/posts/llm-document-parsing-tips/"><meta property="og:image" content="https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-26T19:56:54+07:00"><meta property="article:modified_time" content="2024-10-26T19:56:54+07:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png"><meta name=twitter:title content="Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know"><meta name=twitter:description content="TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding."><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fhrzn.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know","item":"https://fhrzn.github.io/posts/llm-document-parsing-tips/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know","name":"Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know","description":"TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding.","keywords":["jina ai","document parsing","llm understanding","pdf","website"],"articleBody":"TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding.\nDocument Parsing In RAG, it is very common to upload our documents (i.e. PDF, spreadsheet, docs, or website) to the vector database so we can augment the LLM knowledge from it. One integral step is parsing the document itself, which the result must represent the document in the best way.\nThe easiest way to do that, is just read the file and get the contents. However, it is very possible to make the content losing it structures. Gratefully, we can mitigate this by parsing the document into structured format â€“ Markdown. To the best of my experience, most LLMs are work best with Markdown format â€“ especially OpenAIâ€™s family.\nPDF To parse PDF file into Markdown, we can leverage the extended version of PyMuPDF library, called PyMuPDF4LLM.\nThis library is designed specifically to produce the best output that works well with LLM. Without further ado, letâ€™s see how we can use it to parse PDF document.\nFirst, we need to install the library itself.\npip install pymupdf4llm We will use these sample PDFs from https://www.princexml.com/samples/. Optionally, you may also use your own PDF file.\nFortunately, it is very easy to convert it into markdown using pymupdf4llm. In this article we will only work with one of them. You can find the rest implementation on GitHub repo.\nimport pymupdf4llm # these files are downloaded from link above filepath = { \"invoice1\": \"assets/invoice.pdf\", \"invoice2\": \"assets/invoice2.pdf\", \"brochure\": \"assets/brochure.pdf\", \"newsletter\": \"assets/newsletter.pdf\", \"textbook\": \"assets/textbook.pdf\", } newsletter = pymupdf4llm.to_markdown(filepath[\"newsletter\"], show_progress=False) print(newsletter) Here is the parsed document output:\n# DrylabNews #### for investors \u0026 friends Â· May 2017 Welcome to our first newsletter of 2017! It's been a while since the last one, and a lot has happened. We promise to keep them coming every two months hereafter, and permit ourselves to make this one rather long. The big news is the beginnings of our launch in the American market, but there are also interesting updates on sales, development, mentors and (of course) the investment round that closed in January. **New capital: The investment round was** successful. We raised 2.13 MNOK to match the 2.05 MNOK loan from Innovation Norway. Including the development agreement with Filmlance International, the total new capital is 5 MNOK, partly tied to the successful completion of milestones. All formalities associated with this process are now finalized. **New owners: We would especially like to** warmly welcome our new owners to the Drylab family: Unni Jacobsen, Torstein Jahr, Suzanne Bolstad, Eivind Bergene, Turid Brun, Vigdis Trondsen, Lea Blindheim, Kristine ## 34 ### meetingsmeetings NY Â· SFNY Â· SF LA Â· LLA Â· LVV Academy of Motion Picture Arts and Sciences Â· Alesha \u0026 Jamie Metzger Â· Amazon AWS Â· Apple Â· Caitlin Burns, PGA Â· Carlos Melcer Â· Chimney L.A. Â· Dado Valentic Â· Dave Stump Â· DIT WIT Â· ERA NYC Â· Facebook Â· Fancy Film Â· FilmLight Â· Geo Labelle Â· Google Â· IBM Â· Innovation Norway (NYC) Â· Innovation Norway (SF) Â· International Cinematographers Guild Â· NBC Â· Local 871 Â· Netflix Â· Pomfort Â· Radiant Images Â· Screening Room Â· Signiant Â· Moods of Norway Â· Tapad Â· Team Downey ----- Holmsen, Torstein Hansen, and Jostein Aanensen. We look forward to working with you! **Sales: Return customer rate is now 80%,** proving value and willingness to pay. Film Factory Montreal is our first customer in Canada. Lumiere Numeriques have started using us in France. We also have new customers in Norway, and high-profile users such as Gareth Unwin, producer of Oscar[winning The King's Speech. Revenue for the](http://www.imdb.com/title/tt1504320/) first four months is 200 kNOK, compared to 339 kNOK for all of 2016. We are working on a partnership to safeguard sales in Norway while beginning to focus more on the US. ... (We trimmed the output) From the result above, we can see there is ----- token denoting different pages.\nAlthough, the parsed result is not perfect, the outputâ€™s structure is good enough as it also maintain separation of each document parts.\nAs comparison, we shall look into Langchain PyPDFLoader implementation.\nðŸ’¡ Make sure you already install langchain on your machine.\nfrom langchain_community.document_loaders import PyPDFLoader lc_newsletter = \"\\n==================\\n\".join(doc.page_content for doc in list(PyPDFLoader(filepath[\"newsletter\"]).lazy_load())) print(lc_newsletter) ðŸ’¡ Note that langchain document loaderâ€™s implementation always return list of langchain Document object â€“ each page represented by an object. Therefore we join them with separator token ================== to denote different pages.\nHere is the result of Langchain PyPDFLoader:\nDrylab Newsfor in vestors \u0026 friends Â· Ma y 2017 Welcome to our first newsletter of 2017! It's been a while since the last one, and a lot has happened. W e promise to k eep them coming every two months hereafter , and permit ourselv es to mak e this one r ather long. The big news is the beginnings of our launch in the American mark et, but there are also interesting updates on sales, de velopment, mentors and ( of course ) the in vestment round that closed in January . New c apital: The in vestment round was successful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation Norwa y. Including the de velopment agreement with Filmlance International, the total new capital is 5 MNOK, partly tied to the successful completion of milestones. All formalities associated with this process are now finalized. New o wners: We would especially lik e to warmly welcome our new owners to the Drylab family: Unni Jacobsen, T orstein Jahr , Suzanne Bolstad, Eivind Bergene, T urid Brun, Vigdis T rondsen, L ea Blindheim, Kristine 34meetingsmeetings NY Â· SFNY Â· SF LA Â· LLA Â· L VVAcadem yofMotion Picture Arts and Sciences Â·Alesha \u0026Jamie Metzger Â·Amazon AWS Â·Apple Â·Caitlin Burns, PGA Â·Carlos Melcer Â·Chimne yL.A.Â·Dado Valentic Â· DaveStump Â·DIT WIT Â·ERA NYCÂ·Facebook Â·Fancy Film Â·FilmLight Â·Geo Labelle Â· Google Â·IBM Â·Inno vation Norwa y(NY C)Â·Inno vation Norwa y(SF) Â·International Cinematogr aphers Guild Â·NBC Â·Local 871 Â·Netflix Â·Pomfort Â·Radiant Images Â· Screening Room Â· Signiant Â· Moods of Norwa yÂ· Tapad Â· Team Downe y ================== Holmsen, T orstein Hansen, and Jostein Aanensen. W e look forward to working with you! Sales: Return customer r ate is now 80%, pro ving value and willingness to pa y. Film Factory Montreal is our first customer in Canada. Lumiere Numeriques ha ve started using us in F rance. W e also ha ve new customers in Norwa y, and high-profile users such as Gareth Un win, producer of Oscar- winning The King's Speech . Re venue for the first four months is 200 kNOK, compared to 339 kNOK for all of 2016. W e are working on a partnership to safeguard sales in Norwa y while beginning to focus more on the US. Pay attention the output documents structure are not preserved, making its hard to identify which part originally belongs to. In addition, if we take a closer look, some captured words are strangely separated by random whitespace.\nAlthough the LLM may still can understand it, I believe giving better input representation will produce better output as well. Therefore, parsing into markdown format is a good choice to enhance LLM understanding of our document.\nNow we already know how to parse PDF into markdown format. What if I am telling you that you can do the same to webpages?\nLetâ€™s look how to do it.\nWebpage To parse webpage into Markdown, we can utilize Jina AI Reader API.\nJina AI give an API key we can use for the first 1M token processed. Once it reaches the limit, we need to top up if we are intended to use the API key.\nAfraid not, we can still use their service (for now) even without the API key. Simply do the request with omitting the API key. In this example, we will try to omit the API key.\nðŸš§ Please note the rate limiter is very tight when we are not using the API key. Make sure we are doing graceful requests, otherwise we will get Error 429 â€“ Too many requests\nTo use it, we only need to perform GET request to their endpoint.\nSkip it if you have installed it already\npip install requests We will try to parse one of my article Exploring Vision Transformers (ViT) with ðŸ¤— Huggingface. You may also change it into any other websites.\nimport requests import os BASE_URL = \"https://r.jina.ai\" site = \"https://fhrzn.github.io/posts/building-conversational-ai-context-aware-chatbot/\" url = os.path.join(BASE_URL, site) resp = requests.get(url) print(resp.text) And here is the parsed result:\nTitle: Vision Transformers (ViT) with ðŸ¤— Huggingface | Data Folks Indonesia URL Source: https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0 Published Time: 2022-10-14T11:00:46.983Z Markdown Content: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021) ----------------------------------------------------------------------------------------------------- [![Image 1: Affandy Fahrizain](https://miro.medium.com/v2/resize:fill:88:88/1*JCueIcAZjfbCE_ro4ZB8Og.jpeg)](https://medium.com/@fahrizain?source=post_page-----8cdda82920a0--------------------------------)[![Image 2: Data Folks Indonesia](https://miro.medium.com/v2/resize:fill:48:48/1*s8T4-0fscxMhh6V8adR4mQ.png)](https://medium.com/data-folks-indonesia?source=post_page-----8cdda82920a0--------------------------------) Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration! ![Image 3](https://miro.medium.com/v2/resize:fit:700/0*et8V-t6bjFm1w6ds) Photo by [Alex Litvin](https://unsplash.com/@alexlitvin?utm_source=medium\u0026utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium\u0026utm_medium=referral) Background ---------- As self-attention based model like Transformers has successfully become a _standard_ in NLP area, it triggers researchers to adapt attention-based models in Computer Vision too. There were different evidences, such as combine CNN with self-attention and completely replace Convolutions. While this selected paper belongs to the latter aproach. The application of attention mechanism in images requires each pixel attends to every other pixel, which indeed requires expensive computation. Hence, several techniques have been applied such as self-attention only in local neighborhoods \\[1\\], using local multihead dot product self-attention blocks to completely replace convolutions \\[2\\]\\[3\\]\\[4\\], postprocessing CNN outputs using self- attention \\[5\\]\\[6\\], etc. Although shown promising results, these techniques quite hard to be scaled and requires complex engineering to be implemented efficiently on hardware accelerators. On the other hand, Transformers model is based on MLP networks, it has more computational efficiency and scalability, making its possible to train big models with over 100B parameters. Methods ------- ![Image 4](https://miro.medium.com/v2/resize:fit:700/1*-HQPfbnebarylP543i58_Q.png) General architecture of ViT. Taken from the original paper (Dosovitskiy et al., 2021) The original Transformers model treat its input as sequences which very different approach with CNN, hence the inputted images need to be extracted into fixed-size patches and flattened. Similar to BERT \\[CLS\\] token, the so-called _classification token_ will be added into the beginning of the sequences, which will serve as image representation and later will be fed into classification head. Finally, to retain the positional information of the sequences, positional embedding will be added to each patch. The authors designed model following the original Transformers as close as possible. The proposed model then called as Vision Transfomers (ViT). ... (We trimmed the output) Please also pay attention, for some website this technique might not works very well. That probably caused by firewall or cloudflare protection. You may use the proxy to mitigate it.\nAlso, there are a lot more options provided by Jina AI Reader. You may find it out here https://jina.ai/reader/#apiform.\nConclusion Maintaining document structures can ensure the quality of LLM response when it getting asked about our documents. Therefore, choosing the right tools is essential. PyMuPDF4LLM ensure the parsed document output is given in markdown format, which is great in maintaining the document structures.\nOn the other side, Langchain implementation is easy to use. Unfortunately, it lack of ability to preserve document structure. One may extend Langchain API to create a PyMuPDF4LLM integration. So that can take benefit from both sides.\nShould you have other opinions or feedbacks, please never hesitate to comment below!\nLetâ€™s get Connected ðŸ™Œ If you have any inquiries, comments, suggestions, or critics please donâ€™t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","wordCount":"1973","inLanguage":"en","image":"https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png","datePublished":"2024-10-26T19:56:54+07:00","dateModified":"2024-10-26T19:56:54+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fhrzn.github.io/posts/llm-document-parsing-tips/"},"publisher":{"@type":"Organization","name":"fahrizain","logo":{"@type":"ImageObject","url":"https://fhrzn.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fhrzn.github.io/ accesskey=h title="fahrizain (Alt + H)"><img src=https://fhrzn.github.io/favicon-32x32.png alt aria-label=logo height=30>fahrizain</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Enhance Your LLMâ€™s Understanding: Document Parsing Tips You Need to Know</h1><div class=post-meta><span title='2024-10-26 19:56:54 +0700 WIB'>October 26, 2024</span>&nbsp;Â·&nbsp;10 min&nbsp;Â·&nbsp;1973 words&nbsp;Â·&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager srcset="https://fhrzn.github.io/posts/llm-document-parsing-tips/cover_huc53d7ecd18219c2376ab14843c425463_238683_360x0_resize_box_3.png 360w ,https://fhrzn.github.io/posts/llm-document-parsing-tips/cover_huc53d7ecd18219c2376ab14843c425463_238683_480x0_resize_box_3.png 480w ,https://fhrzn.github.io/posts/llm-document-parsing-tips/cover_huc53d7ecd18219c2376ab14843c425463_238683_720x0_resize_box_3.png 720w ,https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png 1024w" sizes="(min-width: 768px) 720px, 100vw" src=https://fhrzn.github.io/posts/llm-document-parsing-tips/cover.png alt="a newspaper representing RAG's document" width=1024 height=1024><p>This image was generated using Microsoft Copilot</p></figure><div class=post-content><p>TLDR; In this article, we will show how to parse PDF and webpage into Markdown format â€“ which can preserve document structure for better LLM understanding.</p><h3 id=document-parsing>Document Parsing<a hidden class=anchor aria-hidden=true href=#document-parsing>#</a></h3><p>In RAG, it is very common to upload our documents (i.e. PDF, spreadsheet, docs, or website) to the vector database so we can augment the LLM knowledge from it. One integral step is parsing the document itself, which the result must represent the document in the best way.</p><p>The easiest way to do that, is just read the file and get the contents. However, it is very possible to make the content losing it structures. Gratefully, we can mitigate this by parsing the document into structured format â€“ Markdown. To the best of my experience, most LLMs are work best with Markdown format â€“ especially OpenAI&rsquo;s family.</p><h2 id=pdf>PDF<a hidden class=anchor aria-hidden=true href=#pdf>#</a></h2><p>To parse PDF file into Markdown, we can leverage the extended version of PyMuPDF library, called <strong>PyMuPDF4LLM</strong>.</p><p>This library is designed specifically to produce the best output that works well with LLM. Without further ado, let&rsquo;s see how we can use it to parse PDF document.</p><p>First, we need to install the library itself.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install pymupdf4llm
</span></span></code></pre></div><p>We will use these sample PDFs from <a href=https://www.princexml.com/samples/>https://www.princexml.com/samples/</a>. Optionally, you may also use your own PDF file.</p><p>Fortunately, it is very easy to convert it into markdown using pymupdf4llm. In this article we will only work with one of them. You can find the rest implementation on <a href=https://github.com/fhrzn/rags-archive>GitHub repo</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pymupdf4llm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># these files are downloaded from link above</span>
</span></span><span style=display:flex><span>filepath <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;invoice1&#34;</span>: <span style=color:#e6db74>&#34;assets/invoice.pdf&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;invoice2&#34;</span>: <span style=color:#e6db74>&#34;assets/invoice2.pdf&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;brochure&#34;</span>: <span style=color:#e6db74>&#34;assets/brochure.pdf&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;newsletter&#34;</span>: <span style=color:#e6db74>&#34;assets/newsletter.pdf&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;textbook&#34;</span>: <span style=color:#e6db74>&#34;assets/textbook.pdf&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>newsletter <span style=color:#f92672>=</span> pymupdf4llm<span style=color:#f92672>.</span>to_markdown(filepath[<span style=color:#e6db74>&#34;newsletter&#34;</span>], show_progress<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>print(newsletter)
</span></span></code></pre></div><p>Here is the parsed document output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># DrylabNews
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#### for investors &amp; friends Â· May 2017
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Welcome to our first newsletter of 2017! It&#39;s
</span></span><span style=display:flex><span>been a while since the last one, and a lot has
</span></span><span style=display:flex><span>happened. We promise to keep them coming
</span></span><span style=display:flex><span>every two months hereafter, and permit
</span></span><span style=display:flex><span>ourselves to make this one rather long. The
</span></span><span style=display:flex><span>big news is the beginnings of our launch in
</span></span><span style=display:flex><span>the American market, but there are also
</span></span><span style=display:flex><span>interesting updates on sales, development,
</span></span><span style=display:flex><span>mentors and (of course) the investment
</span></span><span style=display:flex><span>round that closed in January.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>**New capital: The investment round was**</span>
</span></span><span style=display:flex><span>successful. We raised 2.13 MNOK to match
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>the 2.05 MNOK loan from Innovation
</span></span><span style=display:flex><span>Norway. Including the development
</span></span><span style=display:flex><span>agreement with Filmlance International, the
</span></span><span style=display:flex><span>total new capital is 5 MNOK, partly tied to
</span></span><span style=display:flex><span>the successful completion of milestones. All
</span></span><span style=display:flex><span>formalities associated with this process are
</span></span><span style=display:flex><span>now finalized.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>**New owners: We would especially like to**</span>
</span></span><span style=display:flex><span>warmly welcome our new owners to the
</span></span><span style=display:flex><span>Drylab family: Unni Jacobsen, Torstein Jahr,
</span></span><span style=display:flex><span>Suzanne Bolstad, Eivind Bergene, Turid Brun,
</span></span><span style=display:flex><span>Vigdis Trondsen, Lea Blindheim, Kristine
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 34
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>### meetingsmeetings
</span></span></span><span style=display:flex><span><span style=color:#75715e></span> NY Â· SFNY Â· SF
</span></span><span style=display:flex><span> LA Â· LLA Â· LVV
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Academy of Motion Picture Arts and Sciences Â· Alesha <span style=color:#960050;background-color:#1e0010>&amp;</span> Jamie Metzger Â· Amazon
</span></span><span style=display:flex><span>AWS Â· Apple Â· Caitlin Burns, PGA Â· Carlos Melcer Â· Chimney L.A. Â· Dado Valentic Â·
</span></span><span style=display:flex><span>Dave Stump Â· DIT WIT Â· ERA NYC Â· Facebook Â· Fancy Film Â· FilmLight Â· Geo Labelle Â·
</span></span><span style=display:flex><span>Google Â· IBM Â· Innovation Norway (NYC) Â· Innovation Norway (SF) Â· International
</span></span><span style=display:flex><span>Cinematographers Guild Â· NBC Â· Local 871 Â· Netflix Â· Pomfort Â· Radiant Images Â·
</span></span><span style=display:flex><span>Screening Room Â· Signiant Â· Moods of Norway Â· Tapad Â· Team Downey
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>-----
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Holmsen, Torstein Hansen, and Jostein
</span></span><span style=display:flex><span>Aanensen. We look forward to working with
</span></span><span style=display:flex><span>you!
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>**Sales: Return customer rate is now 80%,**</span>
</span></span><span style=display:flex><span>proving value and willingness to pay. Film
</span></span><span style=display:flex><span>Factory Montreal is our first customer in
</span></span><span style=display:flex><span>Canada. Lumiere Numeriques have started
</span></span><span style=display:flex><span>using us in France. We also have new
</span></span><span style=display:flex><span>customers in Norway, and high-profile users
</span></span><span style=display:flex><span>such as Gareth Unwin, producer of Oscar[winning The King&#39;s Speech. Revenue for the](http://www.imdb.com/title/tt1504320/)
</span></span><span style=display:flex><span>first four months is 200 kNOK, compared to
</span></span><span style=display:flex><span>339 kNOK for all of 2016. We are working
</span></span><span style=display:flex><span>on a partnership to safeguard sales in
</span></span><span style=display:flex><span>Norway while beginning to focus more on
</span></span><span style=display:flex><span>the US.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>... (We trimmed the output)
</span></span></code></pre></div><p>From the result above, we can see there is <code>-----</code> token denoting different pages.</p><p>Although, the parsed result is not perfect, the output&rsquo;s structure is good enough as it also maintain separation of each document parts.</p><h1 id=heading><a hidden class=anchor aria-hidden=true href=#heading>#</a></h1><h1 id=heading-1><a hidden class=anchor aria-hidden=true href=#heading-1>#</a></h1><p>As comparison, we shall look into <strong>Langchain PyPDFLoader</strong> implementation.</p><blockquote><p>ðŸ’¡ Make sure you already install langchain on your machine.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> PyPDFLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lc_newsletter <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>==================</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join(doc<span style=color:#f92672>.</span>page_content <span style=color:#66d9ef>for</span> doc <span style=color:#f92672>in</span> list(PyPDFLoader(filepath[<span style=color:#e6db74>&#34;newsletter&#34;</span>])<span style=color:#f92672>.</span>lazy_load()))
</span></span><span style=display:flex><span>print(lc_newsletter)
</span></span></code></pre></div><blockquote><p>ðŸ’¡ Note that langchain document loader&rsquo;s implementation always return list of langchain <code>Document</code> object â€“ each page represented by an object. Therefore we join them with separator token <code>==================</code> to denote different pages.</p></blockquote><p>Here is the result of Langchain PyPDFLoader:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>Drylab Newsfor in vestors <span style=color:#960050;background-color:#1e0010>&amp;</span> friends Â· Ma y 2017
</span></span><span style=display:flex><span>Welcome to our first newsletter of 2017! It&#39;s
</span></span><span style=display:flex><span>been a while since the last one, and a lot has
</span></span><span style=display:flex><span>happened. W e promise to k eep them coming
</span></span><span style=display:flex><span>every two months hereafter , and permit
</span></span><span style=display:flex><span>ourselv es to mak e this one r ather long. The
</span></span><span style=display:flex><span>big news is the beginnings of our launch in
</span></span><span style=display:flex><span>the American mark et, but there are also
</span></span><span style=display:flex><span>interesting updates on sales, de velopment,
</span></span><span style=display:flex><span>mentors and ( of course ) the in vestment
</span></span><span style=display:flex><span>round that closed in January .
</span></span><span style=display:flex><span>New c apital: The in vestment round was
</span></span><span style=display:flex><span>successful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation
</span></span><span style=display:flex><span>Norwa y. Including the de velopment
</span></span><span style=display:flex><span>agreement with Filmlance International, the
</span></span><span style=display:flex><span>total new capital is 5 MNOK, partly tied to
</span></span><span style=display:flex><span>the successful completion of milestones. All
</span></span><span style=display:flex><span>formalities associated with this process are
</span></span><span style=display:flex><span>now finalized.
</span></span><span style=display:flex><span>New o wners: We would especially lik e to
</span></span><span style=display:flex><span>warmly welcome our new owners to the
</span></span><span style=display:flex><span>Drylab family: Unni Jacobsen, T orstein Jahr ,
</span></span><span style=display:flex><span>Suzanne Bolstad, Eivind Bergene, T urid Brun,
</span></span><span style=display:flex><span>Vigdis T rondsen, L ea Blindheim, Kristine
</span></span><span style=display:flex><span>34meetingsmeetings
</span></span><span style=display:flex><span>NY Â· SFNY Â· SF
</span></span><span style=display:flex><span>LA Â· LLA Â· L VVAcadem yofMotion Picture Arts and Sciences Â·Alesha <span style=color:#960050;background-color:#1e0010>&amp;</span>Jamie Metzger Â·Amazon
</span></span><span style=display:flex><span>AWS Â·Apple Â·Caitlin Burns, PGA Â·Carlos Melcer Â·Chimne yL.A.Â·Dado Valentic Â·
</span></span><span style=display:flex><span>DaveStump Â·DIT WIT Â·ERA NYCÂ·Facebook Â·Fancy Film Â·FilmLight Â·Geo Labelle Â·
</span></span><span style=display:flex><span>Google Â·IBM Â·Inno vation Norwa y(NY C)Â·Inno vation Norwa y(SF) Â·International
</span></span><span style=display:flex><span>Cinematogr aphers Guild Â·NBC Â·Local 871 Â·Netflix Â·Pomfort Â·Radiant Images Â·
</span></span><span style=display:flex><span>Screening Room Â· Signiant Â· Moods of Norwa yÂ· Tapad Â· Team Downe y
</span></span><span style=display:flex><span>==================
</span></span><span style=display:flex><span>Holmsen, T orstein Hansen, and Jostein
</span></span><span style=display:flex><span>Aanensen. W e look forward to working with
</span></span><span style=display:flex><span>you!
</span></span><span style=display:flex><span>Sales: Return customer r ate is now 80%,
</span></span><span style=display:flex><span>pro ving value and willingness to pa y. Film
</span></span><span style=display:flex><span>Factory Montreal is our first customer in
</span></span><span style=display:flex><span>Canada. Lumiere Numeriques ha ve started
</span></span><span style=display:flex><span>using us in F rance. W e also ha ve new
</span></span><span style=display:flex><span>customers in Norwa y, and high-profile users
</span></span><span style=display:flex><span>such as Gareth Un win, producer of Oscar-
</span></span><span style=display:flex><span>winning The King&#39;s Speech . Re venue for the
</span></span><span style=display:flex><span>first four months is 200 kNOK, compared to
</span></span><span style=display:flex><span>339 kNOK for all of 2016. W e are working
</span></span><span style=display:flex><span>on a partnership to safeguard sales in
</span></span><span style=display:flex><span>Norwa y while beginning to focus more on
</span></span><span style=display:flex><span>the US.
</span></span></code></pre></div><p>Pay attention the output documents structure are not preserved, making its hard to identify which part originally belongs to. In addition, if we take a closer look, some captured words are strangely separated by random whitespace.</p><p>Although the LLM may still can understand it, I believe giving better input representation will produce better output as well. Therefore, parsing into markdown format is a good choice to enhance LLM understanding of our document.</p><p>Now we already know how to parse PDF into markdown format. What if I am telling you that you can do the same to webpages?</p><p>Let&rsquo;s look how to do it.</p><h2 id=webpage>Webpage<a hidden class=anchor aria-hidden=true href=#webpage>#</a></h2><p>To parse webpage into Markdown, we can utilize <a href=https://jina.ai/reader/>Jina AI Reader API</a>.</p><p><img loading=lazy src=images/jina.png#center alt="Jina AI Reader API"></p><p>Jina AI give an API key we can use for the first 1M token processed. Once it reaches the limit, we need to top up if we are intended to use the API key.</p><p>Afraid not, we can still use their service (for now) even without the API key. Simply do the request with omitting the API key. In this example, we will try to omit the API key.</p><blockquote><p>ðŸš§ Please note the rate limiter is very tight when we are not using the API key. Make sure we are doing graceful requests, otherwise we will get <strong>Error 429 â€“ Too many requests</strong></p></blockquote><p>To use it, we only need to perform GET request to their endpoint.</p><p>Skip it if you have installed it already</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install requests
</span></span></code></pre></div><p>We will try to parse one of my article <em><a href=https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0>Exploring Vision Transformers (ViT) with ðŸ¤— Huggingface</a></em>. You may also change it into any other websites.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>BASE_URL <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://r.jina.ai&#34;</span>
</span></span><span style=display:flex><span>site <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://fhrzn.github.io/posts/building-conversational-ai-context-aware-chatbot/&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(BASE_URL, site)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>resp <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(resp<span style=color:#f92672>.</span>text)
</span></span></code></pre></div><p>And here is the parsed result:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>Title: Vision Transformers (ViT) with ðŸ¤— Huggingface | Data Folks Indonesia
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>URL Source: https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Published Time: 2022-10-14T11:00:46.983Z
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Markdown Content:
</span></span><span style=display:flex><span>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021)
</span></span><span style=display:flex><span>-----------------------------------------------------------------------------------------------------
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#f92672>![Image 1: Affandy Fahrizain</span>](<span style=color:#a6e22e>https://miro.medium.com/v2/resize:fill:88:88/1*JCueIcAZjfbCE_ro4ZB8Og.jpeg</span>)](https://medium.com/@fahrizain?source=post_page-----8cdda82920a0--------------------------------)[![Image 2: Data Folks Indonesia](https://miro.medium.com/v2/resize:fill:48:48/1*s8T4-0fscxMhh6V8adR4mQ.png)](https://medium.com/data-folks-indonesia?source=post_page-----8cdda82920a0--------------------------------)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration!
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>![<span style=color:#f92672>Image 3</span>](<span style=color:#a6e22e>https://miro.medium.com/v2/resize:fit:700/0*et8V-t6bjFm1w6ds</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Photo by [<span style=color:#f92672>Alex Litvin</span>](<span style=color:#a6e22e>https://unsplash.com/@alexlitvin?utm_source=medium&amp;utm_medium=referral</span>) on [<span style=color:#f92672>Unsplash</span>](<span style=color:#a6e22e>https://unsplash.com/?utm_source=medium&amp;utm_medium=referral</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Background
</span></span><span style=display:flex><span>----------
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>As self-attention based model like Transformers has successfully become a <span style=font-style:italic>_standard_</span> in NLP area, it triggers researchers to adapt attention-based models in Computer Vision too. There were different evidences, such as combine CNN with self-attention and completely replace Convolutions. While this selected paper belongs to the latter aproach.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The application of attention mechanism in images requires each pixel attends to every other pixel, which indeed requires expensive computation. Hence, several techniques have been applied such as self-attention only in local neighborhoods \[1\], using local multihead dot product self-attention blocks to completely replace convolutions \[2\]\[3\]\[4\], postprocessing CNN outputs using self- attention \[5\]\[6\], etc. Although shown promising results, these techniques quite hard to be scaled and requires complex engineering to be implemented efficiently on hardware accelerators.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>On the other hand, Transformers model is based on MLP networks, it has more computational efficiency and scalability, making its possible to train big models with over 100B parameters.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Methods
</span></span><span style=display:flex><span>-------
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>![<span style=color:#f92672>Image 4</span>](<span style=color:#a6e22e>https://miro.medium.com/v2/resize:fit:700/1*-HQPfbnebarylP543i58_Q.png</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>General architecture of ViT. Taken from the original paper (Dosovitskiy et al., 2021)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The original Transformers model treat its input as sequences which very different approach with CNN, hence the inputted images need to be extracted into fixed-size patches and flattened. Similar to BERT \[CLS\] token, the so-called <span style=font-style:italic>_classification token_</span> will be added into the beginning of the sequences, which will serve as image representation and later will be fed into classification head. Finally, to retain the positional information of the sequences, positional embedding will be added to each patch.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The authors designed model following the original Transformers as close as possible. The proposed model then called as Vision Transfomers (ViT).
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>... (We trimmed the output)
</span></span></code></pre></div><p>Please also pay attention, for some website this technique might not works very well. That probably caused by firewall or cloudflare protection. You may use the proxy to mitigate it.</p><p>Also, there are a lot more options provided by Jina AI Reader. You may find it out here <a href=https://jina.ai/reader/#apiform>https://jina.ai/reader/#apiform</a>.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Maintaining document structures can ensure the quality of LLM response when it getting asked about our documents. Therefore, choosing the right tools is essential. PyMuPDF4LLM ensure the parsed document output is given in markdown format, which is great in maintaining the document structures.</p><p>On the other side, Langchain implementation is easy to use. Unfortunately, it lack of ability to preserve document structure. One may extend Langchain API to create a PyMuPDF4LLM integration. So that can take benefit from both sides.</p><h1 id=heading-2><a hidden class=anchor aria-hidden=true href=#heading-2>#</a></h1><h1 id=heading-3><a hidden class=anchor aria-hidden=true href=#heading-3>#</a></h1><p>Should you have other opinions or feedbacks, please never hesitate to comment below!</p><hr><h2 id=lets-get-connected->Letâ€™s get Connected ðŸ™Œ<a hidden class=anchor aria-hidden=true href=#lets-get-connected->#</a></h2><p>If you have any inquiries, comments, suggestions, or critics please donâ€™t hesitate to reach me out:</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/><span class=title>Next Â»</span><br><span>Building Conversational AI with LangChain Part 2: Chat with Your Data</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://fhrzn.github.io/>fahrizain</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>