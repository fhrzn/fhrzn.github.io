<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 | fahrizain</title><meta name=keywords content="RAG,query expansion,multi-turn,information retrieval"><meta name=description content="Building Retrieval-Augmented Generation (RAG) systems sounds straightforward in theory. But after working on several real-world RAG use cases‚ÄîFAQ chatbot builder, image-to-image search, product recommendation engines and voice agents‚ÄîI learned that the reality is far messier.
Rather than covering everything at once, I‚Äôm breaking this down into a short series of posts‚Äîeach one focusing on a specific issue I encountered and how I approached fixing it.
In this first part, we‚Äôll look at one of the most frequent problems: follow-up queries that fail retrieval‚Äîand how query expansion helped solve it."><meta name=author content="Affandy Fahrizain"><link rel=canonical href=https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://fhrzn.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fhrzn.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fhrzn.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fhrzn.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fhrzn.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:url" content="https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/"><meta property="og:site_name" content="fahrizain"><meta property="og:title" content="Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1"><meta property="og:description" content="Building Retrieval-Augmented Generation (RAG) systems sounds straightforward in theory. But after working on several real-world RAG use cases‚ÄîFAQ chatbot builder, image-to-image search, product recommendation engines and voice agents‚ÄîI learned that the reality is far messier.
Rather than covering everything at once, I‚Äôm breaking this down into a short series of posts‚Äîeach one focusing on a specific issue I encountered and how I approached fixing it.
In this first part, we‚Äôll look at one of the most frequent problems: follow-up queries that fail retrieval‚Äîand how query expansion helped solve it."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-31T20:25:21+07:00"><meta property="article:modified_time" content="2025-05-31T20:25:21+07:00"><meta property="article:tag" content="RAG"><meta property="og:image" content="https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/%3Cimage%20path/url%3E"><meta name=twitter:title content="Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1"><meta name=twitter:description content="Building Retrieval-Augmented Generation (RAG) systems sounds straightforward in theory. But after working on several real-world RAG use cases‚ÄîFAQ chatbot builder, image-to-image search, product recommendation engines and voice agents‚ÄîI learned that the reality is far messier.
Rather than covering everything at once, I‚Äôm breaking this down into a short series of posts‚Äîeach one focusing on a specific issue I encountered and how I approached fixing it.
In this first part, we‚Äôll look at one of the most frequent problems: follow-up queries that fail retrieval‚Äîand how query expansion helped solve it."><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fhrzn.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1","item":"https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1","name":"Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1","description":"Building Retrieval-Augmented Generation (RAG) systems sounds straightforward in theory. But after working on several real-world RAG use cases‚ÄîFAQ chatbot builder, image-to-image search, product recommendation engines and voice agents‚ÄîI learned that the reality is far messier.\nRather than covering everything at once, I‚Äôm breaking this down into a short series of posts‚Äîeach one focusing on a specific issue I encountered and how I approached fixing it.\nIn this first part, we‚Äôll look at one of the most frequent problems: follow-up queries that fail retrieval‚Äîand how query expansion helped solve it.\n","keywords":["RAG","query expansion","multi-turn","information retrieval"],"articleBody":"Building Retrieval-Augmented Generation (RAG) systems sounds straightforward in theory. But after working on several real-world RAG use cases‚ÄîFAQ chatbot builder, image-to-image search, product recommendation engines and voice agents‚ÄîI learned that the reality is far messier.\nRather than covering everything at once, I‚Äôm breaking this down into a short series of posts‚Äîeach one focusing on a specific issue I encountered and how I approached fixing it.\nIn this first part, we‚Äôll look at one of the most frequent problems: follow-up queries that fail retrieval‚Äîand how query expansion helped solve it.\nThe Retrieval Doesn‚Äôt Remember ‚Äî and That‚Äôs a Problem With the intelligence of LLM, I‚Äôve seen many end users expect it can handle multi-turn conversations naturally. But in reality, the retriever often fails to understand the context of incoming follow-up question resulting the ‚ÄúI don‚Äôt know‚Äù kind of response (ofc we don‚Äôt want it to produce hallucination responses either).\n‚ÄúBut we did put the conversation history. It should understand the context, no?‚Äù ‚Äì You might asked.\nYes, you‚Äôre absolutely right. We did expose the conversation history to the LLM.\nBut the retriever didn‚Äôt have it. Why?\nBecause the way retriever works is relied on Embedding models which do not take chat history.\nNormally, embedding model will take the input and convert it to a vector embedding. Then, it will be used to get the most similar document in the vector db. IF, there are no similar documents, it will return nothing, hence the LLM don‚Äôt have materials to construct the correct response.\nConsider the following use case.\nAt the first stage, when user ask how many leave do they have, the context is clear enough for both LLM and retriever. Because the ‚Äúleave‚Äù term is explicitly mentioned.\nHowever, at the second stage the context is vague for retriever as typically we will only pass the last message to embedding model. We won‚Äôt pass the whole chat history as it will exceeds the limit tokens.\nContextual Retrieval using Query Expansion üí° Anthropic present better way to handle this case. You can read their article here.\nWe know already the problem lies in the query that is being passed to the embedding. So, rather than simply passing them, we can actually ask the LLM to enhance the query with context based on the chat history.\nQuery expansion allows us to expand the original query with additional information from the chat history by add, inject, or rewrite the query itself. This method is commonly used in Information Retrieval (IR) system since long before the RAG era.\nWe can update our chatbot to the following.\nWe can put both chat history and current user query together, then asked the LLM to enhance the query context given the chat history. The enhanced query now will be helpful for the retriever to get relevant documents, resulting the final correct response.\nAlternatively, if you don‚Äôt want explicitly define the overhead LLM call, you may implement it in a tool call mode.\nWe only need to specify a function for LLM to execute with the parameter description. In this case, we have a retrieve() function with contextual_query as parameter.\nHere‚Äôs how we can implement it using Langchain.\nFirst, we prepare the documents and ingest to vectordb. For demonstration, we will use InmMemoryVectorStore. But in production you can use either Qdrant, ChromaDB, Weaviate, etc.\nfrom langchain_core.vectorstores import InMemoryVectorStore from langchain_core.documents import Document from langchain_openai import OpenAIEmbeddings import os oai = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"text-embedding-3-small\") docs = [ Document(id=1, page_content=\"Employees have 20 leaves per year.\"), Document(id=2, page_content=\"Apply for leave in the HR portal.\"), Document(id=3, page_content=\"Remote work policies are in the handbook.\"), ] # ingest document to vectordb vectorstore = InMemoryVectorStore(oai) await vectorstore.aadd_documents(docs) Then, let‚Äôs define our retrieval function that will be invoked by LLM.\nfrom langchain.tools import tool SIMILARITY_THRESHOLD = 0.5 @tool async def retrieve(contextual_query: str): '''Retrieve relevant document for given query''' docs = [doc.page_content for doc, sim in vectorstore.similarity_search_with_score(contextual_query) if sim \u003e= SIMILARITY_THRESHOLD] return \"\\n\".join(docs) Next, define our chain which consist of chat history, prompt, and chat model. Note that here we put a dummy chat history just to demonstrate the user‚Äôs follow-up question. In real case, the chat history should inferred from actual conversation.\nfrom langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain_core.messages import HumanMessage, ToolMessage, AIMessage, SystemMessage TOOLS = {\"retrieve\": retrieve} system_prompt = '''You are a helpful internal HR assistant. You are given a tool called `retrieve` to search for relevant documents based on a given query. When invoking the tool, always enrich the query with the context based on chat history.''' history = [ SystemMessage(system_prompt), HumanMessage(\"How many leaves do I get?\"), AIMessage(\"\", tool_calls=[{\"id\": \"1\", \"name\": \"retrieve\", \"args\": {\"contextual_query\": \"employee leave entitlements and policies\"}}]), ToolMessage(\"Employees have 20 leaves per year.\", tool_call_id=\"1\"), AIMessage(\"You have 20 days annuallly.\"), (\"human\", \"{query}\") ] prompt = ChatPromptTemplate.from_messages(history) llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2).bind_tools([retrieve]) chain = prompt | llm Notice that we also put an example of how the contextualized query and LLM response should be. This technique also called Few-Shot Prompt i.e. by giving the predefined example to guide the LLM what is the expected response looks like.\nLet‚Äôs try to interpret what is the expanded query looks like.\nres.model_dump() {'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_6G9yXNrxB6Qrz8trICYZLCsz', 'function': {'arguments': '{\"contextual_query\":\"how to apply for leave in the company\"}', 'name': 'retrieve'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 22,} ... }] } As we can see, the contextualized_query argument is contains of our original query and additional context inferred from the chat history.\nFinally, let‚Äôs handle the function invoke and pass the output back to LLM to get final response.\nwhile res.tool_calls: for tcall in res.tool_calls: # execute each function call msg = await TOOLS[tcall['name'].lower()].ainvoke(tcall) # add the response to history history.append(msg) # get LLM response along with function calling output res = chain.invoke(history) history.append(res) print(res.content) To apply for leave, you can do so through the HR portal. Remember, you have 20 leaves available per year. Nice! Our retrieval system now has been succesfully identify user‚Äôs follow-up question and contextually enrich them to retrieve relevant document from vectordb.\n‚ö†Ô∏è Remember there is no single-solution-for-all, this technique is only one of many others. It may not robust on the other case. Though it is worked quite well for most of my cases so far.\nConclusion Effective Retrieval-Augmented Generation (RAG) systems depend on understanding context, especially for follow-up queries. By employing techniques like query expansion, we can enhance the relevancy of responses and ensure smoother multi-turn conversations.\nWhile this method is beneficial, it‚Äôs essential to remember that this approach is not a one-size-fits-all solution and may require adaptation for different contexts.\nLet‚Äôs get Connected üôå If you have any inquiries, comments, suggestions, or critics please don‚Äôt hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","wordCount":"1116","inLanguage":"en","image":"https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/%3Cimage%20path/url%3E","datePublished":"2025-05-31T20:25:21+07:00","dateModified":"2025-05-31T20:25:21+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fhrzn.github.io/posts/common-pitfalls-in-building-rag/"},"publisher":{"@type":"Organization","name":"fahrizain","logo":{"@type":"ImageObject","url":"https://fhrzn.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fhrzn.github.io/ accesskey=h title="fahrizain (Alt + H)"><img src=https://fhrzn.github.io/favicon-32x32.png alt aria-label=logo height=30>fahrizain</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://fhrzn.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://fhrzn.github.io/profile/ title=Profile><span>Profile</span></a></li><li><a href=https://www.linkedin.com/in/fahrizainn/ title=LinkedIn><span>LinkedIn</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1</h1><div class=post-meta><span title='2025-05-31 20:25:21 +0700 WIB'>May 31, 2025</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;1116 words&nbsp;¬∑&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager src=https://fhrzn.github.io/%3Cimage%20path/url%3E alt><figcaption><text></figcaption></figure><div class=post-content><p>Building Retrieval-Augmented Generation (RAG) systems sounds straightforward in theory. But after working on several real-world RAG use cases‚Äî<a href="https://www.youtube.com/watch?v=CFd2MTHoIlA&amp;list=PLy86Ve1I7c3goBsOPRUAMUMoEUpSX4zWd&amp;index=1">FAQ chatbot builder</a>, image-to-image search, <a href=https://wearesocial.com/sg/blog/2025/04/inside-skechers-new-ai-powered-shopping-experience-in-singapore/>product recommendation engines</a> and voice agents‚ÄîI learned that the reality is far messier.</p><p>Rather than covering everything at once, I‚Äôm breaking this down into a short series of posts‚Äîeach one focusing on a specific issue I encountered and how I approached fixing it.</p><p>In this first part, we‚Äôll look at one of the most frequent problems: <em><strong>follow-up queries</strong> that fail retrieval‚Äîand how <strong>query expansion</strong> helped solve it</em>.</p><h2 id=the-retrieval-doesnt-remember--and-thats-a-problem>The Retrieval Doesn‚Äôt Remember ‚Äî and That‚Äôs a Problem<a hidden class=anchor aria-hidden=true href=#the-retrieval-doesnt-remember--and-thats-a-problem>#</a></h2><p>With the intelligence of LLM, I&rsquo;ve seen many end users expect it can handle multi-turn conversations naturally. But in reality, the retriever often fails to understand the context of incoming follow-up question resulting the <em>&ldquo;I don&rsquo;t know&rdquo;</em> kind of response (ofc we don&rsquo;t want it to produce hallucination responses either).</p><p><em>&ldquo;But we did put the conversation history. It should understand the context, no?&rdquo;</em> ‚Äì You might asked.</p><p>Yes, you&rsquo;re absolutely right. We did expose the conversation history to the LLM.</p><p>But the retriever didn&rsquo;t have it. Why?</p><p>Because the way retriever works is relied on Embedding models which do not take chat history.</p><p>Normally, embedding model will take the input and convert it to a vector embedding. Then, it will be used to get the most similar document in the vector db. IF, there are no similar documents, it will return nothing, hence the LLM don&rsquo;t have materials to construct the correct response.</p><p>Consider the following use case.</p><p><img alt="HR FAQ chatbot" loading=lazy src=/posts/common-pitfalls-in-building-rag/images/hr-faq-bot.png></p><p>At the first stage, when user ask how many leave do they have, the context is clear enough for both LLM and retriever. Because the <em>&ldquo;leave&rdquo;</em> term is explicitly mentioned.</p><p>However, at the second stage the context is vague for retriever as typically we will only pass the last message to embedding model. We won&rsquo;t pass the whole chat history as it will exceeds the limit tokens.</p><h2 id=contextual-retrieval-using-query-expansion>Contextual Retrieval using Query Expansion<a hidden class=anchor aria-hidden=true href=#contextual-retrieval-using-query-expansion>#</a></h2><blockquote><p>üí° <em>Anthropic present better way to handle this case. You can read their article <a href=https://www.anthropic.com/news/contextual-retrieval>here</a>.</em></p></blockquote><p>We know already the problem lies in the query that is being passed to the embedding. So, rather than simply passing them, we can actually ask the LLM to enhance the query with context based on the chat history.</p><p>Query expansion allows us to expand the original query with additional information from the chat history by add, inject, or rewrite the query itself. This method is commonly used in Information Retrieval (IR) system since long before the RAG era.</p><p>We can update our chatbot to the following.</p><p><img alt="HR FAQ Chatbot with Query Expansion" loading=lazy src=/posts/common-pitfalls-in-building-rag/images/hr-faq-bot-query-expansion.png></p><p>We can put both chat history and current user query together, then asked the LLM to enhance the query context given the chat history. The enhanced query now will be helpful for the retriever to get relevant documents, resulting the final correct response.</p><p>Alternatively, if you don&rsquo;t want explicitly define the overhead LLM call, you may implement it in a tool call mode.</p><p><img alt="HR FAQ Chatbot with Query Expansion in Tool Calling Mode" loading=lazy src=/posts/common-pitfalls-in-building-rag/images/hr-faq-bot-query-expansion-tool-call.png></p><p>We only need to specify a function for LLM to execute with the parameter description. In this case, we have a <code>retrieve()</code> function with <code>contextual_query</code> as parameter.</p><p>Here&rsquo;s how we can implement it using Langchain.</p><p>First, we prepare the documents and ingest to vectordb. For demonstration, we will use <code>InmMemoryVectorStore</code>. But in production you can use either Qdrant, ChromaDB, Weaviate, etc.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_core.vectorstores</span> <span class=kn>import</span> <span class=n>InMemoryVectorStore</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_core.documents</span> <span class=kn>import</span> <span class=n>Document</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_openai</span> <span class=kn>import</span> <span class=n>OpenAIEmbeddings</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>oai</span> <span class=o>=</span> <span class=n>OpenAIEmbeddings</span><span class=p>(</span><span class=n>api_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;OPENAI_API_KEY&#34;</span><span class=p>),</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;text-embedding-3-small&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>docs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>Document</span><span class=p>(</span><span class=nb>id</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>page_content</span><span class=o>=</span><span class=s2>&#34;Employees have 20 leaves per year.&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>Document</span><span class=p>(</span><span class=nb>id</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>page_content</span><span class=o>=</span><span class=s2>&#34;Apply for leave in the HR portal.&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>Document</span><span class=p>(</span><span class=nb>id</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>page_content</span><span class=o>=</span><span class=s2>&#34;Remote work policies are in the handbook.&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ingest document to vectordb</span>
</span></span><span class=line><span class=cl><span class=n>vectorstore</span> <span class=o>=</span> <span class=n>InMemoryVectorStore</span><span class=p>(</span><span class=n>oai</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>await</span> <span class=n>vectorstore</span><span class=o>.</span><span class=n>aadd_documents</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
</span></span></code></pre></div><br><p>Then, let&rsquo;s define our retrieval function that will be invoked by LLM.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.tools</span> <span class=kn>import</span> <span class=n>tool</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>SIMILARITY_THRESHOLD</span> <span class=o>=</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@tool</span>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>retrieve</span><span class=p>(</span><span class=n>contextual_query</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Retrieve relevant document for given query&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>docs</span> <span class=o>=</span> <span class=p>[</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span> <span class=k>for</span> <span class=n>doc</span><span class=p>,</span> <span class=n>sim</span> <span class=ow>in</span> 
</span></span><span class=line><span class=cl>            <span class=n>vectorstore</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=n>contextual_query</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>sim</span> <span class=o>&gt;=</span> <span class=n>SIMILARITY_THRESHOLD</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
</span></span></code></pre></div><br><p>Next, define our chain which consist of chat history, prompt, and chat model. Note that here we put a dummy chat history just to demonstrate the user&rsquo;s follow-up question. In real case, the chat history should inferred from actual conversation.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_openai</span> <span class=kn>import</span> <span class=n>ChatOpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.prompts</span> <span class=kn>import</span> <span class=n>ChatPromptTemplate</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_core.messages</span> <span class=kn>import</span> <span class=n>HumanMessage</span><span class=p>,</span> <span class=n>ToolMessage</span><span class=p>,</span> <span class=n>AIMessage</span><span class=p>,</span> <span class=n>SystemMessage</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TOOLS</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;retrieve&#34;</span><span class=p>:</span> <span class=n>retrieve</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>system_prompt</span> <span class=o>=</span> <span class=s1>&#39;&#39;&#39;You are a helpful internal HR assistant.
</span></span></span><span class=line><span class=cl><span class=s1>You are given a tool called `retrieve` to search for relevant documents based on a given query.
</span></span></span><span class=line><span class=cl><span class=s1>When invoking the tool, always enrich the query with the context based on chat history.&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>history</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>SystemMessage</span><span class=p>(</span><span class=n>system_prompt</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>HumanMessage</span><span class=p>(</span><span class=s2>&#34;How many leaves do I get?&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>AIMessage</span><span class=p>(</span><span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=n>tool_calls</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;id&#34;</span><span class=p>:</span> <span class=s2>&#34;1&#34;</span><span class=p>,</span> <span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;retrieve&#34;</span><span class=p>,</span> <span class=s2>&#34;args&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;contextual_query&#34;</span><span class=p>:</span> <span class=s2>&#34;employee leave entitlements and policies&#34;</span><span class=p>}}]),</span>
</span></span><span class=line><span class=cl>    <span class=n>ToolMessage</span><span class=p>(</span><span class=s2>&#34;Employees have 20 leaves per year.&#34;</span><span class=p>,</span> <span class=n>tool_call_id</span><span class=o>=</span><span class=s2>&#34;1&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>AIMessage</span><span class=p>(</span><span class=s2>&#34;You have 20 days annuallly.&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;human&#34;</span><span class=p>,</span> <span class=s2>&#34;</span><span class=si>{query}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=n>ChatPromptTemplate</span><span class=o>.</span><span class=n>from_messages</span><span class=p>(</span><span class=n>history</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatOpenAI</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4o-mini&#34;</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mf>0.2</span><span class=p>)</span><span class=o>.</span><span class=n>bind_tools</span><span class=p>([</span><span class=n>retrieve</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>prompt</span> <span class=o>|</span> <span class=n>llm</span>
</span></span></code></pre></div><p>Notice that we also put an example of how the contextualized query and LLM response should be. This technique also called <code>Few-Shot Prompt</code> i.e. by giving the predefined example to guide the LLM what is the expected response looks like.</p><p>Let&rsquo;s try to interpret what is the expanded query looks like.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=n>res</span><span class=o>.</span><span class=n>model_dump</span><span class=p>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>{</span><span class=s1>&#39;content&#39;</span>: <span class=s1>&#39;&#39;</span>,
</span></span><span class=line><span class=cl> <span class=s1>&#39;additional_kwargs&#39;</span>: <span class=o>{</span><span class=s1>&#39;tool_calls&#39;</span>: <span class=o>[{</span><span class=s1>&#39;id&#39;</span>: <span class=s1>&#39;call_6G9yXNrxB6Qrz8trICYZLCsz&#39;</span>,
</span></span><span class=line><span class=cl>    <span class=s1>&#39;function&#39;</span>: <span class=o>{</span><span class=s1>&#39;arguments&#39;</span>: <span class=s1>&#39;{&#34;contextual_query&#34;:&#34;how to apply for leave in the company&#34;}&#39;</span>,
</span></span><span class=line><span class=cl>     <span class=s1>&#39;name&#39;</span>: <span class=s1>&#39;retrieve&#39;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl>    <span class=s1>&#39;type&#39;</span>: <span class=s1>&#39;function&#39;</span><span class=o>}]</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;refusal&#39;</span>: None<span class=o>}</span>,
</span></span><span class=line><span class=cl> <span class=s1>&#39;response_metadata&#39;</span>: <span class=o>{</span><span class=s1>&#39;token_usage&#39;</span>: <span class=o>{</span><span class=s1>&#39;completion_tokens&#39;</span>: 22,<span class=o>}</span>
</span></span><span class=line><span class=cl> ...
</span></span><span class=line><span class=cl> <span class=o>}]</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>As we can see, the <code>contextualized_query</code> argument is contains of our original query and additional context inferred from the chat history.</p><p>Finally, let&rsquo;s handle the function invoke and pass the output back to LLM to get final response.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=k>while</span> <span class=n>res</span><span class=o>.</span><span class=n>tool_calls</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>tcall</span> <span class=ow>in</span> <span class=n>res</span><span class=o>.</span><span class=n>tool_calls</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># execute each function call</span>
</span></span><span class=line><span class=cl>        <span class=n>msg</span> <span class=o>=</span> <span class=k>await</span> <span class=n>TOOLS</span><span class=p>[</span><span class=n>tcall</span><span class=p>[</span><span class=s1>&#39;name&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>lower</span><span class=p>()]</span><span class=o>.</span><span class=n>ainvoke</span><span class=p>(</span><span class=n>tcall</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># add the response to history</span>
</span></span><span class=line><span class=cl>        <span class=n>history</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>msg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># get LLM response along with function calling output</span>
</span></span><span class=line><span class=cl>    <span class=n>res</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=n>history</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>history</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>res</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>res</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>To apply for leave, you can do so through the HR portal.
Remember, you have 20 leaves available per year.
</code></pre><p>Nice! Our retrieval system now has been succesfully identify user&rsquo;s follow-up question and contextually enrich them to retrieve relevant document from vectordb.</p><blockquote><p>‚ö†Ô∏è Remember there is no single-solution-for-all, this technique is only one of many others. It may not robust on the other case. Though it is worked quite well for most of my cases so far.</p></blockquote><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Effective Retrieval-Augmented Generation (RAG) systems depend on understanding context, especially for follow-up queries. By employing techniques like query expansion, we can enhance the relevancy of responses and ensure smoother multi-turn conversations.</p><p>While this method is beneficial, it&rsquo;s essential to remember that this approach is not a one-size-fits-all solution and may require adaptation for different contexts.</p><hr><h3 id=lets-get-connected->Let‚Äôs get Connected üôå<a hidden class=anchor aria-hidden=true href=#lets-get-connected->#</a></h3><p>If you have any inquiries, comments, suggestions, or critics please don‚Äôt hesitate to reach me out:</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://fhrzn.github.io/tags/rag/>RAG</a></li></ul><nav class=paginav><a class=next href=https://fhrzn.github.io/posts/the-secret-sauce-behind-better-product-recommendation-mmr-explained/><span class=title>Next ¬ª</span><br><span>The Secret Sauce Behind Better Product Recommendation: MMR Explained</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 on x" href="https://x.com/intent/tweet/?text=Lesson%20Learned%3a%20Common%20Pitfalls%20in%20Building%20RAG%20%e2%80%93%20Part%201&amp;url=https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f&amp;hashtags=RAG"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f&amp;title=Lesson%20Learned%3a%20Common%20Pitfalls%20in%20Building%20RAG%20%e2%80%93%20Part%201&amp;summary=Lesson%20Learned%3a%20Common%20Pitfalls%20in%20Building%20RAG%20%e2%80%93%20Part%201&amp;source=https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f&title=Lesson%20Learned%3a%20Common%20Pitfalls%20in%20Building%20RAG%20%e2%80%93%20Part%201"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 on whatsapp" href="https://api.whatsapp.com/send?text=Lesson%20Learned%3a%20Common%20Pitfalls%20in%20Building%20RAG%20%e2%80%93%20Part%201%20-%20https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 on telegram" href="https://telegram.me/share/url?text=Lesson%20Learned%3a%20Common%20Pitfalls%20in%20Building%20RAG%20%e2%80%93%20Part%201&amp;url=https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Lesson Learned: Common Pitfalls in Building RAG ‚Äì Part 1 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Lesson%20Learned%3a%20Common%20Pitfalls%20in%20Building%20RAG%20%e2%80%93%20Part%201&u=https%3a%2f%2ffhrzn.github.io%2fposts%2fcommon-pitfalls-in-building-rag%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://fhrzn.github.io/>fahrizain</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>