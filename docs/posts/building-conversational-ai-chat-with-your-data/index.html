<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building Conversational AI with LangChain Part 2: Chat with Your Data | fahrizain</title>
<meta name=keywords content="chatbot,conversational ai,RAG,qa,langchain"><meta name=description content="LLM is powerful, but it cannot answer questions it doesn&rsquo;t know before. Thanks to RAG we can inject some knowledge to extend LLM capability. Now let&rsquo;s build our first RAG!"><meta name=author content="Affandy Fahrizain"><link rel=canonical href=https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/><link crossorigin=anonymous href=/assets/css/stylesheet.f0568d4df87da526a07cdd5f492b4a146e3fa93d5ee950200eaafb2bb50d6fd8.css integrity="sha256-8FaNTfh9pSagfN1fSStKFG4/qT1e6VAgDqr7K7UNb9g=" rel="preload stylesheet" as=style><link rel=icon href=https://fhrzn.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fhrzn.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fhrzn.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fhrzn.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fhrzn.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:title" content="Building Conversational AI with LangChain Part 2: Chat with Your Data"><meta property="og:description" content="LLM is powerful, but it cannot answer questions it doesn&rsquo;t know before. Thanks to RAG we can inject some knowledge to extend LLM capability. Now let&rsquo;s build our first RAG!"><meta property="og:type" content="article"><meta property="og:url" content="https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/"><meta property="og:image" content="https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/%3Cimage%20path/url%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-30T02:14:36+07:00"><meta property="article:modified_time" content="2024-06-30T02:14:36+07:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/%3Cimage%20path/url%3E"><meta name=twitter:title content="Building Conversational AI with LangChain Part 2: Chat with Your Data"><meta name=twitter:description content="LLM is powerful, but it cannot answer questions it doesn&rsquo;t know before. Thanks to RAG we can inject some knowledge to extend LLM capability. Now let&rsquo;s build our first RAG!"><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fhrzn.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Building Conversational AI with LangChain Part 2: Chat with Your Data","item":"https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building Conversational AI with LangChain Part 2: Chat with Your Data","name":"Building Conversational AI with LangChain Part 2: Chat with Your Data","description":"LLM is powerful, but it cannot answer questions it doesn\u0026rsquo;t know before. Thanks to RAG we can inject some knowledge to extend LLM capability. Now let\u0026rsquo;s build our first RAG!","keywords":["chatbot","conversational ai","RAG","qa","langchain"],"articleBody":"While the LLM is powerful already, it is not impossible to extend its capability to answer the question according to our private data (which never exposed to the LLM itself before). In this part, we will explore how to inject the LLM with our data and ask anything about it!\nIn this part, we will extend the code from the first part. So if you haven‚Äôt see it yet, please have a look here.\n‚ö°Ô∏è Additionally, if you want to jump directly to the code, you may access it here.\nLet‚Äôs gooo! üí™\nIntroduction to RAG Before we start, let‚Äôs first talk about what is RAG and why we should know about it.\nSo, RAG stands for Retrieval Augmented Generation is a method which we will use to inject additonal knowledge (our private data) to the LLM. Note that the LLM capability in answering our questions are relied on the knowledge it has been trained for.\nNow, take a look on the diagram below. There are two new components introduced here, namely embedding and vectordb which we will discuss more in the following sections.\nIn short, we retrieve the relevant documents from vectordb according to user input. Then, we add the output to the prompt to inject some knowledge from our private data. So the LLM will able to answer our question based on those data.\nNow, let‚Äôs discuss a bit further about Embedding and VectorDB.\nEmbedding If you are quite new to AI field, you may be wondering what is Embedding actually?\nSimply put, embedding is just another AI model. However, the key difference is it doesn‚Äôt predict labels or next tokens like the way LLM did. Instead, it only give us strange bunch of float numbers called a vector representation.\nHere is the example of embedding result.\nSee? The result is just float numbers with specific size. In the example above, I just show the example of encoded hello world! using OpenAI embedding model. The size generated by each embedding model itself vary, in this case OpenAI embedding model produced a vector sized 1536.\nBut are we really need this embedding?\nWell, in essence the LLM itself do embedding process to understand and generate sentences. Probably I will make another post about it later.\nBut in this case, we always need to encode our text to interact with VectorDB. As its literal name, a Vector Database store vector representation instead of tabular dataset or file-based data like we‚Äôve known in SQL and NoSQL database.\nLet‚Äôs now discuss a little bit about VectorDB itself.\nVector Database Unlike the other databases we know so far, vector database require us to encode the data we want to store to be encoded in a vector representation. And it can be achieved by leveraging an Embedding model. Not just during ingestion process, the query process also require a vector representation as its input.\nYou may be curious, then how the query process works in VectorDB?\nSince we are interacting with vectors, the usage of similarity algorithm become the standard way to retrieve data. Remember that we need to encode our query sentence? The vectordb itself internally will find the closest document according to our query.\nThere are various similarity algorithm can be used in this case. However, the most popular are cosine similarity, L2/euclidean distance, and inner product.\nNow we have covered the RAG concept and its component as well. Let‚Äôs move to coding section!\nCoding Time! ‚òïÔ∏è We will have slight modification where we add vectordb collection options to choose which data we are going to ask to the chatbot. And we will add one more page for our data ingestion to the vectordb.\nLets get started!\nSetup VectorDB Here we will use Milvus lite which a lightweight version of open-source Milvus vector database.\nFirst thing first, install the library:\npip install \"pymilvus\u003e=2.4.2\" üö® This lite version is good only for building demos and prototype. It is not recommended to use it in production environment. Instead, run Milvus locally with Docker or use the cloud version.\nNext, let‚Äôs put all vectordb logic into a file named knowledge.py. Let‚Äôs start by creating function for opening connection to vectordb.\n# knowledge.py from pymilvus import MilvusClient import logging logger = logging.getLogger(__file__) MILVUS = None def init_vectordb(path: Optional[str] = None): if not path: path = \"./milvus.db\" global MILVUS if not MILVUS: logger.info(\"initiating vectordb\") MILVUS = MilvusClient(path) Then, create functions to upload and ingest data, get available collections in database, and perform query.\nfrom typing import Union, List, Optional from pymilvus import model import re from langchain_community.document_loaders import CSVLoader, PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_core.documents import Document import gradio as gr import time def upload_file(collection_name: str, file: Union[str, list[str], None]): start_upload = time.time() re_ptn = r'((\\w+?\\-)+)?\\w+\\.(csv|pdf)' filename = re.search(re_ptn, file).group() extension = filename.split('.')[-1] if extension == 'csv': loader = CSVLoader(file) data = loader.load() elif extension == 'pdf': loader = PyPDFLoader(file) data = loader.load() splitter = RecursiveCharacterTextSplitter() data = splitter.split_documents(data) else: raise NotImplementedError(f\"Loader for {extension} not implemented yet.\") __encode_and_insert(MILVUS, data, collection_name) # re-retrieve all collection_name to update the dropdown collections = get_collections() logger.info(f\"Time elapsed {time.time() - start_upload:.1f}s | Collection name: {collection_name}\") return [ gr.Textbox(value=None), gr.File(value=None), gr.Dropdown(choices=collections, interactive=True, value=collections[0]), gr.Tabs(selected=\"chat\") ] def __encode_and_insert(client: MilvusClient, data: List[Document], collection_name: str): # extract content content = [item.page_content for item in data] # encode content to vector embedding_fn = model.DefaultEmbeddingFunction() vectors = embedding_fn.encode_documents(content) data = [{\"id\": i, \"vector\": vectors[i], **data[i].dict()} for i in range(len(vectors))] client.create_collection( collection_name=collection_name, dimension=embedding_fn.dim, # auto_id=True ) client.insert(collection_name=collection_name, data=data) Here we will only create a file parser for csv and pdf only using LangChain document loaders. Additionally, for pdf file we apply text splitter to split documents into smaller chunks. Other than that, we will throw an error.\nAfter documents parsed, we will encode them using an embedding model. Here we use the default embedding provided by Milvus which refer to all-MiniLM-L6-v2. However, you may use other embeddings such as openai, vertex, sentence-transformer, etc. In addition, you may want to visit Milvus page about embedding here.\nOnce the document encoded into vector representation, all we need to do is pack it with the text we parsed earlier in a list of dictionary. Finally, the create_collection() function will do the rest for us.\nNote that when creating a collection, we need to supply the dimension of vector representation. This value must match with the size of our embedding model.\nIf you are wondering why we are returning gradio components, dont worry about it now because the explanation is up ahead.\nFor now, lets move to functions for getting collections and perform query.\ndef get_collections(): init_vectordb() collections = MILVUS.list_collections() return collections def query(query: str, collection_name: str): query_str = query if isinstance(query, dict) and \"input\" in query: query_str = query[\"input\"] start_query = time.time() embedding_fn = model.DefaultEmbeddingFunction() query_vector = embedding_fn.encode_queries([query_str]) result = MILVUS.search( collection_name=collection_name, data=query_vector, output_fields=[ \"page_content\" ], limit=1000 ) logger.info(f\"Time elapsed: {time.time() - start_query:.1f}s | Query: \\\"{query_str}\\\"\") context_str = \"\" for res in result: for r in res: context_str += r['entity']['page_content'] + \"\\n\" return context_str To retrieve available collections in the database, we use list_collections function from Milvus client. Additionally, we also call init_vectordb to make sure that the Milvus instance is created before getting the collections.\nThen, to perform query we need to put our string query and collection name. As we‚Äôve discussed earlier, our query string needs to be converted into vector representation as well before we passed it to the search function.\nFinally, we concatenate the retrieved document into a single string which then will be passed to the LLM.\nHere is the full version of our knowledge.py file.\nfrom pymilvus import MilvusClient, model from typing import Union, List, Optional import logging import re from langchain_community.document_loaders import CSVLoader, PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_core.documents import Document import gradio as gr import os import time logger = logging.getLogger(__file__) MILVUS = None def init_vectordb(path: Optional[str] = None): if not path: path = os.getenv(\"MILVUS_DB_PATH\") global MILVUS if not MILVUS: logger.info(\"initiating vectordb\") MILVUS = MilvusClient(path) def close_vectordb(): if MILVUS: logger.info(\"closing vectordb\") MILVUS.close() def upload_file(collection_name: str, file: Union[str, list[str], None]): start_upload = time.time() re_ptn = r'((\\w+?\\-)+)?\\w+\\.(csv|pdf|txt)' filename = re.search(re_ptn, file).group() extension = filename.split('.')[-1] if extension == 'csv': loader = CSVLoader(file) data = loader.load() elif extension == 'pdf': loader = PyPDFLoader(file) data = loader.load() splitter = RecursiveCharacterTextSplitter() data = splitter.split_documents(data) else: raise NotImplementedError(f\"Loader for {extension} not implemented yet.\") __encode_and_insert(MILVUS, data, collection_name) # re-retrieve all collection_name to update the dropdown collections = get_collections() logger.info(f\"Time elapsed {time.time() - start_upload:.1f}s | Collection name: {collection_name}\") return [ gr.Textbox(value=None), gr.File(value=None), gr.Dropdown(choices=collections, interactive=True, value=collections[0]), gr.Tabs(selected=\"chat\") ] def __encode_and_insert(client: MilvusClient, data: List[Document], collection_name: str): # extract content content = [item.page_content for item in data] # encode content to vector embedding_fn = model.DefaultEmbeddingFunction() vectors = embedding_fn.encode_documents(content) data = [{\"id\": i, \"vector\": vectors[i], **data[i].dict()} for i in range(len(vectors))] client.create_collection( collection_name=collection_name, dimension=embedding_fn.dim, # auto_id=True ) client.insert(collection_name=collection_name, data=data) def get_collections(): init_vectordb() collections = MILVUS.list_collections() return collections def query(query: str, collection_name: str): query_str = query if isinstance(query, dict) and \"input\" in query: query_str = query[\"input\"] start_query = time.time() embedding_fn = model.DefaultEmbeddingFunction() query_vector = embedding_fn.encode_queries([query_str]) result = MILVUS.search( collection_name=collection_name, data=query_vector, output_fields=[ \"page_content\" ], limit=1000 ) logger.info(f\"Time elapsed: {time.time() - start_query:.1f}s | Query: \\\"{query_str}\\\"\") context_str = \"\" for res in result: for r in res: context_str += r['entity']['page_content'] + \"\\n\" return context_str Great, now let‚Äôs move to the UI!\nUpdate the UI Let‚Äôs create a new section for uploading our file to vectordb, then wrap both chat interface and the new section in a tab interface.\n# main.py import knowledge with gr.Blocks(fill_height=True) as demo: with gr.Tabs() as tabs: with gr.TabItem(\"Chat\", id=\"chat\"): models = get_free_models() collections = knowledge.get_collections() user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) with gr.Row(): model_choice = gr.Dropdown( choices=models, show_label=True, label=\"Model Choice\", interactive=True, value=models[0], ) collection_list = gr.Dropdown( choices=collections, label=\"VectorDB Collection\", interactive=True, value=collections[0] if collections else None ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1, height=600) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids, collection_list], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) with gr.TabItem(\"Knowledge\", id=\"knowledge\"): collection_name = gr.Textbox(label=\"Collection Name\") upfile = gr.File(label=\"Upload File\") submit_file = gr.Button(\"Submit Knowledge\", variant=\"primary\") submit_file.click(knowledge.upload_file, inputs=[collection_name, upfile], outputs=[collection_name, upfile, collection_list, tabs]) In the chat interface part, there are slight changes where we get list of collections then supply it to a dropdown called collection_list.\nAlso, pay attention since we want to pass the collection name we are using for RAG, we need to pass the collection_list dropdown as additional inputs of ChatInterface.\nWhile on the new section (lets call it ingestion section) quite simple, there are only 3 components: a textbox for collection name, an upload file, and a submit button.\nTake a look on our submit button click action, we take both textbox for collection name and upload file as inputs. Then, we define 4 components as the output, there are both textbox collection_name and the upfile, the collection_list dropdown on the chat interface section, and finally the root of Tab interface itself called tabs.\nBriefly, after the upload file is done we want to switch to chat interface tab automatically and populate the latest list of collection_name in the dropdown. And to do that, we need to update components state by putting them as the output of button click action.\nLet‚Äôs recall a little bit to our code which responsible for handling uploaded files in knowledge.py earlier, where we returned 4 gradio components.\n# knowledge.py def upload_file(collection_name: str, file: Union[str, list[str], None]): ... __encode_and_insert(MILVUS, data, collection_name) # re-retrieve all collection_name to update the dropdown collections = get_collections() return [ gr.Textbox(value=None), gr.File(value=None), gr.Dropdown(choices=collections, interactive=True, value=collections[0]), gr.Tabs(selected=\"chat\") ] Look that we always call get_collections() retrieve collections from vectordb after inserting the file to vectordb.\nThere we set both textbox and upload file component values as None, because we want to reset them to initial state i.e. no prefilled text and file.\nFinally, we populate the retrieved collections list to the dropdown and set the selected tabs to chat which is an id of chat interface tab.\nHere is the full code of our main.py\nimport gradio as gr from dotenv import load_dotenv import logging from chatbot import predict_chat import knowledge import httpx import uuid load_dotenv() logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(filename)s:%(lineno)d - %(message)s\") logger = logging.getLogger(__name__) # MILVUS_CLIENT = None def get_free_models(): res = httpx.get(\"https://openrouter.ai/api/v1/models\") if res: res = res.json() models = [item[\"id\"] for item in res[\"data\"] if \"free\" in item[\"id\"]] return sorted(models) with gr.Blocks(fill_height=True) as demo: with gr.Tabs() as tabs: with gr.TabItem(\"Chat\", id=\"chat\"): models = get_free_models() collections = knowledge.get_collections() user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) with gr.Row(): model_choice = gr.Dropdown( choices=models, show_label=True, label=\"Model Choice\", interactive=True, value=models[0], ) collection_list = gr.Dropdown( choices=collections, label=\"VectorDB Collection\", interactive=True, value=collections[0] if collections else None ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1, height=600) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids, collection_list], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) with gr.TabItem(\"Knowledge\", id=\"knowledge\"): collection_name = gr.Textbox(label=\"Collection Name\") upfile = gr.File(label=\"Upload File\") submit_file = gr.Button(\"Submit Knowledge\", variant=\"primary\") submit_file.click(knowledge.upload_file, inputs=[collection_name, upfile], outputs=[collection_name, upfile, collection_list, tabs]) demo.load(lambda: knowledge.get_collections()) if __name__ == \"__main__\": demo.queue() demo.launch() Great! At this point, we should able to play around with upload file functionality. To test it out, you may use the example file I‚Äôve provided here or you may want to use your own file.\nUpdate Chatbot Logic Cool, now just the main part left. Here we will adapt our previous code to perform RAG. Lets goo! üî•\nFirst of all, let‚Äôs update our prompt.\nprompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are an AI assistant that capable to interact with users using friendly tone. \" \"Whenever you think it needed, add some emojis to your response. No need to use hashtags.\" \"\\n\\n\" \"Answer user's query based on the following context:\\n\" \"{context}\" \"\\n---------------\\n\" \"Chat history:\\n\"), MessagesPlaceholder(\"history\"), (\"human\", \"{input}\") ]) The key difference is now we have a new templated variable in our prompt denoted by {context} which later will be filled with retrieved documents from vectordb.\nPlease also note since we need selected collection name from the ui, we need to add it as function parameter as well. Here, our predict_chat has a new parameter collection_name.\ndef predict_chat(message: str, history: list, model_name: str, user_id: str, collection_name: str): ... Then, we will use LangChain‚Äôs Runnable to wrap our retrieval function we created earlier.\nfrom langchain_core.runnables import RunnableLambda import knowledge from functools import partial # runnable for retrieving knowledge query_runnable = RunnableLambda(partial(knowledge.query, collection_name=collection_name)) Here RunnableLambda is useful to wrap the custom function that later will be executed within the chains. In this case, our custom function is the query function inside knowledge.py file.\nEssentially, it accept a function with a single parameter only. But since our query function consist of two parameters, we use partial here to prefill collection_name argument.\nIf you are curious what those each line did, you may debug by invoking each runnable like this:\n# debugging query_runnable = RunnableLambda(partial(knowledge.query, collection_name=collection_name)) _context = query_runnable.invoke(input=message) logger.info(f\"context: {_context}\") \u003e\u003e\u003e context: Product: St. Isaac Cathedral Ticket Category: Trips Qty (Kg): 1 Price (Ruble): 284 Total Price (Ruble): 284 Purchased at: August 1, 2022 2:51 PM (GMT+3) Store: Trips Source Account: Tinkoff Purchase Date: August 1, 2022 6:51 PM (GMT+7) ... As we can see, when we invoke the runnable it will perform query to vectordb and returning the concatenated relevant documents.\nThen, let‚Äôs put our runnable result to chain using RunnablePassThrough.\nchain = ( RunnablePassthrough.assign(context=query_runnable) | prompt | llm ) Shortly, we can think its there to inject our retrieved context from vectordb to the prompt. The name of argument we put there should match with the variable we want to inject in prompt template.\nFinally, the full code will look like this:\n# chatbot.py from langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.runnables.history import RunnableWithMessageHistory from langchain_community.chat_message_histories import SQLChatMessageHistory from langchain.callbacks.tracers import ConsoleCallbackHandler from typing import Optional import logging import os from langchain_core.runnables import RunnablePassthrough, RunnableLambda import knowledge from functools import partial logger = logging.getLogger(__name__) def get_chat_history(session_id: str, limit: Optional[int] = None, **kwargs): if isinstance(session_id, dict) and \"session_id\" in session_id: session_id = session_id[\"session_id\"] chat_history = SQLChatMessageHistory(session_id=session_id, connection_string=\"sqlite:///memory.db\") if limit: chat_history.messages = chat_history.messages[-limit:] return chat_history def predict_chat(message: str, history: list, model_name: str, user_id: str, collection_name: str): prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are an AI assistant that capable to interact with users using friendly tone. \" \"Whenever you think it needed, add some emojis to your response. No need to use hashtags.\" \"\\n\\n\" \"Answer user's query based on the following context:\\n\" \"{context}\" \"\\n---------------\\n\" \"Chat history:\\n\"), MessagesPlaceholder(\"history\"), (\"human\", \"{input}\") ]) # Optionally, you may use this format as well # prompt = ChatPromptTemplate.from_template( # 'You are an AI assistant that capable to interact with users using friendly tone.' # 'Whenever you think it needed, add some emojis to your response. No need to use hashtags.' # '\\n\\n' # 'Answer user\\'s input based on the following context below. If the context doesn\\'t contains' # 'the suitable answers, just say you dont know. Dont make up the answer!\\n' # '{context}' # '\\n---------------\\n' # 'Chat history:\\n' # '{history}' # '\\n---------------\\n' # 'User input: {input}' # ) llm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"), openai_api_base=os.getenv(\"OPENROUTER_BASE\") ) # runnable for retrieving knowledge query_runnable = RunnableLambda(partial(knowledge.query, collection_name=collection_name)) # ##### FOR DEBUGGING ONLY ##### # _context = query_runnable.invoke(input=message) # logger.info(f\"context: {_context}\") chain = ( RunnablePassthrough.assign(context=query_runnable) | prompt | llm ) history_runnable = RunnableWithMessageHistory( chain, get_session_history=get_chat_history, input_messages_key=\"input\", history_messages_key=\"history\" ) ################ #### STREAM #### ################ partial_msg = \"\" for chunk in history_runnable.stream({\"input\": message}, config={\"configurable\": {\"session_id\": user_id}, \"callbacks\": [ConsoleCallbackHandler()]}): partial_msg = partial_msg + chunk.content yield partial_msg ######################## ##### REGULAR CALL ##### ######################## # response = chain.invoke({\"input\": message, \"session_id\": user_id}, config={\"callbacks\": [ConsoleCallbackHandler()]}) # yield response.content The Moment of Truth! Let‚Äôs now test our chatbot!\nHere is subset of the file I‚Äôm going to ask it. Demo: And that‚Äôs it, yeay! ü•≥\nFull Project https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter\nConclusion In this article, we start improving our chatbot from having context-aware conversation into having capability to answer question based on the given data, with still maintain its core function to aware with previous contexes.\nWe also discussed a bit about embeddings, vector database, and why both is needed in RAG system. Although it was only brief explanation and very simplified project, I hope it is useful especially for those who want to deep dive in recent trend of AI Engineering.\nIn the upcoming article there are interesting topics I want to discuss for example tracking our token usages and LLM responses, perform automatic evaluation, and also demonstrating how we can extract information from images using multimodal LLM.\nStay tune! üëã\nReferences How to invoke runnables in parallel Parallel: Formatting inputs and outputs Runnable Lambda: Run custom functions RunnableWithMessageHistory in RAG pipeline Milvus lite Run Milvus lite locally Let‚Äôs get Connected üôå If you like this article or it useful for you please let me know through the comment section below. Should be there any improvement and suggestions for me you can also reach me out here. Cheers! ü•Ç\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","wordCount":"3098","inLanguage":"en","image":"https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/%3Cimage%20path/url%3E","datePublished":"2024-06-30T02:14:36+07:00","dateModified":"2024-06-30T02:14:36+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fhrzn.github.io/posts/building-conversational-ai-chat-with-your-data/"},"publisher":{"@type":"Organization","name":"fahrizain","logo":{"@type":"ImageObject","url":"https://fhrzn.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fhrzn.github.io/ accesskey=h title="fahrizain (Alt + H)"><img src=https://fhrzn.github.io/favicon-32x32.png alt aria-label=logo height=30>fahrizain</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Building Conversational AI with LangChain Part 2: Chat with Your Data</h1><div class=post-meta><span title='2024-06-30 02:14:36 +0700 WIB'>June 30, 2024</span>&nbsp;¬∑&nbsp;15 min&nbsp;¬∑&nbsp;3098 words&nbsp;¬∑&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager src=https://fhrzn.github.io/%3Cimage%20path/url%3E alt><p></p></figure><div class=post-content><p>While the LLM is powerful already, it is not impossible to extend its capability to answer the question according to our private data (which never exposed to the LLM itself before). In this part, we will explore how to inject the LLM with our data and ask anything about it!</p><p><em>In this part, we will extend the code from the first part. So if you haven&rsquo;t see it yet, please have a look <a href=https://fhrzn.github.io/posts/building-conversational-ai-context-aware-chatbot/>here</a>.</em></p><blockquote><p>‚ö°Ô∏è Additionally, if you want to jump directly to the code, you may access it <a href=https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter/talk-with-data>here</a>.</p></blockquote><p>Let&rsquo;s gooo! üí™</p><h2 id=introduction-to-rag>Introduction to RAG<a hidden class=anchor aria-hidden=true href=#introduction-to-rag>#</a></h2><p>Before we start, let&rsquo;s first talk about what is RAG and why we should know about it.</p><p>So, RAG stands for <strong>Retrieval Augmented Generation</strong> is a method which we will use to inject additonal knowledge (our private data) to the LLM. Note that the LLM capability in answering our questions are relied on the knowledge it has been trained for.</p><p>Now, take a look on the diagram below.
<img loading=lazy src=images/rag-arch.png#center alt="RAG architecture"></p><p>There are two new components introduced here, namely embedding and vectordb which we will discuss more in the following sections.</p><p>In short, we retrieve the relevant documents from vectordb according to user input. Then, we add the output to the prompt to inject some knowledge from our private data. So the LLM will able to answer our question based on those data.</p><p>Now, let&rsquo;s discuss a bit further about Embedding and VectorDB.</p><h3 id=embedding>Embedding<a hidden class=anchor aria-hidden=true href=#embedding>#</a></h3><p>If you are quite new to AI field, you may be wondering what is Embedding actually?</p><p>Simply put, embedding is just another AI model. However, the key difference is it doesn&rsquo;t predict labels or next tokens like the way LLM did. Instead, it only give us strange bunch of float numbers called a <strong>vector representation</strong>.</p><p>Here is the example of embedding result.</p><p><img loading=lazy src=images/sample-embedding.png#center alt="Embedding sample"></p><p>See? The result is just float numbers with specific size. In the example above, I just show the example of encoded <code>hello world!</code> using OpenAI embedding model. The size generated by each embedding model itself vary, in this case OpenAI embedding model produced a vector sized <em>1536</em>.</p><p><strong>But are we really need this embedding?</strong></p><p>Well, in essence the LLM itself do embedding process to understand and generate sentences. Probably I will make another post about it later.</p><p>But in this case, we always need to encode our text to interact with VectorDB. As its literal name, a Vector Database store vector representation instead of tabular dataset or file-based data like we&rsquo;ve known in SQL and NoSQL database.</p><p>Let&rsquo;s now discuss a little bit about VectorDB itself.</p><h3 id=vector-database>Vector Database<a hidden class=anchor aria-hidden=true href=#vector-database>#</a></h3><p>Unlike the other databases we know so far, vector database require us to encode the data we want to store to be encoded in a vector representation. And it can be achieved by leveraging an <strong>Embedding model</strong>. Not just during ingestion process, the query process also require a vector representation as its input.</p><p>You may be curious, then how the query process works in VectorDB?</p><p>Since we are interacting with vectors, the usage of similarity algorithm become the standard way to retrieve data. Remember that we need to encode our query sentence? The vectordb itself internally will find the closest document according to our query.</p><p><img loading=lazy src=images/vectordb.png#center alt="vectordb illustration"></p><p>There are various similarity algorithm can be used in this case. However, the most popular are cosine similarity, L2/euclidean distance, and inner product.</p><p>Now we have covered the RAG concept and its component as well. Let&rsquo;s move to coding section!</p><h2 id=coding-time->Coding Time! ‚òïÔ∏è<a hidden class=anchor aria-hidden=true href=#coding-time->#</a></h2><p>We will have slight modification where we add vectordb collection options to choose which data we are going to ask to the chatbot. And we will add one more page for our data ingestion to the vectordb.</p><p><img loading=lazy src=images/overview.gif#center alt="chat interface"></p><p>Lets get started!</p><h3 id=setup-vectordb>Setup VectorDB<a hidden class=anchor aria-hidden=true href=#setup-vectordb>#</a></h3><p>Here we will use Milvus lite which a lightweight version of open-source Milvus vector database.</p><p>First thing first, install the library:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install <span style=color:#e6db74>&#34;pymilvus&gt;=2.4.2&#34;</span>
</span></span></code></pre></div><blockquote><p>üö® <em>This lite version is good only for building demos and prototype. It is not recommended to use it in production environment. Instead, run Milvus locally with Docker or use the cloud version.</em></p></blockquote><p>Next, let&rsquo;s put all vectordb logic into a file named <code>knowledge.py</code>.
Let&rsquo;s start by creating function for opening connection to vectordb.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#75715e># knowledge.py</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pymilvus <span style=color:#f92672>import</span> MilvusClient
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> logging
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logger <span style=color:#f92672>=</span> logging<span style=color:#f92672>.</span>getLogger(__file__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MILVUS <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_vectordb</span>(path: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> path:
</span></span><span style=display:flex><span>        path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;./milvus.db&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> MILVUS
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> MILVUS:
</span></span><span style=display:flex><span>        logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;initiating vectordb&#34;</span>)
</span></span><span style=display:flex><span>        MILVUS <span style=color:#f92672>=</span> MilvusClient(path)
</span></span></code></pre></div><p>Then, create functions to upload and ingest data, get available collections in database, and perform query.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Union, List, Optional
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pymilvus <span style=color:#f92672>import</span> model
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> CSVLoader, PyPDFLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.documents <span style=color:#f92672>import</span> Document
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> gradio <span style=color:#66d9ef>as</span> gr
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>upload_file</span>(collection_name: str, file: Union[str, list[str], <span style=color:#66d9ef>None</span>]):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    start_upload <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    re_ptn <span style=color:#f92672>=</span> <span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;((\w+?\-)+)?\w+\.(csv|pdf)&#39;</span>
</span></span><span style=display:flex><span>    filename <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>search(re_ptn, file)<span style=color:#f92672>.</span>group()
</span></span><span style=display:flex><span>    extension <span style=color:#f92672>=</span> filename<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;.&#39;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> extension <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;csv&#39;</span>:
</span></span><span style=display:flex><span>        loader <span style=color:#f92672>=</span> CSVLoader(file)
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> extension <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;pdf&#39;</span>:
</span></span><span style=display:flex><span>        loader <span style=color:#f92672>=</span> PyPDFLoader(file)
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>        splitter <span style=color:#f92672>=</span> RecursiveCharacterTextSplitter()
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> splitter<span style=color:#f92672>.</span>split_documents(data)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>NotImplementedError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Loader for </span><span style=color:#e6db74>{</span>extension<span style=color:#e6db74>}</span><span style=color:#e6db74> not implemented yet.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __encode_and_insert(MILVUS, data, collection_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># re-retrieve all collection_name to update the dropdown</span>
</span></span><span style=display:flex><span>    collections <span style=color:#f92672>=</span> get_collections()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Time elapsed </span><span style=color:#e6db74>{</span>time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start_upload<span style=color:#e6db74>:</span><span style=color:#e6db74>.1f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>s | Collection name: </span><span style=color:#e6db74>{</span>collection_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Textbox(value<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>File(value<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Dropdown(choices<span style=color:#f92672>=</span>collections, interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, value<span style=color:#f92672>=</span>collections[<span style=color:#ae81ff>0</span>]),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Tabs(selected<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;chat&#34;</span>)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__encode_and_insert</span>(client: MilvusClient, data: List[Document], collection_name: str):
</span></span><span style=display:flex><span>    <span style=color:#75715e># extract content</span>
</span></span><span style=display:flex><span>    content <span style=color:#f92672>=</span> [item<span style=color:#f92672>.</span>page_content <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> data]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># encode content to vector</span>
</span></span><span style=display:flex><span>    embedding_fn <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>DefaultEmbeddingFunction()
</span></span><span style=display:flex><span>    vectors <span style=color:#f92672>=</span> embedding_fn<span style=color:#f92672>.</span>encode_documents(content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> [{<span style=color:#e6db74>&#34;id&#34;</span>: i, <span style=color:#e6db74>&#34;vector&#34;</span>: vectors[i], <span style=color:#f92672>**</span>data[i]<span style=color:#f92672>.</span>dict()} <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(vectors))]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    client<span style=color:#f92672>.</span>create_collection(
</span></span><span style=display:flex><span>        collection_name<span style=color:#f92672>=</span>collection_name,
</span></span><span style=display:flex><span>        dimension<span style=color:#f92672>=</span>embedding_fn<span style=color:#f92672>.</span>dim,
</span></span><span style=display:flex><span>        <span style=color:#75715e># auto_id=True</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    client<span style=color:#f92672>.</span>insert(collection_name<span style=color:#f92672>=</span>collection_name, data<span style=color:#f92672>=</span>data)
</span></span></code></pre></div><p>Here we will only create a file parser for csv and pdf only using LangChain document loaders. Additionally, for pdf file we apply text splitter to split documents into smaller chunks. Other than that, we will throw an error.</p><p>After documents parsed, we will encode them using an embedding model. Here we use the default embedding provided by Milvus which refer to <em>all-MiniLM-L6-v2</em>. However, you may use other embeddings such as openai, vertex, sentence-transformer, etc. In addition, you may want to visit Milvus page about embedding <a href=https://milvus.io/docs/embeddings.md>here</a>.</p><p>Once the document encoded into vector representation, all we need to do is pack it with the text we parsed earlier in a list of dictionary. Finally, the <code>create_collection()</code> function will do the rest for us.</p><blockquote><p><em>Note that when creating a collection, we need to supply the dimension of vector representation. This value must match with the size of our embedding model.</em></p></blockquote><p>If you are wondering why we are returning gradio components, dont worry about it now because the explanation is up ahead.</p><p>For now, lets move to functions for getting collections and perform query.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_collections</span>():
</span></span><span style=display:flex><span>    init_vectordb()
</span></span><span style=display:flex><span>    collections <span style=color:#f92672>=</span> MILVUS<span style=color:#f92672>.</span>list_collections()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> collections
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>query</span>(query: str, collection_name: str):
</span></span><span style=display:flex><span>    query_str <span style=color:#f92672>=</span> query
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> isinstance(query, dict) <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;input&#34;</span> <span style=color:#f92672>in</span> query:
</span></span><span style=display:flex><span>        query_str <span style=color:#f92672>=</span> query[<span style=color:#e6db74>&#34;input&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    start_query <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>    embedding_fn <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>DefaultEmbeddingFunction()
</span></span><span style=display:flex><span>    query_vector <span style=color:#f92672>=</span> embedding_fn<span style=color:#f92672>.</span>encode_queries([query_str])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> MILVUS<span style=color:#f92672>.</span>search(
</span></span><span style=display:flex><span>        collection_name<span style=color:#f92672>=</span>collection_name,
</span></span><span style=display:flex><span>        data<span style=color:#f92672>=</span>query_vector,
</span></span><span style=display:flex><span>        output_fields<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;page_content&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        limit<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Time elapsed: </span><span style=color:#e6db74>{</span>time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start_query<span style=color:#e6db74>:</span><span style=color:#e6db74>.1f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>s | Query: </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>{</span>query_str<span style=color:#e6db74>}</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    context_str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> res <span style=color:#f92672>in</span> result:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> r <span style=color:#f92672>in</span> res:
</span></span><span style=display:flex><span>            context_str <span style=color:#f92672>+=</span> r[<span style=color:#e6db74>&#39;entity&#39;</span>][<span style=color:#e6db74>&#39;page_content&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> context_str
</span></span></code></pre></div><p>To retrieve available collections in the database, we use <code>list_collections</code> function from Milvus client. Additionally, we also call <code>init_vectordb</code> to make sure that the Milvus instance is created before getting the collections.</p><p>Then, to perform query we need to put our string query and collection name. As we&rsquo;ve discussed earlier, our query string needs to be converted into vector representation as well before we passed it to the <code>search</code> function.</p><p>Finally, we concatenate the retrieved document into a single string which then will be passed to the LLM.</p><p>Here is the full version of our <code>knowledge.py</code> file.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> pymilvus <span style=color:#f92672>import</span> MilvusClient, model
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Union, List, Optional
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> logging
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> CSVLoader, PyPDFLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.documents <span style=color:#f92672>import</span> Document
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> gradio <span style=color:#66d9ef>as</span> gr
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logger <span style=color:#f92672>=</span> logging<span style=color:#f92672>.</span>getLogger(__file__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MILVUS <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_vectordb</span>(path: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> path:
</span></span><span style=display:flex><span>        path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;MILVUS_DB_PATH&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> MILVUS
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> MILVUS:
</span></span><span style=display:flex><span>        logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;initiating vectordb&#34;</span>)
</span></span><span style=display:flex><span>        MILVUS <span style=color:#f92672>=</span> MilvusClient(path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>close_vectordb</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> MILVUS:
</span></span><span style=display:flex><span>        logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;closing vectordb&#34;</span>)
</span></span><span style=display:flex><span>        MILVUS<span style=color:#f92672>.</span>close()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>upload_file</span>(collection_name: str, file: Union[str, list[str], <span style=color:#66d9ef>None</span>]):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    start_upload <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    re_ptn <span style=color:#f92672>=</span> <span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;((\w+?\-)+)?\w+\.(csv|pdf|txt)&#39;</span>
</span></span><span style=display:flex><span>    filename <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>search(re_ptn, file)<span style=color:#f92672>.</span>group()
</span></span><span style=display:flex><span>    extension <span style=color:#f92672>=</span> filename<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;.&#39;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> extension <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;csv&#39;</span>:
</span></span><span style=display:flex><span>        loader <span style=color:#f92672>=</span> CSVLoader(file)
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> extension <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;pdf&#39;</span>:
</span></span><span style=display:flex><span>        loader <span style=color:#f92672>=</span> PyPDFLoader(file)
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>        splitter <span style=color:#f92672>=</span> RecursiveCharacterTextSplitter()
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> splitter<span style=color:#f92672>.</span>split_documents(data)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>NotImplementedError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Loader for </span><span style=color:#e6db74>{</span>extension<span style=color:#e6db74>}</span><span style=color:#e6db74> not implemented yet.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __encode_and_insert(MILVUS, data, collection_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># re-retrieve all collection_name to update the dropdown</span>
</span></span><span style=display:flex><span>    collections <span style=color:#f92672>=</span> get_collections()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Time elapsed </span><span style=color:#e6db74>{</span>time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start_upload<span style=color:#e6db74>:</span><span style=color:#e6db74>.1f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>s | Collection name: </span><span style=color:#e6db74>{</span>collection_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Textbox(value<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>File(value<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Dropdown(choices<span style=color:#f92672>=</span>collections, interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, value<span style=color:#f92672>=</span>collections[<span style=color:#ae81ff>0</span>]),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Tabs(selected<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;chat&#34;</span>)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__encode_and_insert</span>(client: MilvusClient, data: List[Document], collection_name: str):
</span></span><span style=display:flex><span>    <span style=color:#75715e># extract content</span>
</span></span><span style=display:flex><span>    content <span style=color:#f92672>=</span> [item<span style=color:#f92672>.</span>page_content <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> data]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># encode content to vector</span>
</span></span><span style=display:flex><span>    embedding_fn <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>DefaultEmbeddingFunction()
</span></span><span style=display:flex><span>    vectors <span style=color:#f92672>=</span> embedding_fn<span style=color:#f92672>.</span>encode_documents(content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> [{<span style=color:#e6db74>&#34;id&#34;</span>: i, <span style=color:#e6db74>&#34;vector&#34;</span>: vectors[i], <span style=color:#f92672>**</span>data[i]<span style=color:#f92672>.</span>dict()} <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(vectors))]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    client<span style=color:#f92672>.</span>create_collection(
</span></span><span style=display:flex><span>        collection_name<span style=color:#f92672>=</span>collection_name,
</span></span><span style=display:flex><span>        dimension<span style=color:#f92672>=</span>embedding_fn<span style=color:#f92672>.</span>dim,
</span></span><span style=display:flex><span>        <span style=color:#75715e># auto_id=True</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    client<span style=color:#f92672>.</span>insert(collection_name<span style=color:#f92672>=</span>collection_name, data<span style=color:#f92672>=</span>data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_collections</span>():
</span></span><span style=display:flex><span>    init_vectordb()
</span></span><span style=display:flex><span>    collections <span style=color:#f92672>=</span> MILVUS<span style=color:#f92672>.</span>list_collections()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> collections
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>query</span>(query: str, collection_name: str):
</span></span><span style=display:flex><span>    query_str <span style=color:#f92672>=</span> query
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> isinstance(query, dict) <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;input&#34;</span> <span style=color:#f92672>in</span> query:
</span></span><span style=display:flex><span>        query_str <span style=color:#f92672>=</span> query[<span style=color:#e6db74>&#34;input&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    start_query <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>    embedding_fn <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>DefaultEmbeddingFunction()
</span></span><span style=display:flex><span>    query_vector <span style=color:#f92672>=</span> embedding_fn<span style=color:#f92672>.</span>encode_queries([query_str])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> MILVUS<span style=color:#f92672>.</span>search(
</span></span><span style=display:flex><span>        collection_name<span style=color:#f92672>=</span>collection_name,
</span></span><span style=display:flex><span>        data<span style=color:#f92672>=</span>query_vector,
</span></span><span style=display:flex><span>        output_fields<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;page_content&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        limit<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Time elapsed: </span><span style=color:#e6db74>{</span>time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> start_query<span style=color:#e6db74>:</span><span style=color:#e6db74>.1f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>s | Query: </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>{</span>query_str<span style=color:#e6db74>}</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    context_str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> res <span style=color:#f92672>in</span> result:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> r <span style=color:#f92672>in</span> res:
</span></span><span style=display:flex><span>            context_str <span style=color:#f92672>+=</span> r[<span style=color:#e6db74>&#39;entity&#39;</span>][<span style=color:#e6db74>&#39;page_content&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> context_str
</span></span></code></pre></div><p>Great, now let&rsquo;s move to the UI!</p><h3 id=update-the-ui>Update the UI<a hidden class=anchor aria-hidden=true href=#update-the-ui>#</a></h3><p>Let&rsquo;s create a new section for uploading our file to vectordb, then wrap both chat interface and the new section in a tab interface.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#75715e># main.py</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> knowledge
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Blocks(fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#66d9ef>as</span> demo:
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Tabs() <span style=color:#66d9ef>as</span> tabs:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>TabItem(<span style=color:#e6db74>&#34;Chat&#34;</span>, id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;chat&#34;</span>):
</span></span><span style=display:flex><span>            models <span style=color:#f92672>=</span> get_free_models()
</span></span><span style=display:flex><span>            collections <span style=color:#f92672>=</span> knowledge<span style=color:#f92672>.</span>get_collections()
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>            user_ids <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Textbox(visible<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, value<span style=color:#f92672>=</span>uuid<span style=color:#f92672>.</span>uuid4())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Row():
</span></span><span style=display:flex><span>                model_choice <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Dropdown(
</span></span><span style=display:flex><span>                    choices<span style=color:#f92672>=</span>models,
</span></span><span style=display:flex><span>                    show_label<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                    label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Model Choice&#34;</span>,
</span></span><span style=display:flex><span>                    interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                    value<span style=color:#f92672>=</span>models[<span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                collection_list <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Dropdown(
</span></span><span style=display:flex><span>                    choices<span style=color:#f92672>=</span>collections,
</span></span><span style=display:flex><span>                    label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;VectorDB Collection&#34;</span>,
</span></span><span style=display:flex><span>                    interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                    value<span style=color:#f92672>=</span>collections[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>if</span> collections <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            chat_window <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Chatbot(bubble_full_width<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, render<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, height<span style=color:#f92672>=</span><span style=color:#ae81ff>600</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            chat <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>ChatInterface(
</span></span><span style=display:flex><span>                predict_chat,
</span></span><span style=display:flex><span>                chatbot<span style=color:#f92672>=</span>chat_window,
</span></span><span style=display:flex><span>                additional_inputs<span style=color:#f92672>=</span>[model_choice, user_ids, collection_list],
</span></span><span style=display:flex><span>                fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                retry_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                undo_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                clear_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>TabItem(<span style=color:#e6db74>&#34;Knowledge&#34;</span>, id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;knowledge&#34;</span>):
</span></span><span style=display:flex><span>            collection_name <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Textbox(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Collection Name&#34;</span>)
</span></span><span style=display:flex><span>            upfile <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>File(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Upload File&#34;</span>)
</span></span><span style=display:flex><span>            submit_file <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Button(<span style=color:#e6db74>&#34;Submit Knowledge&#34;</span>, variant<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;primary&#34;</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>            submit_file<span style=color:#f92672>.</span>click(knowledge<span style=color:#f92672>.</span>upload_file, inputs<span style=color:#f92672>=</span>[collection_name, upfile], outputs<span style=color:#f92672>=</span>[collection_name, upfile, collection_list, tabs])
</span></span></code></pre></div><p>In the chat interface part, there are slight changes where we get list of collections then supply it to a dropdown called <code>collection_list</code>.</p><p>Also, pay attention since we want to pass the collection name we are using for RAG, we need to pass the <code>collection_list</code> dropdown as additional inputs of <code>ChatInterface</code>.</p><p>While on the new section (lets call it ingestion section) quite simple, there are only 3 components: a textbox for collection name, an upload file, and a submit button.</p><p>Take a look on our submit button click action, we take both textbox for collection name and upload file as inputs. Then, we define 4 components as the output, there are both textbox <code>collection_name</code> and the <code>upfile</code>, the <code>collection_list</code> dropdown on the chat interface section, and finally the root of Tab interface itself called <code>tabs</code>.</p><p>Briefly, after the upload file is done we want to switch to chat interface tab automatically and populate the latest list of collection_name in the dropdown. And to do that, we need to update components state by putting them as the output of button click action.</p><p>Let&rsquo;s recall a little bit to our code which responsible for handling uploaded files in <code>knowledge.py</code> earlier, where we returned 4 gradio components.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#75715e># knowledge.py</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>upload_file</span>(collection_name: str, file: Union[str, list[str], <span style=color:#66d9ef>None</span>]):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __encode_and_insert(MILVUS, data, collection_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># re-retrieve all collection_name to update the dropdown</span>
</span></span><span style=display:flex><span>    collections <span style=color:#f92672>=</span> get_collections()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Textbox(value<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>File(value<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Dropdown(choices<span style=color:#f92672>=</span>collections, interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, value<span style=color:#f92672>=</span>collections[<span style=color:#ae81ff>0</span>]),
</span></span><span style=display:flex><span>        gr<span style=color:#f92672>.</span>Tabs(selected<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;chat&#34;</span>)
</span></span><span style=display:flex><span>    ]
</span></span></code></pre></div><p>Look that we always call <code>get_collections()</code> retrieve collections from vectordb after inserting the file to vectordb.</p><p>There we set both textbox and upload file component values as None, because we want to reset them to initial state i.e. no prefilled text and file.</p><p>Finally, we populate the retrieved collections list to the dropdown and set the selected tabs to <code>chat</code> which is an id of chat interface tab.</p><p>Here is the full code of our <code>main.py</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>import</span> gradio <span style=color:#66d9ef>as</span> gr
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> logging
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> chatbot <span style=color:#f92672>import</span> predict_chat
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> knowledge
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> httpx
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> uuid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>load_dotenv()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logging<span style=color:#f92672>.</span>basicConfig(level<span style=color:#f92672>=</span>logging<span style=color:#f92672>.</span>INFO, format<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;[</span><span style=color:#e6db74>%(levelname)s</span><span style=color:#e6db74>] </span><span style=color:#e6db74>%(filename)s</span><span style=color:#e6db74>:</span><span style=color:#e6db74>%(lineno)d</span><span style=color:#e6db74> - </span><span style=color:#e6db74>%(message)s</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>logger <span style=color:#f92672>=</span> logging<span style=color:#f92672>.</span>getLogger(__name__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># MILVUS_CLIENT = None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_free_models</span>():
</span></span><span style=display:flex><span>    res <span style=color:#f92672>=</span> httpx<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;https://openrouter.ai/api/v1/models&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> res:
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>        models <span style=color:#f92672>=</span> [item[<span style=color:#e6db74>&#34;id&#34;</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> res[<span style=color:#e6db74>&#34;data&#34;</span>] <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;free&#34;</span> <span style=color:#f92672>in</span> item[<span style=color:#e6db74>&#34;id&#34;</span>]]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> sorted(models)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Blocks(fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#66d9ef>as</span> demo:
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Tabs() <span style=color:#66d9ef>as</span> tabs:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>TabItem(<span style=color:#e6db74>&#34;Chat&#34;</span>, id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;chat&#34;</span>):
</span></span><span style=display:flex><span>            models <span style=color:#f92672>=</span> get_free_models()
</span></span><span style=display:flex><span>            collections <span style=color:#f92672>=</span> knowledge<span style=color:#f92672>.</span>get_collections()
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>            user_ids <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Textbox(visible<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, value<span style=color:#f92672>=</span>uuid<span style=color:#f92672>.</span>uuid4())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>Row():
</span></span><span style=display:flex><span>                model_choice <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Dropdown(
</span></span><span style=display:flex><span>                    choices<span style=color:#f92672>=</span>models,
</span></span><span style=display:flex><span>                    show_label<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                    label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Model Choice&#34;</span>,
</span></span><span style=display:flex><span>                    interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                    value<span style=color:#f92672>=</span>models[<span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                collection_list <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Dropdown(
</span></span><span style=display:flex><span>                    choices<span style=color:#f92672>=</span>collections,
</span></span><span style=display:flex><span>                    label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;VectorDB Collection&#34;</span>,
</span></span><span style=display:flex><span>                    interactive<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                    value<span style=color:#f92672>=</span>collections[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>if</span> collections <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            chat_window <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Chatbot(bubble_full_width<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, render<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, height<span style=color:#f92672>=</span><span style=color:#ae81ff>600</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            chat <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>ChatInterface(
</span></span><span style=display:flex><span>                predict_chat,
</span></span><span style=display:flex><span>                chatbot<span style=color:#f92672>=</span>chat_window,
</span></span><span style=display:flex><span>                additional_inputs<span style=color:#f92672>=</span>[model_choice, user_ids, collection_list],
</span></span><span style=display:flex><span>                fill_height<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                retry_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                undo_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                clear_btn<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> gr<span style=color:#f92672>.</span>TabItem(<span style=color:#e6db74>&#34;Knowledge&#34;</span>, id<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;knowledge&#34;</span>):
</span></span><span style=display:flex><span>            collection_name <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Textbox(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Collection Name&#34;</span>)
</span></span><span style=display:flex><span>            upfile <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>File(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Upload File&#34;</span>)
</span></span><span style=display:flex><span>            submit_file <span style=color:#f92672>=</span> gr<span style=color:#f92672>.</span>Button(<span style=color:#e6db74>&#34;Submit Knowledge&#34;</span>, variant<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;primary&#34;</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>            submit_file<span style=color:#f92672>.</span>click(knowledge<span style=color:#f92672>.</span>upload_file, inputs<span style=color:#f92672>=</span>[collection_name, upfile], outputs<span style=color:#f92672>=</span>[collection_name, upfile, collection_list, tabs])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>load(<span style=color:#66d9ef>lambda</span>: knowledge<span style=color:#f92672>.</span>get_collections())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>queue()
</span></span><span style=display:flex><span>    demo<span style=color:#f92672>.</span>launch()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    
</span></span></code></pre></div><p>Great! At this point, we should able to play around with upload file functionality. To test it out, you may use the example file I&rsquo;ve provided <a href="https://www.dropbox.com/scl/fi/yf0rresfuso5p59fg0443/expenses_top.csv?rlkey=m1cstcs2agbdxy06fz7nv4r6m&amp;st=bf8w6i6l&amp;dl=1">here</a> or you may want to use your own file.</p><p><img loading=lazy src=images/upload-file-test.gif alt="testing upload functionality"></p><h2 id=update-chatbot-logic>Update Chatbot Logic<a hidden class=anchor aria-hidden=true href=#update-chatbot-logic>#</a></h2><p>Cool, now just the main part left. Here we will adapt our previous code to perform RAG. Lets goo! üî•</p><p>First of all, let&rsquo;s update our prompt.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span>prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_messages([
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;You are an AI assistant that capable to interact with users using friendly tone. &#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Whenever you think it needed, add some emojis to your response. No need to use hashtags.&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Answer user&#39;s query based on the following context:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{context}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>---------------</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Chat history:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>),
</span></span><span style=display:flex><span>    MessagesPlaceholder(<span style=color:#e6db74>&#34;history&#34;</span>),
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#34;human&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{input}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><p>The key difference is now we have a new templated variable in our prompt denoted by <code>{context}</code> which later will be filled with retrieved documents from vectordb.</p><p>Please also note since we need selected collection name from the ui, we need to add it as function parameter as well. Here, our <code>predict_chat</code> has a new parameter <code>collection_name</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>(message: str, history: list, model_name: str, user_id: str, collection_name: str):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span></code></pre></div><p>Then, we will use LangChain&rsquo;s Runnable to wrap our retrieval function we created earlier.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables <span style=color:#f92672>import</span> RunnableLambda
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> knowledge
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> functools <span style=color:#f92672>import</span> partial
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># runnable for retrieving knowledge</span>
</span></span><span style=display:flex><span>query_runnable <span style=color:#f92672>=</span> RunnableLambda(partial(knowledge<span style=color:#f92672>.</span>query, collection_name<span style=color:#f92672>=</span>collection_name))
</span></span></code></pre></div><p>Here <code>RunnableLambda</code> is useful to wrap the custom function that later will be executed within the chains. In this case, our custom function is the <code>query</code> function inside <code>knowledge.py</code> file.</p><p>Essentially, it accept a function with a single parameter only. But since our query function consist of two parameters, we use <code>partial</code> here to prefill <code>collection_name</code> argument.</p><p>If you are curious what those each line did, you may debug by invoking each runnable like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#75715e># debugging</span>
</span></span><span style=display:flex><span>query_runnable <span style=color:#f92672>=</span> RunnableLambda(partial(knowledge<span style=color:#f92672>.</span>query, collection_name<span style=color:#f92672>=</span>collection_name))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>_context <span style=color:#f92672>=</span> query_runnable<span style=color:#f92672>.</span>invoke(input<span style=color:#f92672>=</span>message)
</span></span><span style=display:flex><span>logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;context: </span><span style=color:#e6db74>{</span>_context<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> context: Product: St<span style=color:#f92672>.</span> Isaac Cathedral Ticket
</span></span><span style=display:flex><span>Category: Trips
</span></span><span style=display:flex><span>Qty (Kg): <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>Price (Ruble): <span style=color:#ae81ff>284</span>
</span></span><span style=display:flex><span>Total Price (Ruble): <span style=color:#ae81ff>284</span>
</span></span><span style=display:flex><span>Purchased at: August <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2022</span> <span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>51</span> PM (GMT<span style=color:#f92672>+</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>Store: Trips
</span></span><span style=display:flex><span>Source Account: Tinkoff
</span></span><span style=display:flex><span>Purchase Date: August <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2022</span> <span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>51</span> PM (GMT<span style=color:#f92672>+</span><span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span></code></pre></div><p>As we can see, when we invoke the runnable it will perform query to vectordb and returning the concatenated relevant documents.</p><p>Then, let&rsquo;s put our runnable result to chain using RunnablePassThrough.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span>chain <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    RunnablePassthrough<span style=color:#f92672>.</span>assign(context<span style=color:#f92672>=</span>query_runnable)
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> prompt
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Shortly, we can think its there to inject our retrieved context from vectordb to the prompt. The name of argument we put there should match with the variable we want to inject in prompt template.</p><p>Finally, the full code will look like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python3 data-lang=python3><span style=display:flex><span><span style=color:#75715e># chatbot.py</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_openai <span style=color:#f92672>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate, MessagesPlaceholder
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables.history <span style=color:#f92672>import</span> RunnableWithMessageHistory
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_message_histories <span style=color:#f92672>import</span> SQLChatMessageHistory
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.callbacks.tracers <span style=color:#f92672>import</span> ConsoleCallbackHandler
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Optional
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> logging
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables <span style=color:#f92672>import</span> RunnablePassthrough, RunnableLambda
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> knowledge
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> functools <span style=color:#f92672>import</span> partial
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logger <span style=color:#f92672>=</span> logging<span style=color:#f92672>.</span>getLogger(__name__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_chat_history</span>(session_id: str, limit: Optional[int] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>, <span style=color:#f92672>**</span>kwargs):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> isinstance(session_id, dict) <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;session_id&#34;</span> <span style=color:#f92672>in</span> session_id:
</span></span><span style=display:flex><span>        session_id <span style=color:#f92672>=</span> session_id[<span style=color:#e6db74>&#34;session_id&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat_history <span style=color:#f92672>=</span> SQLChatMessageHistory(session_id<span style=color:#f92672>=</span>session_id, connection_string<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sqlite:///memory.db&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> limit:
</span></span><span style=display:flex><span>        chat_history<span style=color:#f92672>.</span>messages <span style=color:#f92672>=</span> chat_history<span style=color:#f92672>.</span>messages[<span style=color:#f92672>-</span>limit:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> chat_history
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_chat</span>(message: str, history: list, model_name: str, user_id: str, collection_name: str):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_messages([
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;You are an AI assistant that capable to interact with users using friendly tone. &#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;Whenever you think it needed, add some emojis to your response. No need to use hashtags.&#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;Answer user&#39;s query based on the following context:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{context}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>---------------</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;Chat history:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>),
</span></span><span style=display:flex><span>        MessagesPlaceholder(<span style=color:#e6db74>&#34;history&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#e6db74>&#34;human&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{input}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Optionally, you may use this format as well</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># prompt = ChatPromptTemplate.from_template(</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;You are an AI assistant that capable to interact with users using friendly tone.&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;Whenever you think it needed, add some emojis to your response. No need to use hashtags.&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;\n\n&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;Answer user\&#39;s input based on the following context below. If the context doesn\&#39;t contains&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;the suitable answers, just say you dont know. Dont make up the answer!\n&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;{context}&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;\n---------------\n&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;Chat history:\n&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;{history}&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;\n---------------\n&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     &#39;User input: {input}&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># )</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#f92672>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model_name,
</span></span><span style=display:flex><span>        openai_api_key<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_API_KEY&#34;</span>),
</span></span><span style=display:flex><span>        openai_api_base<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;OPENROUTER_BASE&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># runnable for retrieving knowledge</span>
</span></span><span style=display:flex><span>    query_runnable <span style=color:#f92672>=</span> RunnableLambda(partial(knowledge<span style=color:#f92672>.</span>query, collection_name<span style=color:#f92672>=</span>collection_name))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ##### FOR DEBUGGING ONLY #####</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># _context = query_runnable.invoke(input=message)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># logger.info(f&#34;context: {_context}&#34;)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chain <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        RunnablePassthrough<span style=color:#f92672>.</span>assign(context<span style=color:#f92672>=</span>query_runnable)
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> prompt
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    history_runnable <span style=color:#f92672>=</span> RunnableWithMessageHistory(
</span></span><span style=display:flex><span>        chain,
</span></span><span style=display:flex><span>        get_session_history<span style=color:#f92672>=</span>get_chat_history,
</span></span><span style=display:flex><span>        input_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;input&#34;</span>,
</span></span><span style=display:flex><span>        history_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;history&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>################</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#### STREAM ####</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>################</span>
</span></span><span style=display:flex><span>    partial_msg <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> history_runnable<span style=color:#f92672>.</span>stream({<span style=color:#e6db74>&#34;input&#34;</span>: message}, config<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;configurable&#34;</span>: {<span style=color:#e6db74>&#34;session_id&#34;</span>: user_id}, <span style=color:#e6db74>&#34;callbacks&#34;</span>: [ConsoleCallbackHandler()]}):
</span></span><span style=display:flex><span>        partial_msg <span style=color:#f92672>=</span> partial_msg <span style=color:#f92672>+</span> chunk<span style=color:#f92672>.</span>content
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> partial_msg
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>########################</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>##### REGULAR CALL #####</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>########################</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># response = chain.invoke({&#34;input&#34;: message, &#34;session_id&#34;: user_id}, config={&#34;callbacks&#34;: [ConsoleCallbackHandler()]})</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># yield response.content</span>
</span></span></code></pre></div><h2 id=the-moment-of-truth>The Moment of Truth!<a hidden class=anchor aria-hidden=true href=#the-moment-of-truth>#</a></h2><p>Let&rsquo;s now test our chatbot!</p><p>Here is subset of the file I&rsquo;m going to ask it.
<img loading=lazy src=images/data-preview.png#center alt="Subset data I&amp;rsquo;m going to ask"></p><p>Demo:
<img loading=lazy src=images/chat-demo.gif alt="Final chat demo"></p><p>And that&rsquo;s it, yeay! ü•≥</p><h3 id=full-project>Full Project<a hidden class=anchor aria-hidden=true href=#full-project>#</a></h3><p><a href=https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter>https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter</a></p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In this article, we start improving our chatbot from having context-aware conversation into having capability to answer question based on the given data, with still maintain its core function to aware with previous contexes.</p><p>We also discussed a bit about embeddings, vector database, and why both is needed in RAG system. Although it was only brief explanation and very simplified project, I hope it is useful especially for those who want to deep dive in recent trend of AI Engineering.</p><p>In the upcoming article there are interesting topics I want to discuss for example tracking our token usages and LLM responses, perform automatic evaluation, and also demonstrating how we can extract information from images using multimodal LLM.</p><p>Stay tune! üëã</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://python.langchain.com/v0.2/docs/how_to/parallel/>How to invoke runnables in parallel</a></li><li><a href=https://python.langchain.com/v0.1/docs/expression_language/primitives/parallel/>Parallel: Formatting inputs and outputs</a></li><li><a href=https://python.langchain.com/v0.1/docs/expression_language/primitives/functions/>Runnable Lambda: Run custom functions</a></li><li><a href=https://github.com/langchain-ai/langchain/discussions/16582>RunnableWithMessageHistory in RAG pipeline</a></li><li><a href=https://milvus.io/blog/introducing-milvus-lite.md>Milvus lite</a></li><li><a href=https://milvus.io/docs/milvus_lite.md>Run Milvus lite locally</a></li></ol><hr><h2 id=lets-get-connected->Let&rsquo;s get Connected üôå<a hidden class=anchor aria-hidden=true href=#lets-get-connected->#</a></h2><p>If you like this article or it useful for you please let me know through the comment section below. Should be there any improvement and suggestions for me you can also reach me out here. Cheers! ü•Ç</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://fhrzn.github.io/tags/chatbot/>chatbot</a></li><li><a href=https://fhrzn.github.io/tags/conversational-ai/>conversational ai</a></li><li><a href=https://fhrzn.github.io/tags/rag/>RAG</a></li><li><a href=https://fhrzn.github.io/tags/qa/>qa</a></li><li><a href=https://fhrzn.github.io/tags/langchain/>langchain</a></li></ul><nav class=paginav><a class=next href=https://fhrzn.github.io/posts/building-conversational-ai-context-aware-chatbot/><span class=title>Next ¬ª</span><br><span>Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://fhrzn.github.io/>fahrizain</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>