<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Autoencoders: Your First Step into Generative AI | Fahrizen</title>
<meta name=keywords content="deeplearning,generative ai,autoencoder,data compress"><meta name=description content="Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one."><meta name=author content="Affandy Fahrizain"><link rel=canonical href=https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/><link crossorigin=anonymous href=/assets/css/stylesheet.f0568d4df87da526a07cdd5f492b4a146e3fa93d5ee950200eaafb2bb50d6fd8.css integrity="sha256-8FaNTfh9pSagfN1fSStKFG4/qT1e6VAgDqr7K7UNb9g=" rel="preload stylesheet" as=style><link rel=icon href=https://fhrzn.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fhrzn.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fhrzn.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fhrzn.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fhrzn.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:title" content="Autoencoders: Your First Step into Generative AI"><meta property="og:description" content="Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one."><meta property="og:type" content="article"><meta property="og:url" content="https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/"><meta property="og:image" content="https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/cover.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-02T21:00:00+07:00"><meta property="article:modified_time" content="2024-01-02T21:00:00+07:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/cover.jpg"><meta name=twitter:title content="Autoencoders: Your First Step into Generative AI"><meta name=twitter:description content="Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one."><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fhrzn.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Autoencoders: Your First Step into Generative AI","item":"https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Autoencoders: Your First Step into Generative AI","name":"Autoencoders: Your First Step into Generative AI","description":"Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one.","keywords":["deeplearning","generative ai","autoencoder","data compress"],"articleBody":"Generative AI When we discuss about generative models, most of us might be quickly triggered to imagine the greatness of current Large Language Models (LLMs) such as ChatGPT, Bard, Gemini, LLaMA, Mixtral, etc. Or instead, the Text-to-Image models like Dall-e and stable diffusion.\nBasically, the generative model works by trying to produce or generate new data that similar into the sampled one. Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one.\nAutoencoder Network Autoencoder is a neural network architecture that learn representation (encode) the input data, then tries to reconstruct the original data as close as possible by leveraging the learned representation (encoded) data. It is useful for denoising image, reduce dimensionality, or detecting outlier.\nThis network mainly consist of three parts: Encoder network, Decoder network, and Latent Space; each of them play an important role to make the model works. Letâ€™s breakdown each of them below.\nEncoder Network Encoder network is responsible to take an input data, then compress it into smaller representation of data (latent space). The layer size of this network usually shrinks as it closer to the last layer that produce the latent space. It is inline with our human intuition where compress means to make things smaller, and in this case, it is works by passing the input data to hidden layers which shrinks at each stage.\nLatent Space Latent space is the encoder outputs, which we can also call it learned representation. It is actually the collection of vectors just like the input data, but in smaller dimension. In my opinion, the latent space has smaller dimension as it is only keep the important parts of the input data. These important parts were selected and evaluated in every step of forward propagation in the encoder network, hence producing such smaller representation.\nThis smaller representation later will be consumed as input for the decoder network.\nDecoder Network In contrast to the encoder network, decoder network is responsible to reconstruct (decode) the learned representation to be as close as possible with the original input data. Instead of using the original input data, this network use the latent space as its input. It also means the decoder network forced to generate new data based on those representation (which hopefully representing the input data enough). Finally, in each training step the generated data will be compared to the original one until it is already close enough.\nLoss Function As we mentioned earlier, the generated data needs to be compared with the original one to ensure it closeness. To do that, we need to define specific loss function which can be different for each field (e.g. image, text, audio). In this case, we will use image data for our discussion.\nNow, to measure the closeness between generated and original images, we can employ MSE Loss below.\n$$ MSE = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 $$\nwhere:\n$N:$ is the total samples\n$y_i:$ is the original input data\n$\\hat{y_i}:$ is the reconstructed data\nApplication of Autoencoder Dimensionality Reduction: similar to PCA (Principal Component Analysis) which useful for visualization purpose. Feature Extraction: generate the compressed latent representation to be used for the downstream tasks such as classification or prediction. The base model of BERT, GPT2, and its family is the examples of this application. File Compression: by reducing the data dimensionality we are able to reconstruct data with smaller size (but also risk some data quality). Image Denoising: remove the noise of the image that might be produced by high ISO or corrupted image. In order to do that, we must train the model to learn the difference between the clean image and the noisy one. Once trained, it expected able to reconstruct image with less noise. Image Generator: able to generate or modify images that are similar to input data. It used the variant of autoencoder named Variational Autoencoder (VAE). It is also useful for data augmentation. Coding Time! In this article, we will try to make our very first autoencoder. We will start with the simple one using Linear layers to compress the EMNIST letter images.\nIf you are prefer to jump ahead into the notebook, please visit this link.\nDataset Preparation First of all, letâ€™s import and create config for our training purpose.\n# data manipulation import numpy as np import matplotlib.pyplot as plt import pandas as pd # utils import os import gzip import string from tqdm.auto import tqdm import time # pytorch import torch from torch.utils.data import Dataset, DataLoader from torch import nn import torch.nn.functional as F from torchvision.transforms import transforms from torchvision import datasets class config: batch_size = 512 device = 'cuda' if torch.cuda.is_available() else 'cpu' epochs = 20 learning_rate = 1e-3 log_step = 100 seed=42 latent_dim = 32 inp_out_dim = 784 hidden_dim = 128 When I explored the dataset, I found the original images were flipped and rotated like this.\nTherefore, we need to fix it into correct direction. Fortunately, torchvision has very helpful utilities to perform data transformation.\n# transform data transform = transforms.Compose([ transforms.RandomHorizontalFlip(1), # we need to flip and rotate cz the transforms.RandomRotation((90,90)), # original img was flip and rotated. transforms.ToTensor() ]) There, we define 3 transformations.\nRandomHorizontalFlip: flip image horizontally. This function require a parameter of probability of an image being flipped. Here we need all images to be flipped, therefore we fixed the probability to 1. RandomRotation: rotate image by angle. This function require sequence number of angles, if we put only single number it will assume the rotation degree ranging from (-degrees, +degrees). Since we need all images to be rotated in the same direction, we fixed the degrees by feeding sequences (tuple) of 90. ToTensor: convert images to tensor and scale it to range (0, 1) at the same time. Now, lets download EMNIST data and put our defined transformation here. Donâ€™t forget to set the splits into letters as we want to reconstruct letters data instead of numbers.\n# load EMNIST data train_data = datasets.EMNIST(root='data', train=True, download=True, transform=transform, split='letters') test_data = datasets.EMNIST(root='data', train=False, download=True, transform=transform, split='letters') With the transformation applied, now if we interpret our data it will be in the correct direction.\nNext, letâ€™s prepare our dataloaders!\n# setup the dataloaders trainloader = DataLoader(train_data, shuffle=True, batch_size=config.batch_size) testloader = DataLoader(test_data, shuffle=True, batch_size=config.batch_size) Designing Model Architecture Now, we are close enough to the fun part. But before that, letâ€™s build our model architecture first. Here we will use Linear Layer for both our Encoder and Decoder networks. Remember that our data is scaled within range (0,1). Therefore, we should put Sigmoid layer in the very last part of Decoder network.\nclass LinearAutoencoder(nn.Module): def __init__(self, inp_out_dim, hidden_dim, latent_dim): super(LinearAutoencoder, self).__init__() # encoder layer to latent space self.encoder = nn.Sequential( nn.Linear(inp_out_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, latent_dim) ) # latent space to decoder layer self.decoder = nn.Sequential( nn.Linear(latent_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, inp_out_dim), nn.Sigmoid() # we use sigmoid cz the input and output should be in range 0,1 ) def forward(self, x): # pass input to encoder and activate it with ReLU x = self.encoder(x) # pass latent space to decoder and scale it with Sigmoid x = self.decoder(x) return x Letâ€™s define our model and interpret its architecture.\n# define model model = LinearAutoencoder(inp_out_dim=config.inp_out_dim, hidden_dim=config.hidden_dim, latent_dim=config.latent_dim) # move to GPU device model = model.to(config.device) print(model) # our model architecture LinearAutoencoder( (encoder): Sequential( (0): Linear(in_features=784, out_features=128, bias=True) (1): ReLU() (2): Linear(in_features=128, out_features=32, bias=True) ) (decoder): Sequential( (0): Linear(in_features=32, out_features=128, bias=True) (1): ReLU() (2): Linear(in_features=128, out_features=784, bias=True) (3): Sigmoid() ) ) Take a look for a while in our model architecture.\nInitially our model will accept the input image within the shape of (batch_size, 784). For those who are wondering why is it 784 and not other value, well, it is actually obtained from 28 * 28 which our original image size.\nA little explanationâ€¦\nFor better intuition, let me break down a little bit for this part.\nBy default, our data is arranged with the following shape format (batch_size, color_channel, height, width). If you take a batch from our trainloader, you will observe that our dataset having shape like this.\ntorch.Size([512, 1, 28, 28])\nThen, we need to flatten it into 2-d array. So, later we will have dataset within shape (512, 784) which then will fed to our model.\nNow, back to our model architecture.\nInstead of having single Linear Layer, we stack it with another hidden layer on each Encoder and Decoder network. You may try to modify the dimension of hidden layer by changing hidden_dim value in config.\nThen, from hidden layer, we produce a latent representation within size dimension of 32. You also may modify it by changing latent_dim in config. Finally, the latent space will be act as the input of Decoder network.\nðŸ’¡ Note that the Encoder network should be shrinking to its end and the opposite for the Decoder network as our objective is to compress the images.\nTraining Model And we arrived to the most interesting part. Here we define our loss function (criterion), optimizer, and training function.\n# loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate) # logging history = { 'train_loss': [] } # progressbar num_train_steps = len(trainloader) * config.epochs progressbar = tqdm(range(num_train_steps)) # training function epochtime = time.time() for epoch in range(config.epochs): trainloss = 0 batchtime = time.time() for idx, batch in enumerate(trainloader): # unpack data features, _ = batch features = features.to(config.device) # reshape input data into (batch_size x 784) features = features.view(features.size(0), -1) # clear gradient optimizer.zero_grad() # forward pass output = model(features) # calculate loss loss = criterion(output, features) loss.backward() # optimize optimizer.step() # update running training loss trainloss += loss.item() # update progressbar progressbar.update(1) progressbar.set_postfix_str(f\"Loss: {loss.item()}:.3f\") # log step if idx % config.log_step == 0: print(\"Epoch: %03d/%03d | Batch: %04d/%04d | Loss: %.4f\" \\ % ((epoch+1), config.epochs, idx, \\ len(trainloader), trainloss / (idx + 1))) # log epoch history['train_loss'].append(trainloss / len(trainloader)) print(\"***Epoch: %03d/%03d | Loss: %.3f\" \\ % ((epoch+1), config.epochs, loss.item())) # log time print('Time elapsed: %.2f min' % ((time.time() - batchtime) / 60)) print('Total Training Time: %.2f min' % ((time.time() - epochtime) / 60)) Here we will train for 20 epochs in total, and we log our model performances to console for every 100 training steps.\nAdditionally, we can also plot our training history to get better understanding on model performance.\nplt.figure(figsize=(5, 7)) plt.plot(range(len(history['train_loss'])), history['train_loss'], label='Train Loss') plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.legend() plt.show() After training for several epochs, we then evaluate it on our test set. Donâ€™t forget to turn off the gradient by putting torch.no_grad() during evaluation since we donâ€™t need any backpropagation process.\n# evaluate model testloss = 0 testtime = time.time() for batch in tqdm(testloader): # unpack data test_feats, _ = batch # reshape image test_feats = test_feats.view(test_feats.size(0), -1).to(config.device) # forward pass with torch.no_grad(): test_out = model(test_feats) # compute loss loss = criterion(test_out, test_feats) testloss += loss.item() print('Test Loss: %.4f' % (testloss / len(testloader))) print('Total Testing Time: %.2f min' % ((time.time() - testtime) / 60)) Inference Itâ€™s time to use our human intuition to see how good our model compressionâ€™s result. Letâ€™s take a batch from the test set and compress it with our model.\n# obtain one batch of test images test_feats, test_labels = next(iter(testloader)) original_img = test_feats.numpy() # reshape image test_feats = test_feats.view(test_feats.size(0), -1).to(config.device) # forward pass with torch.no_grad(): infer_output = model(test_feats).detach().cpu() # resize outputs back to batch of images reconstructed_img = infer_output.view(config.batch_size, 1, 28, 28).numpy() Finally, we will compare both original data and the compressed one.\n# plot the first ten input images and the reconstructed images fig, axes = plt.subplots(2, 10, sharex=True, sharey=True, figsize=(25, 4)) # input images on top, reconstruction on bottom for idx, (images, row) in enumerate(zip([original_img, reconstructed_img], axes)): for img, lbl, ax in zip(images, test_labels, row): ax.imshow(img.squeeze(), cmap=plt.cm.binary) if idx == 0: ax.set_title(f\"Label: {alphabets[lbl-1]}\") ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) Save Model Lastly, if we are satisfied already with our model performance, we can save it. So we can use it anytime later without needing to run through all the codes above.\ntorch.save(model.state_dict(), 'emnist-linear-autoencoder.pt') Conclusion So we already discussed the Autoencoder network which also a family of Generative AI. It consists of 3 main parts: Encoder network, Decoder network, and the Latent representation. We also covered the implementation of Autoencoder using simple stacks of Linear Layer.\nAlthough simple network, our model performs quite good on test set and able to compress and reconstruct letter images.\nIf you have any inquiries, comments, suggestions, or critics please donâ€™t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn Cheers! ðŸ¥‚\nReferences https://www.analyticsvidhya.com/blog/2021/06/autoencoders-a-gentle-introduction/ https://structilmy.com/blog/2020/03/17/pengenalan-autoencoder-neural-network-untuk-kompresi-data/ https://medium.com/@samuelsena/pengenalan-deep-learning-part-6-deep-autoencoder-40d79e9c7866 https://deepai.org/machine-learning-glossary-and-terms/autoencoder https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder/linear-autoencoder https://www.nist.gov/itl/products-and-services/emnist-dataset https://www.youtube.com/watch?v=345wRyqKkQ0\u0026list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51\u0026index=138 https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L16 ","wordCount":"2102","inLanguage":"en","image":"https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/cover.jpg","datePublished":"2024-01-02T21:00:00+07:00","dateModified":"2024-01-02T21:00:00+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/"},"publisher":{"@type":"Organization","name":"Fahrizen","logo":{"@type":"ImageObject","url":"https://fhrzn.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fhrzn.github.io/ accesskey=h title="Fahrizen (Alt + H)"><img src=https://fhrzn.github.io/favicon.ico alt aria-label=logo height=30>Fahrizen</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Autoencoders: Your First Step into Generative AI</h1><div class=post-description>Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one.</div><div class=post-meta><span title='2024-01-02 21:00:00 +0700 WIB'>January 2, 2024</span>&nbsp;Â·&nbsp;10 min&nbsp;Â·&nbsp;2102 words&nbsp;Â·&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager srcset="https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_90731_360x0_resize_q75_box.jpg 360w ,https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_90731_480x0_resize_q75_box.jpg 480w ,https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/cover.jpg 640w" sizes="(min-width: 768px) 720px, 100vw" src=https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/cover.jpg alt="Cover Post" width=640 height=427><p>Photo by <a href=https://unsplash.com/@perotto>Alexandre Perotto</a> on <a href=https://unsplash.com/photos/low-angle-photography-of-building-zCevd81eJDU>Unsplash</a></p></figure><div class=post-content><h2 id=generative-ai>Generative AI<a hidden class=anchor aria-hidden=true href=#generative-ai>#</a></h2><p>When we discuss about generative models, most of us might be quickly triggered to imagine the greatness of current Large Language Models (LLMs) such as ChatGPT, Bard, Gemini, LLaMA, Mixtral, etc. Or instead, the Text-to-Image models like Dall-e and stable diffusion.</p><p>Basically, the generative model works by trying to produce or generate new data that similar into the sampled one. Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one.</p><h2 id=autoencoder-network>Autoencoder Network<a hidden class=anchor aria-hidden=true href=#autoencoder-network>#</a></h2><p><img loading=lazy src=images/Autoencoder_diagram_%281%29.png alt="Autoencoder diagram (1).png"></p><p>Autoencoder is a neural network architecture that learn representation <em>(encode)</em> the input data, then tries to reconstruct the original data as close as possible by leveraging the learned representation <em>(encoded)</em> data. It is useful for denoising image, reduce dimensionality, or detecting outlier.</p><p>This network mainly consist of three parts: Encoder network, Decoder network, and Latent Space; each of them play an important role to make the model works. Letâ€™s breakdown each of them below.</p><h3 id=encoder-network>Encoder Network<a hidden class=anchor aria-hidden=true href=#encoder-network>#</a></h3><p>Encoder network is responsible to take an input data, then compress it into smaller representation of data (latent space). The layer size of this network usually shrinks as it closer to the last layer that produce the latent space. It is inline with our human intuition where <em>compress</em> means to make things smaller, and in this case, it is works by passing the input data to hidden layers which shrinks at each stage.</p><h3 id=latent-space>Latent Space<a hidden class=anchor aria-hidden=true href=#latent-space>#</a></h3><p>Latent space is the encoder outputs, which we can also call it <em>learned representation</em>. It is actually the collection of vectors just like the input data, but in smaller dimension. In my opinion, the latent space has smaller dimension as it is only keep the important parts of the input data. These important parts were selected and evaluated in every step of <em>forward propagation</em> in the encoder network, hence producing such smaller representation.</p><p>This smaller representation later will be consumed as input for the decoder network.</p><h3 id=decoder-network>Decoder Network<a hidden class=anchor aria-hidden=true href=#decoder-network>#</a></h3><p>In contrast to the encoder network, decoder network is responsible to reconstruct <em>(decode)</em> the learned representation to be as close as possible with the original input data. Instead of using the original input data, this network use the latent space as its input. It also means the decoder network forced to <em>generate</em> new data based on those representation (which hopefully representing the input data enough). Finally, in each training step the generated data will be compared to the original one until it is already close enough.</p><h3 id=loss-function>Loss Function<a hidden class=anchor aria-hidden=true href=#loss-function>#</a></h3><p>As we mentioned earlier, the generated data needs to be compared with the original one to ensure it closeness. To do that, we need to define specific loss function which can be different for each field (e.g. image, text, audio). In this case, we will use image data for our discussion.</p><p>Now, to measure the closeness between generated and original images, we can employ MSE Loss below.</p><p>$$
MSE = \frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y_i})^2
$$</p><p>where:</p><p>$N:$ is the total samples</p><p>$y_i:$ is the original input data</p><p>$\hat{y_i}:$ is the reconstructed data</p><h2 id=application-of-autoencoder>Application of Autoencoder<a hidden class=anchor aria-hidden=true href=#application-of-autoencoder>#</a></h2><ol><li><strong>Dimensionality Reduction:</strong> similar to PCA (Principal Component Analysis) which useful for visualization purpose.</li><li><strong>Feature Extraction:</strong> generate the compressed latent representation to be used for the downstream tasks such as classification or prediction. The base model of BERT, GPT2, and its family is the examples of this application.</li><li><strong>File Compression:</strong> by reducing the data dimensionality we are able to reconstruct data with smaller size (but also risk some data quality).</li><li><strong>Image Denoising:</strong> remove the noise of the image that might be produced by high ISO or corrupted image. In order to do that, we must train the model to learn the difference between the clean image and the noisy one. Once trained, it expected able to reconstruct image with less noise.</li><li><strong>Image Generator:</strong> able to generate or modify images that are similar to input data. It used the variant of autoencoder named Variational Autoencoder (VAE). It is also useful for data augmentation.</li></ol><h2 id=coding-time>Coding Time!<a hidden class=anchor aria-hidden=true href=#coding-time>#</a></h2><p>In this article, we will try to make our very first autoencoder. We will start with the simple one using Linear layers to compress the <a href=http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip>EMNIST letter images</a>.</p><p>If you are prefer to jump ahead into the notebook, <a href="https://colab.research.google.com/drive/1dfxufRQtSKVPgKTLleE4ylEtuVdsCRkn?usp=sharing">please visit this link</a>.</p><h3 id=dataset-preparation>Dataset Preparation<a hidden class=anchor aria-hidden=true href=#dataset-preparation>#</a></h3><p>First of all, letâ€™s import and create config for our training purpose.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># data manipulation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># utils</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> gzip
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> string
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tqdm.auto <span style=color:#f92672>import</span> tqdm
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># pytorch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> Dataset, DataLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision.transforms <span style=color:#f92672>import</span> transforms
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> datasets
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>config</span>:
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>    device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cuda&#39;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;cpu&#39;</span>
</span></span><span style=display:flex><span>    epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>    learning_rate <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-3</span>
</span></span><span style=display:flex><span>    log_step <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>    seed<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>
</span></span><span style=display:flex><span>    latent_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>    inp_out_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>784</span>
</span></span><span style=display:flex><span>    hidden_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span></code></pre></div><p>When I explored the dataset, I found the original images were flipped and rotated like this.</p><p><img loading=lazy src=images/Untitled.png#center alt="Rotated EMNIST images"></p><p>Therefore, we need to fix it into correct direction. Fortunately, torchvision has very helpful utilities to perform data transformation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># transform data</span>
</span></span><span style=display:flex><span>transform <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>RandomHorizontalFlip(<span style=color:#ae81ff>1</span>),     <span style=color:#75715e># we need to flip and rotate cz the</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>RandomRotation((<span style=color:#ae81ff>90</span>,<span style=color:#ae81ff>90</span>)),     <span style=color:#75715e># original img was flip and rotated.</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>ToTensor()
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><p>There, we define 3 transformations.</p><ol><li><strong>RandomHorizontalFlip:</strong> flip image horizontally. This function require a parameter of probability of an image being flipped. Here we need all images to be flipped, therefore we fixed the probability to 1.</li><li><strong>RandomRotation:</strong> rotate image by angle. This function require sequence number of angles, if we put only single number it will assume the rotation degree ranging from (-degrees, +degrees). Since we need all images to be rotated in the same direction, we fixed the degrees by feeding sequences (tuple) of 90.</li><li><strong>ToTensor:</strong> convert images to tensor and scale it to range (0, 1) at the same time.</li></ol><p>Now, lets download EMNIST data and put our defined transformation here. Donâ€™t forget to set the splits into <code>letters</code> as we want to reconstruct letters data instead of numbers.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># load EMNIST data</span>
</span></span><span style=display:flex><span>train_data <span style=color:#f92672>=</span> datasets<span style=color:#f92672>.</span>EMNIST(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;data&#39;</span>, 
</span></span><span style=display:flex><span>                             train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, 
</span></span><span style=display:flex><span>                             download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, 
</span></span><span style=display:flex><span>                             transform<span style=color:#f92672>=</span>transform,
</span></span><span style=display:flex><span>                             split<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;letters&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>test_data <span style=color:#f92672>=</span> datasets<span style=color:#f92672>.</span>EMNIST(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;data&#39;</span>,
</span></span><span style=display:flex><span>                            train<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>                            download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                            transform<span style=color:#f92672>=</span>transform,
</span></span><span style=display:flex><span>                            split<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;letters&#39;</span>)
</span></span></code></pre></div><p>With the transformation applied, now if we interpret our data it will be in the correct direction.</p><p><img loading=lazy src=images/Untitled%201.png#center alt="Transformed EMNIST images"></p><p>Next, letâ€™s prepare our dataloaders!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># setup the dataloaders</span>
</span></span><span style=display:flex><span>trainloader <span style=color:#f92672>=</span> DataLoader(train_data, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, batch_size<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>batch_size)
</span></span><span style=display:flex><span>testloader <span style=color:#f92672>=</span> DataLoader(test_data, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, batch_size<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>batch_size)
</span></span></code></pre></div><h3 id=designing-model-architecture>Designing Model Architecture<a hidden class=anchor aria-hidden=true href=#designing-model-architecture>#</a></h3><p>Now, we are close enough to the fun part. But before that, letâ€™s build our model architecture first. Here we will use Linear Layer for both our Encoder and Decoder networks. Remember that our data is scaled within range (0,1). Therefore, we should put Sigmoid layer in the very last part of Decoder network.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LinearAutoencoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, inp_out_dim, hidden_dim, latent_dim):
</span></span><span style=display:flex><span>        super(LinearAutoencoder, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># encoder layer to latent space</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>encoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(inp_out_dim, hidden_dim),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(hidden_dim, latent_dim)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># latent space to decoder layer</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>decoder <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(latent_dim, hidden_dim),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>ReLU(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(hidden_dim, inp_out_dim),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Sigmoid() <span style=color:#75715e># we use sigmoid cz the input and output should be in range 0,1</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># pass input to encoder and activate it with ReLU</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>encoder(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># pass latent space to decoder and scale it with Sigmoid</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decoder(x)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>Letâ€™s define our model and interpret its architecture.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># define model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> LinearAutoencoder(inp_out_dim<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>inp_out_dim,
</span></span><span style=display:flex><span>                          hidden_dim<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>hidden_dim,
</span></span><span style=display:flex><span>                          latent_dim<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>latent_dim)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># move to GPU device</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(config<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>print(model)
</span></span></code></pre></div><pre tabindex=0><code># our model architecture
LinearAutoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=32, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=32, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=784, bias=True)
    (3): Sigmoid()
  )
)
</code></pre><p>Take a look for a while in our model architecture.</p><p>Initially our model will accept the input image within the shape of (batch_size, 784). For those who are wondering why is it 784 and not other value, well, it is actually obtained from 28 * 28 which our original image size.</p><blockquote><p><strong>A little explanationâ€¦</strong></p><p>For better intuition, let me break down a little bit for this part.</p><p>By default, our data is arranged with the following shape format (batch_size, color_channel, height, width). If you take a batch from our trainloader, you will observe that our dataset having shape like this.</p><p><code>torch.Size([512, 1, 28, 28])</code></p><p>Then, we need to flatten it into 2-d array. So, later we will have dataset within shape (512, 784) which then will fed to our model.</p></blockquote><p>Now, back to our model architecture.</p><p>Instead of having single Linear Layer, we stack it with another hidden layer on each Encoder and Decoder network. You may try to modify the dimension of hidden layer by changing <code>hidden_dim</code> value in <code>config</code>.</p><p>Then, from hidden layer, we produce a latent representation within size dimension of 32. You also may modify it by changing <code>latent_dim</code> in <code>config</code>. Finally, the latent space will be act as the input of Decoder network.</p><blockquote><p>ðŸ’¡ Note that the Encoder network should be shrinking to its end and the opposite for the Decoder network as our objective is to compress the images.</p></blockquote><h3 id=training-model>Training Model<a hidden class=anchor aria-hidden=true href=#training-model>#</a></h3><p>And we arrived to the most interesting part. Here we define our loss function (criterion), optimizer, and training function.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># loss and optimizer</span>
</span></span><span style=display:flex><span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>config<span style=color:#f92672>.</span>learning_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># logging</span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;train_loss&#39;</span>: []
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># progressbar</span>
</span></span><span style=display:flex><span>num_train_steps <span style=color:#f92672>=</span> len(trainloader) <span style=color:#f92672>*</span> config<span style=color:#f92672>.</span>epochs
</span></span><span style=display:flex><span>progressbar <span style=color:#f92672>=</span> tqdm(range(num_train_steps))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># training function</span>
</span></span><span style=display:flex><span>epochtime <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(config<span style=color:#f92672>.</span>epochs):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    trainloss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    batchtime <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx, batch <span style=color:#f92672>in</span> enumerate(trainloader):
</span></span><span style=display:flex><span>        <span style=color:#75715e># unpack data</span>
</span></span><span style=display:flex><span>        features, _ <span style=color:#f92672>=</span> batch
</span></span><span style=display:flex><span>        features <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>to(config<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># reshape input data into (batch_size x 784)</span>
</span></span><span style=display:flex><span>        features <span style=color:#f92672>=</span> features<span style=color:#f92672>.</span>view(features<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>), <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># clear gradient</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># forward pass</span>
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> model(features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># calculate loss</span>
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> criterion(output, features)
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># optimize</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#  update running training loss</span>
</span></span><span style=display:flex><span>        trainloss <span style=color:#f92672>+=</span> loss<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># update progressbar</span>
</span></span><span style=display:flex><span>        progressbar<span style=color:#f92672>.</span>update(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        progressbar<span style=color:#f92672>.</span>set_postfix_str(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>item()<span style=color:#e6db74>}</span><span style=color:#e6db74>:.3f&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># log step</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> idx <span style=color:#f92672>%</span> config<span style=color:#f92672>.</span>log_step <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;Epoch: </span><span style=color:#e6db74>%03d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%03d</span><span style=color:#e6db74> | Batch: </span><span style=color:#e6db74>%04d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%04d</span><span style=color:#e6db74> | Loss: </span><span style=color:#e6db74>%.4f</span><span style=color:#e6db74>&#34;</span> \
</span></span><span style=display:flex><span>                  <span style=color:#f92672>%</span> ((epoch<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>), config<span style=color:#f92672>.</span>epochs, idx, \
</span></span><span style=display:flex><span>                     len(trainloader), trainloss <span style=color:#f92672>/</span> (idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>                    
</span></span><span style=display:flex><span>    <span style=color:#75715e># log epoch</span>
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;train_loss&#39;</span>]<span style=color:#f92672>.</span>append(trainloss <span style=color:#f92672>/</span> len(trainloader))
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;***Epoch: </span><span style=color:#e6db74>%03d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%03d</span><span style=color:#e6db74> | Loss: </span><span style=color:#e6db74>%.3f</span><span style=color:#e6db74>&#34;</span> \
</span></span><span style=display:flex><span>          <span style=color:#f92672>%</span> ((epoch<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>), config<span style=color:#f92672>.</span>epochs, loss<span style=color:#f92672>.</span>item()))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># log time</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;Time elapsed: </span><span style=color:#e6db74>%.2f</span><span style=color:#e6db74> min&#39;</span> <span style=color:#f92672>%</span> ((time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> batchtime) <span style=color:#f92672>/</span> <span style=color:#ae81ff>60</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Total Training Time: </span><span style=color:#e6db74>%.2f</span><span style=color:#e6db74> min&#39;</span> <span style=color:#f92672>%</span> ((time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> epochtime) <span style=color:#f92672>/</span> <span style=color:#ae81ff>60</span>))
</span></span></code></pre></div><p>Here we will train for 20 epochs in total, and we log our model performances to console for every 100 training steps.</p><p>Additionally, we can also plot our training history to get better understanding on model performance.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>7</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(range(len(history[<span style=color:#e6db74>&#39;train_loss&#39;</span>])), history[<span style=color:#e6db74>&#39;train_loss&#39;</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Train Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epochs&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;MSE Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled%202.png#center alt="Training Loss"></p><p>After training for several epochs, we then evaluate it on our test set. Donâ€™t forget to turn off the gradient by putting <code>torch.no_grad()</code> during evaluation since we donâ€™t need any backpropagation process.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># evaluate model</span>
</span></span><span style=display:flex><span>testloss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>testtime <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> tqdm(testloader):
</span></span><span style=display:flex><span>    <span style=color:#75715e># unpack data</span>
</span></span><span style=display:flex><span>    test_feats, _ <span style=color:#f92672>=</span> batch
</span></span><span style=display:flex><span>    <span style=color:#75715e># reshape image</span>
</span></span><span style=display:flex><span>    test_feats <span style=color:#f92672>=</span> test_feats<span style=color:#f92672>.</span>view(test_feats<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>), <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>to(config<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    <span style=color:#75715e># forward pass</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        test_out <span style=color:#f92672>=</span> model(test_feats)
</span></span><span style=display:flex><span>    <span style=color:#75715e># compute loss</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> criterion(test_out, test_feats)
</span></span><span style=display:flex><span>    testloss <span style=color:#f92672>+=</span> loss<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Test Loss: </span><span style=color:#e6db74>%.4f</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> (testloss <span style=color:#f92672>/</span> len(testloader)))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;Total Testing Time: </span><span style=color:#e6db74>%.2f</span><span style=color:#e6db74> min&#39;</span> <span style=color:#f92672>%</span> ((time<span style=color:#f92672>.</span>time() <span style=color:#f92672>-</span> testtime) <span style=color:#f92672>/</span> <span style=color:#ae81ff>60</span>))
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled%203.png#center alt="Test Loss"></p><h3 id=inference>Inference<a hidden class=anchor aria-hidden=true href=#inference>#</a></h3><p>Itâ€™s time to use our human intuition to see how good our model compressionâ€™s result. Letâ€™s take a batch from the test set and compress it with our model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># obtain one batch of test images</span>
</span></span><span style=display:flex><span>test_feats, test_labels <span style=color:#f92672>=</span> next(iter(testloader))
</span></span><span style=display:flex><span>original_img <span style=color:#f92672>=</span> test_feats<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># reshape image</span>
</span></span><span style=display:flex><span>test_feats <span style=color:#f92672>=</span> test_feats<span style=color:#f92672>.</span>view(test_feats<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>), <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>to(config<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># forward pass</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>    infer_output <span style=color:#f92672>=</span> model(test_feats)<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># resize outputs back to batch of images</span>
</span></span><span style=display:flex><span>reconstructed_img <span style=color:#f92672>=</span> infer_output<span style=color:#f92672>.</span>view(config<span style=color:#f92672>.</span>batch_size, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>numpy()
</span></span></code></pre></div><p>Finally, we will compare both original data and the compressed one.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># plot the first ten input images and the reconstructed images</span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>10</span>, sharex<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, sharey<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>25</span>, <span style=color:#ae81ff>4</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># input images on top, reconstruction on bottom</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> idx, (images, row) <span style=color:#f92672>in</span> enumerate(zip([original_img, reconstructed_img], axes)):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> img, lbl, ax <span style=color:#f92672>in</span> zip(images, test_labels, row):
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>imshow(img<span style=color:#f92672>.</span>squeeze(), cmap<span style=color:#f92672>=</span>plt<span style=color:#f92672>.</span>cm<span style=color:#f92672>.</span>binary)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> idx <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            ax<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Label: </span><span style=color:#e6db74>{</span>alphabets[lbl<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>get_xaxis()<span style=color:#f92672>.</span>set_visible(<span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>get_yaxis()<span style=color:#f92672>.</span>set_visible(<span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled%204.png alt="Inference Samples"></p><h3 id=save-model>Save Model<a hidden class=anchor aria-hidden=true href=#save-model>#</a></h3><p>Lastly, if we are satisfied already with our model performance, we can save it. So we can use it anytime later without needing to run through all the codes above.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>save(model<span style=color:#f92672>.</span>state_dict(), <span style=color:#e6db74>&#39;emnist-linear-autoencoder.pt&#39;</span>)
</span></span></code></pre></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>So we already discussed the Autoencoder network which also a family of Generative AI. It consists of 3 main parts: Encoder network, Decoder network, and the Latent representation. We also covered the implementation of Autoencoder using simple stacks of Linear Layer.</p><p>Although simple network, our model performs quite good on test set and able to compress and reconstruct letter images.</p><p>If you have any inquiries, comments, suggestions, or critics please donâ€™t hesitate to reach me out:</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul><p>Cheers! ðŸ¥‚</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://www.analyticsvidhya.com/blog/2021/06/autoencoders-a-gentle-introduction/>https://www.analyticsvidhya.com/blog/2021/06/autoencoders-a-gentle-introduction/</a></li><li><a href=https://structilmy.com/blog/2020/03/17/pengenalan-autoencoder-neural-network-untuk-kompresi-data/>https://structilmy.com/blog/2020/03/17/pengenalan-autoencoder-neural-network-untuk-kompresi-data/</a></li><li><a href=https://medium.com/@samuelsena/pengenalan-deep-learning-part-6-deep-autoencoder-40d79e9c7866>https://medium.com/@samuelsena/pengenalan-deep-learning-part-6-deep-autoencoder-40d79e9c7866</a></li><li><a href=https://deepai.org/machine-learning-glossary-and-terms/autoencoder>https://deepai.org/machine-learning-glossary-and-terms/autoencoder</a></li><li><a href=https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder/linear-autoencoder>https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder/linear-autoencoder</a></li><li><a href=https://www.nist.gov/itl/products-and-services/emnist-dataset>https://www.nist.gov/itl/products-and-services/emnist-dataset</a></li><li><a href="https://www.youtube.com/watch?v=345wRyqKkQ0&amp;list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&amp;index=138">https://www.youtube.com/watch?v=345wRyqKkQ0&amp;list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&amp;index=138</a></li><li><a href=https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L16>https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L16</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://fhrzn.github.io/tags/deeplearning/>deeplearning</a></li><li><a href=https://fhrzn.github.io/tags/representation/>representation</a></li><li><a href=https://fhrzn.github.io/tags/autoencoder/>autoencoder</a></li></ul><nav class=paginav><a class=next href=https://fhrzn.github.io/posts/exploring-visual-transformers-vit-with-huggingface/><span class=title>Next Â»</span><br><span>Exploring Vision Transformers (ViT) with ðŸ¤— Huggingface</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://fhrzn.github.io/>Fahrizen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>