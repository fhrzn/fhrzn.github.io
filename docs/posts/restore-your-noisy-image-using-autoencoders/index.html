<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Restore your Noisy Image Using Autoencoders | fahrizain</title><meta name=keywords content="deep learning,image reconstruction,autoencoder"><meta name=description content="Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration."><meta name=author content="Affandy Fahrizain"><link rel=canonical href=https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://fhrzn.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fhrzn.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fhrzn.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fhrzn.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fhrzn.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous referrerpolicy=no-referrer type=text/javascript></script><script type=text/javascript>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-27DEESLMGL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-27DEESLMGL")</script><meta property="og:url" content="https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/"><meta property="og:site_name" content="fahrizain"><meta property="og:title" content="Restore your Noisy Image Using Autoencoders"><meta property="og:description" content="Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-07T19:19:52+07:00"><meta property="article:modified_time" content="2024-01-07T19:19:52+07:00"><meta property="article:tag" content="Autoencoder"><meta property="article:tag" content="Deeplearning"><meta property="og:image" content="https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg"><meta name=twitter:title content="Restore your Noisy Image Using Autoencoders"><meta name=twitter:description content="Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration."><meta name=twitter:site content="@fhrzn_"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fhrzn.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Restore your Noisy Image Using Autoencoders","item":"https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Restore your Noisy Image Using Autoencoders","name":"Restore your Noisy Image Using Autoencoders","description":"Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration.","keywords":["deep learning","image reconstruction","autoencoder"],"articleBody":" This is the next series of my notes while exploring autoencoders. You may also interested to read my other notes on Autoencoder network:\n1. Autoencoders: Your First Step into Generative AI\n2. Restore your Noisy Image using Autoencoders\nConvolutional Autoencoder (CAE) In the previous article, we implemented the image compression model using Linear Autoencoder. However, when it comes to larger image and having more colors (RGB, not grayscale or just black and white) it is a good idea to try incorporating Convolutional layer instead of simple Linear layer. You may do your mini research by yourself, but this time let’s just modify our network using Convolutional layer.\nBefore we deep dive to code, let’s cover some basic theory so we can completely understand what is happening in our model.\nConvolutional Network source: https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699\nLet’s do quick review how Convolutional Neural Network (CNN) works.\nWhat will happen if we feed our original image with size 512x512 pixel into our model? Well, if our image is grayscale it would produce 512 * 512 = 262,144 data. While if our image is RGB, it would produce 512 * 512 * 3 = 786,432 data. And that’s a lot for our computer memory!\nThis is where the implementation of CNN is certainly needed.\nBasically, CNN used a so-called kernel to “scan” each part of the image. This kernel is actually 2-dimensional matrix within size $M\\times N$. It travels through whole image by moving within $S$ stride on each step. And this kernel is duplicated as much as image’s channels size, for example colorful image will have 3 channels for each Red Green Blue colors. Therefore, there will be 3 for such image.\nFor easier interpretation you may see the illustration below.\nsource: https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel\nThe above step will produce a “projection layer” which contains the observation of each kernels as shown in the previous illustration. Then, from this projection layer we will try to reduce the pixel size by applying a Pooling layer.\nThere are two popular kind of Pooling layer, Max Pooling and Average Pooling. And you can guess by its name, the former one reduce the data dimension by taking the maximum on each observation space, while the latter perform it by taking the average.\nsource: https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b\nFinally, the reduced data will be passed to the next layer and will be going through the same process every time it passed to CNN layers.\nNote that you may look for other references for further explanation of CNNs as we won’t cover it too much here.\nWhat CNN’s Actually do? Overall, as in my understanding, I could say that CNN operation is trying to extract features and reduce the data dimension at the same time. By applying Max pooling or Average pooling, it expected to keeps the relevant information while reducing the dimension.\nCAE Network Design The network design for Convolutional based autoencoder was basically same with the Linear one. There are Encoder, Decoder, and Latent space. However, the key differences is located on how we implement Encoder and Decoder layers.\nEncoder Layer The Encoder layer design is not complicated as it is very similar to common CNN implementation for classification. The layers, of course, designed to shrink as it closer to the latent space, expecting it to extract key features of our images and map it to $C\\times W\\times H$ of latent space.\nwhere:\n$C:$ channels\n$W:$ image width\n$H:$ image height\nIn the last layer of encoder output, you may choose to flatten it then feed to Linear layer or leave it as is for the latent space.\n(P.S. if you have opinions or best practices, please don’t hesitate to leave it in comment section)\nDecoder Layer The Decoder layer design in general still the same to regular autoencoder network, which is need to expand over the time (the opposite shape of encoder layer). However, there are some adjustment to the convolutional layer for the decoder.\nWhile convolutional layer is intended to reduce the data dimension size, we need to make it do the opposite for the implementation in Decoder layer. Instead of shrink the dimension, it needs to upsample it. Fortunately, in PyTorch we can achieve it easily with Conv2dTranspose layer.\nsource: https://stackoverflow.com/a/55119869\nIn short, we can say it do the opposite operation of regular CNN layer. And we will use it to upsample our latent space back to its original size image.\nIf you are looking for more explanation on this upsample operation, I recommend you to watch Intro to Deep Learning and Generative Models Course by Sebastian Raschka.\nCoding Time! This time we will try to denoise flowers image using Convolutional Autoencoders. We will use 16 different Flowers dataset which available in Kaggle.\nIf you wants to jump ahead into the notebook, please visit this link.\nDataset Preparation Let’s import the necessary libraries and setup our base config.\n# data manipulation import numpy as np import matplotlib.pyplot as plt import pandas as pd # utils import os import gzip import string from tqdm.auto import tqdm import time import random # sklearn from sklearn.model_selection import train_test_split # torch import torch from torchvision.datasets import ImageFolder import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader, Subset class config: batch_size = 128 device = 'cuda' if torch.cuda.is_available() else 'cpu' epochs = 20 learning_rate = 1e-3 log_step = 50 seed=42 latent_dim = 32 inp_out_dim = 784 hidden_dim = 128 We wants to make sure that all images must be in size 256 x 256. Therefore, we can add a transformation operation to crop and resize the image.\n# transform data transform = transforms.Compose([ transforms.RandomResizedCrop(256), # crop and/or resize image to 256x256 transforms.ToTensor() ]) Now, since our dataset is only images stored in folders, we may load it to pytorch using ImageFolder utils from torchvision like this. Don’t forget to pass our previously defined transformation operations.\n# load data from folder dataset = ImageFolder(root=\"/kaggle/input/flowers/flowers\", transform=transform) target = dataset.targets Then, let’s make train–test split and setup our dataloaders.\n# make train test split train_idx, test_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=target, random_state=42) train_subset = Subset(dataset, indices=train_idx) test_subset = Subset(dataset, indices=test_idx) # make trainloader trainloader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True) testloader = DataLoader(test_subset, batch_size=config.batch_size, shuffle=True) Designing Model Architecture It’s time to implement our previous discussion on the convolutional autoencoder network!\nFor the encoder part, let’s stack 4 Convolutional layers with shrinking output to its end.\n# encoder self.encoder = nn.Sequential( nn.Conv2d(3, 16, 3, padding=1, stride=1), nn.ReLU(), nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.ReLU(), nn.Conv2d(32, 64, 3, padding=1, stride=2), nn.ReLU(), nn.Conv2d(64, 128, 3, padding=1, stride=2), ) Here, I am trying to reduce the data dimension by using only stride parameter instead of employing the Max Pooling layer. You may try it by yourself to incorporate the Max Pooling layer and leave the stride parameter as 1.\n💡 There is also discussion whether to use stride or max pooling layer for reducing data dimension. You can find it here: https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling\nDuring the process, I found difficulties to guess what is the output size of each convolution layers. Then I found the full explanation and guide from Stanford which I find very useful. Here is the link: https://cs231n.github.io/convolutional-networks/#layers.\nOr simply, you can follow this formula which I obtained from the link above.\n$$ (W−F+2P)/S+1 $$\nwhere:\n$W:$ input volume size\n$F:$ kernel (filter) size\n$P:$ padding size\n$S:$ stride size\nNow, for the decoder part, as the opposite of the encoder layer, let’s stack 4 Transpose Convolutional layers with expanding output to its end.\n# decoder self.decoder = nn.Sequential( nn.ConvTranspose2d(128, 64, 3, padding=1, stride=2), nn.ReLU(), nn.ConvTranspose2d(64, 32, 3, padding=0, stride=2), nn.ReLU(), nn.ConvTranspose2d(32, 16, 3, padding=0, stride=2), nn.ReLU(), nn.ConvTranspose2d(16, 3, 2, padding=0, stride=1), # for the final layer I used kernel size 2, as I found it empirically match the original input image size nn.Sigmoid() ) Honestly, specific on this part I still had a difficulties to tell the reason why the padding need to set as 0 and the final stride to 1. What I did was actually following the tutorial here and here and do my observation through try and error by feeding random tensor in equal shape with the latent space to this network.\nWhile to calculate the output of each Transpose Convolutional layer, I follow its formula defined on PyTorch page here: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html.\nFinally, the forward function is defined as follows:\ndef forward(self, x): x = self.encoder(x) x = self.decoder(x) return x The full code:\n# define our network class ConvDenoiseAutoencoder(nn.Module): def __init__(self): super(ConvDenoiseAutoencoder, self).__init__() # encoder self.encoder = nn.Sequential( nn.Conv2d(3, 16, 3, padding=1, stride=1), nn.ReLU(), nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.ReLU(), nn.Conv2d(32, 64, 3, padding=1, stride=2), nn.ReLU(), nn.Conv2d(64, 128, 3, padding=1, stride=2), ) # here we downsample the image using only stride instead of maxpool. # you may try maxpool approach as it is only my experiment using stride. # see discussion here: https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling # decoder self.decoder = nn.Sequential( nn.ConvTranspose2d(128, 64, 3, padding=1, stride=2), nn.ReLU(), nn.ConvTranspose2d(64, 32, 3, padding=0, stride=2), nn.ReLU(), nn.ConvTranspose2d(32, 16, 3, padding=0, stride=2), nn.ReLU(), nn.ConvTranspose2d(16, 3, 2, padding=0, stride=1), # for the final layer I used kernel size 2, as I found it empirically match the original input image size nn.Sigmoid() ) def forward(self, x): x = self.encoder(x) x = self.decoder(x) return x Let’s define our model and see its architecture.\n# define model model = ConvDenoiseAutoencoder() # move to GPU device model = model.to(config.device) print(model) ConvDenoiseAutoencoder( (encoder): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (3): ReLU() (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (5): ReLU() (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) ) (decoder): Sequential( (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2)) (3): ReLU() (4): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2)) (5): ReLU() (6): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(1, 1)) (7): Sigmoid() ) ) Add Noise to Images In order to make our model understand to clean the noise from images, we first need to introduce a random noise to the original images.\n# get one batch from trainloader features, _ = next(iter(trainloader)) # define random noise and how much its influence noise_factor = .5 noise = noise_factor * torch.randn(*features.shape) noisy_img = features + noise # keep image colors stay in scale between 0 to 1 features = torch.clamp(noisy_img, 0., 1.) Note that the denoising autoencoder learn the representation by measuring the distance between the generated image and the original one. The generated image here is the attempt output of denoising autoencoder network.\nBy minimizing the distance between noisy and original one, it means the model will also learn to remove the noise or at least reduce it.\nNoise Probability During training phase, I found its challenging for our model to learn image representation if the whole training data are corrupted. Therefore, as part of my experiment, I tried to add threshold probability to determine whether batch of images need to be corrupted or not.\n# add noise to image if random.random() \u003e p: if noise_factor: noise = noise_factor * torch.randn(*features.shape) noisy_img = features + noise features = torch.clamp(noisy_img, 0., 1.) By adding threshold probability, we let some non-corrupted dataset to be learned as well. And by doing so, we hope the model will be able to learn underlying images representation and have a little insight on difference between the corrupted and non-corrupted images.\nTraining Model Let’s now define our training loop. Here we would like to wrap our code inside a function, so we can reproduce easily later and the code itself become cleaner.\ndef train(model, noise_factor: float = None, p_threshold: float = 0.0): # loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate) # loss logging history = { 'train_loss': [] } # progressbar num_train_steps = len(trainloader) * config.epochs progressbar = tqdm(range(num_train_steps)) epochtime = time.time() for epoch in range(config.epochs): trainloss = 0 batchtime = time.time() for idx, batch in enumerate(trainloader): # unpack data features, _ = batch **# add noise to image if random.random() \u003e p: if noise_factor: noise = noise_factor * torch.randn(*features.shape) noisy_img = features + noise features = torch.clamp(noisy_img, 0., 1.)** features = features.to(config.device) # clear gradient optimizer.zero_grad() # forward pass output = model(features) # calculate loss loss = criterion(output, features) loss.backward() # optimize optimizer.step() # update running training loss trainloss += loss.item() # update progressbar progressbar.update(1) progressbar.set_postfix_str(f\"Loss: {loss.item():.3f}\") # log step if idx % config.log_step == 0: print(\"Epoch: %03d/%03d | Batch: %04d/%04d | Loss: %.4f\" \\ % ((epoch+1), config.epochs, idx, \\ len(trainloader), trainloss / (idx + 1))) # log epoch history['train_loss'].append(trainloss / len(trainloader)) print(\"***Epoch: %03d/%03d | Loss: %.3f\" \\ % ((epoch+1), config.epochs, loss.item())) # log time print('Time elapsed: %.2f min' % ((time.time() - batchtime) / 60)) print('Total Training Time: %.2f min' % ((time.time() - epochtime) / 60)) return model, history And the function call itself.\nmodel, history = train(model, noise_factor=0.15, p_threshold=0.5) Here we are training our model with probability of data corruption 0.5 and the noise factor itself 0.15.\nAfter training for a while, let’s now plot our train loss history.\nplt.figure(figsize=(5, 7)) plt.plot(range(len(history['train_loss'])), history['train_loss'], label='Train Loss') plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.legend() plt.show() Testing time! testloss = 0 testtime = time.time() criterion = nn.MSELoss() noise_factor = 0.25 for batch in tqdm(testloader): # unpack data test_feats, _ = batch **# add noise to image if noise_factor: noise = noise_factor * torch.randn(*test_feats.shape) noisy_img = test_feats + noise test_feats = torch.clamp(noisy_img, 0., 1.)** test_feats = test_feats.to(config.device) # forward pass with torch.no_grad(): test_out = model(test_feats) # compute loss loss = criterion(test_out, test_feats) testloss += loss.item() print('Test Loss: %.4f' % (testloss / len(testloader))) print('Total Testing Time: %.2f min' % ((time.time() - testtime) / 60)) Visualize our Test Result Let’s take the first 5 original and reconstructed image from latest batch in test set.\norig = test_feats[:5].detach().cpu() recon = test_out[:5].detach().cpu() fig, axes = plt.subplots(2, 5, sharex=True, sharey=True, figsize=(20, 8)) for images, row in zip([orig, recon], axes): for img, ax in zip(images, row): ax.imshow(img.permute(1, 2, 0)) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) Great! Based on the result, although the image not completely denoised, our model seems to be able reduce it a little bit. You may also try to inference your denoised image several time and see if the noise will be more reduced.\nFurther Exploration Experiment with Different Model Architecture To fulfill my curiosity, I tried incorporate the Max Pooling layer. So I changed the model architecture to be like this.\nclass ConvDenoiseAutoencoderV3(nn.Module): def __init__(self): super(ConvDenoiseAutoencoderV3, self).__init__() # encoder self.encoder = nn.Sequential( nn.Conv2d(3, 16, 3, padding=1, stride=1), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(16, 32, 3, padding=1, stride=1), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(32, 64, 3, padding=1, stride=1), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(64, 128, 3, padding=1, stride=1), nn.ReLU(), nn.MaxPool2d(2, 2) ) # decoder self.decoder = nn.Sequential( nn.ConvTranspose2d(128, 128, 3, stride=2), nn.ReLU(), nn.ConvTranspose2d(128, 64, 2, padding=1, stride=2), nn.ReLU(), nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU(), nn.ConvTranspose2d(32, 16, 2, stride=2), # for the final layer I used kernel size 2, as I found it empirically match the original input image size nn.ReLU(), nn.Conv2d(16, 3, 3, padding=1), nn.Sigmoid() ) def forward(self, x): x = self.encoder(x) x = self.decoder(x) return x ConvDenoiseAutoencoderV3( (encoder): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): ReLU() (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU() (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (10): ReLU() (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (decoder): Sequential( (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2)) (1): ReLU() (2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1)) (3): ReLU() (4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2)) (5): ReLU() (6): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2)) (7): ReLU() (8): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): Sigmoid() ) ) Experimental Result We keep the training setup same to our previous setup. The only difference is now we are setting noise probability to 0 and noise_factor to 0.5, ensuring that all images are quite badly corrupted.\nmodel, history = train(model, noise_factor=0.5, p_threshold=0.0) With that settings, here is our training loss looks like.\nAnd finally, this is our denoised images using such architecture.\nFrom the output we could tell that the model might be able to remove noise better than our previous model architecture. However, the generated result seems to be blurred.\nAgain, our model architecture is very simple compared to the latest image reconstruction architecture. Of course the performance and reconstruction quality is differs by large margin. You may try to improve the model architecture or implement published paper’s architecture.\nConclusion Now we know more advanced Autoencoder architecture using Convolutional layers. The key difference is how our image will be processed through each layer. It works by reducing the dimension of images and projected to specific size of dimension.\nWe also discovered how to corrupt image simply by creating random noise and add it to our data. By learning the distance between corrupted images and the original one, it is expected that our model is able to remove (or at least reduce) the noise from image, leaving better image quality.\nIf you have any inquiries, comments, suggestions, or critics please don’t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn Until next time! 👋\nReferences Intro to Deep Learning and Generative Models Course by Sebastian Raschka. Udacity Intro to Deep Learning with Pytorch Github repos CS231n Convolutional Neural Networks for Visual Recognition by Stanford Pooling vs Stride for downsampling Autoencoders in Deep Learning by V7 ","wordCount":"2905","inLanguage":"en","image":"https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg","datePublished":"2024-01-07T19:19:52+07:00","dateModified":"2024-01-07T19:19:52+07:00","author":{"@type":"Person","name":"Affandy Fahrizain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/"},"publisher":{"@type":"Organization","name":"fahrizain","logo":{"@type":"ImageObject","url":"https://fhrzn.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fhrzn.github.io/ accesskey=h title="fahrizain (Alt + H)"><img src=https://fhrzn.github.io/favicon-32x32.png alt aria-label=logo height=30>fahrizain</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://fhrzn.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://fhrzn.github.io/profile/ title=Profile><span>Profile</span></a></li><li><a href=https://www.linkedin.com/in/fahrizainn/ title=LinkedIn><span>LinkedIn</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Restore your Noisy Image Using Autoencoders</h1><div class=post-description>Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration.</div><div class=post-meta><span title='2024-01-07 19:19:52 +0700 WIB'>January 7, 2024</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2905 words&nbsp;·&nbsp;Affandy Fahrizain</div></header><figure class=entry-cover><img loading=eager srcset='https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover_hu_1a97a742946e80b9.jpg 360w,https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover_hu_38e96d180047afeb.jpg 480w,https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg 640w' src=https://fhrzn.github.io/posts/restore-your-noisy-image-using-autoencoders/cover.jpg sizes="(min-width: 768px) 720px, 100vw" width=640 height=427 alt="Cover Post"><figcaption>Photo by <a href=https://unsplash.com/@perotto>Alexandre Perotto</a> on <a href=https://unsplash.com/photos/low-angle-photography-of-building-zCevd81eJDU>Unsplash</a></figcaption></figure><div class=post-content><blockquote><p><em>This is the next series of my notes while exploring autoencoders.</em>
<em>You may also interested to read my other notes on Autoencoder network:</em></p><p><em>1. <a href=https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/>Autoencoders: Your First Step into Generative AI</a></em></p><p><em>2. <strong>Restore your Noisy Image using Autoencoders</strong></em></p></blockquote><h2 id=convolutional-autoencoder-cae>Convolutional Autoencoder (CAE)<a hidden class=anchor aria-hidden=true href=#convolutional-autoencoder-cae>#</a></h2><p>In the previous article, we implemented the image compression model using Linear Autoencoder. However, when it comes to larger image and having more colors (RGB, not grayscale or just black and white) it is a good idea to try incorporating Convolutional layer instead of simple Linear layer. You may do your mini research by yourself, but this time let’s just modify our network using Convolutional layer.</p><p>Before we deep dive to code, let’s cover some basic theory so we can completely understand what is happening in our model.</p><h3 id=convolutional-network>Convolutional Network<a hidden class=anchor aria-hidden=true href=#convolutional-network>#</a></h3><p><input type=checkbox id=zoomCheck-60508 hidden>
<label for=zoomCheck-60508><img class=zoomCheck loading=lazy decoding=async src=images/cnn.jpeg#center alt="Convolutional Neural Network diagram. source: https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699">
</label>source: <a href=https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699>https://medium.com/analytics-vidhya/vggnet-convolutional-network-for-classification-and-detection-3543aaf61699</a></p><p>Let’s do quick review how Convolutional Neural Network (CNN) works.</p><p><em>What will happen if we feed our original image with size 512x512 pixel into our model?</em>
Well, if our image is grayscale it would produce <code>512 * 512 = 262,144 data</code>. While if our image is RGB, it would produce <code>512 * 512 * 3 = 786,432 data</code>. And that’s a lot for our computer memory!</p><p>This is where the implementation of CNN is certainly needed.</p><p>Basically, CNN used a so-called kernel to “scan” each part of the image. This kernel is actually 2-dimensional matrix within size $M\times N$. It travels through whole image by moving within $S$ stride on each step. And this kernel is duplicated as much as image’s channels size, for example colorful image will have 3 channels for each Red Green Blue colors. Therefore, there will be 3 for such image.</p><p>For easier interpretation you may see the illustration below.</p><p><input type=checkbox id=zoomCheck-86d91 hidden>
<label for=zoomCheck-86d91><img class=zoomCheck loading=lazy decoding=async src=images/cnn_kernel.gif#center alt="CNN Kernel. source: https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel">
</label>source: <a href=https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel>https://www.quora.com/How-does-a-convolutional-layer-convert-a-64-channel-input-into-a-3-channel-or-one-channel</a></p><p>The above step will produce a “projection layer” which contains the observation of each kernels as shown in the previous illustration. Then, from this projection layer we will try to reduce the pixel size by applying a Pooling layer.</p><p>There are two popular kind of Pooling layer, Max Pooling and Average Pooling. And you can guess by its name, the former one reduce the data dimension by taking the maximum on each observation space, while the latter perform it by taking the average.</p><p><input type=checkbox id=zoomCheck-02083 hidden>
<label for=zoomCheck-02083><img class=zoomCheck loading=lazy decoding=async src=images/pooling_layers.png#center alt="Pooling layers. source: https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b"></label></p><p>source: <a href=https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b>https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b</a></p><p>Finally, the reduced data will be passed to the next layer and will be going through the same process every time it passed to CNN layers.</p><blockquote><p><em>Note that you may look for other references for further explanation of CNNs as we won’t cover it too much here.</em></p></blockquote><h3 id=what-cnns-actually-do>What CNN’s Actually do?<a hidden class=anchor aria-hidden=true href=#what-cnns-actually-do>#</a></h3><p>Overall, as in my understanding, I could say that CNN operation is trying to extract features and reduce the data dimension at the same time. By applying Max pooling or Average pooling, it expected to keeps the relevant information while reducing the dimension.</p><h2 id=cae-network-design>CAE Network Design<a hidden class=anchor aria-hidden=true href=#cae-network-design>#</a></h2><p>The network design for Convolutional based autoencoder was basically same with the Linear one. There are Encoder, Decoder, and Latent space. However, the key differences is located on how we implement Encoder and Decoder layers.</p><h3 id=encoder-layer>Encoder Layer<a hidden class=anchor aria-hidden=true href=#encoder-layer>#</a></h3><p>The Encoder layer design is not complicated as it is very similar to common CNN implementation for classification. The layers, of course, designed to shrink as it closer to the latent space, expecting it to extract key features of our images and map it to $C\times W\times H$ of latent space.</p><p>where:</p><p>$C:$ channels</p><p>$W:$ image width</p><p>$H:$ image height</p><p><input type=checkbox id=zoomCheck-30129 hidden>
<label for=zoomCheck-30129><img class=zoomCheck loading=lazy decoding=async src=images/convolutional_encoder.png#center alt="convolutional encoder.png"></label></p><p>In the last layer of encoder output, you may choose to flatten it then feed to Linear layer or leave it as is for the latent space.</p><p><em>(P.S. if you have opinions or best practices, please don’t hesitate to leave it in comment section)</em></p><h3 id=decoder-layer>Decoder Layer<a hidden class=anchor aria-hidden=true href=#decoder-layer>#</a></h3><p>The Decoder layer design in general still the same to regular autoencoder network, which is need to expand over the time (the opposite shape of encoder layer). However, there are some adjustment to the convolutional layer for the decoder.</p><p>While convolutional layer is intended to reduce the data dimension size, we need to make it do the opposite for the implementation in Decoder layer. Instead of shrink the dimension, it needs to upsample it. Fortunately, in PyTorch we can achieve it easily with <code>Conv2dTranspose</code> layer.</p><p><input type=checkbox id=zoomCheck-d957e hidden>
<label for=zoomCheck-d957e><img class=zoomCheck loading=lazy decoding=async src=images/convtranspose.gif#center alt="Convolutional Transpose. source: https://stackoverflow.com/a/55119869"></label></p><p>source: <a href=https://stackoverflow.com/a/55119869>https://stackoverflow.com/a/55119869</a></p><p>In short, we can say it do the opposite operation of regular CNN layer. And we will use it to upsample our latent space back to its original size image.</p><p><input type=checkbox id=zoomCheck-e37cb hidden>
<label for=zoomCheck-e37cb><img class=zoomCheck loading=lazy decoding=async src=images/convolutional_decoder.png#center alt="convolutional decoder.png"></label></p><p>If you are looking for more explanation on this upsample operation, I recommend you to watch <a href="https://www.youtube.com/watch?v=345wRyqKkQ0&amp;list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&amp;index=139">Intro to Deep Learning and Generative Models Course</a> by <a href=https://www.youtube.com/@SebastianRaschka>Sebastian Raschka</a>.</p><h2 id=coding-time>Coding Time!<a hidden class=anchor aria-hidden=true href=#coding-time>#</a></h2><p>This time we will try to denoise flowers image using Convolutional Autoencoders. We will use 16 different <a href=https://www.kaggle.com/datasets/l3llff/flowers>Flowers dataset which available in Kaggle</a>.</p><p>If you wants to jump ahead into the notebook, <a href=https://www.kaggle.com/code/affand20/denoising-autoencoder/notebook>please visit this link</a>.</p><h3 id=dataset-preparation>Dataset Preparation<a hidden class=anchor aria-hidden=true href=#dataset-preparation>#</a></h3><p>Let’s import the necessary libraries and setup our base config.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># data manipulation</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># utils</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gzip</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>string</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tqdm.auto</span> <span class=kn>import</span> <span class=n>tqdm</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># sklearn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.datasets</span> <span class=kn>import</span> <span class=n>ImageFolder</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.utils.data</span> <span class=kn>import</span> <span class=n>DataLoader</span><span class=p>,</span> <span class=n>Subset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>config</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>epochs</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>1e-3</span>
</span></span><span class=line><span class=cl>    <span class=n>log_step</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl>    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span>
</span></span><span class=line><span class=cl>    <span class=n>latent_dim</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>    <span class=n>inp_out_dim</span> <span class=o>=</span> <span class=mi>784</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>128</span>
</span></span></code></pre></div><p>We wants to make sure that all images must be in size 256 x 256. Therefore, we can add a transformation operation to crop and resize the image.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># transform data</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>RandomResizedCrop</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>    <span class=c1># crop and/or resize image to 256x256</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span></code></pre></div><p>Now, since our dataset is only images stored in folders, we may load it to pytorch using ImageFolder utils from torchvision like this. Don’t forget to pass our previously defined transformation operations.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># load data from folder</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>ImageFolder</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s2>&#34;/kaggle/input/flowers/flowers&#34;</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>target</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>targets</span>
</span></span></code></pre></div><p>Then, let’s make train–test split and setup our dataloaders.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># make train test split</span>
</span></span><span class=line><span class=cl><span class=n>train_idx</span><span class=p>,</span> <span class=n>test_idx</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>)),</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>stratify</span><span class=o>=</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_subset</span> <span class=o>=</span> <span class=n>Subset</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>indices</span><span class=o>=</span><span class=n>train_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_subset</span> <span class=o>=</span> <span class=n>Subset</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>indices</span><span class=o>=</span><span class=n>test_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># make trainloader</span>
</span></span><span class=line><span class=cl><span class=n>trainloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_subset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>testloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>test_subset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=designing-model-architecture>Designing Model Architecture<a hidden class=anchor aria-hidden=true href=#designing-model-architecture>#</a></h3><p>It’s time to implement our previous discussion on the convolutional autoencoder network!</p><p>For the encoder part, let’s stack 4 Convolutional layers with shrinking output to its end.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># encoder</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Here, I am trying to reduce the data dimension by using only <code>stride</code> parameter instead of employing the Max Pooling layer. You may try it by yourself to incorporate the Max Pooling layer and leave the <code>stride</code> parameter as 1.</p><blockquote><p>💡 <em>There is also discussion whether to use stride or max pooling layer for reducing data dimension. You can find it here: <a href=https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling>https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling</a></em></p></blockquote><p>During the process, I found difficulties to guess what is the output size of each convolution layers. Then I found the full explanation and guide from Stanford which I find very useful. Here is the link: <a href=https://cs231n.github.io/convolutional-networks/#layers>https://cs231n.github.io/convolutional-networks/#layers</a>.</p><p>Or simply, you can follow this formula which I obtained from the link above.</p><p>$$
(W−F+2P)/S+1
$$</p><p>where:</p><p>$W:$ input volume size</p><p>$F:$ kernel (filter) size</p><p>$P:$ padding size</p><p>$S:$ stride size</p><p>Now, for the decoder part, as the opposite of the encoder layer, let’s stack 4 Transpose Convolutional layers with expanding output to its end.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># decoder</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>   <span class=c1># for the final layer I used kernel size 2, as I found it empirically match the original input image size</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Honestly, specific on this part I still had a difficulties to tell the reason why the <code>padding</code> need to set as 0 and the final <code>stride</code> to 1. What I did was actually following the tutorial <a href=https://github.com/udacity/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/Denoising_Autoencoder_Solution.ipynb>here</a> and <a href=https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L16/conv-autoencoder_mnist.ipynb>here</a> and do my observation through try and error by feeding random tensor in equal shape with the latent space to this network.</p><p>While to calculate the output of each Transpose Convolutional layer, I follow its formula defined on PyTorch page here: <a href=https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html>https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html</a>.</p><p>Finally, the forward function is defined as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>The full code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># define our network</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ConvDenoiseAutoencoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>ConvDenoiseAutoencoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># encoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># here we downsample the image using only stride instead of maxpool.</span>
</span></span><span class=line><span class=cl>        <span class=c1># you may try maxpool approach as it is only my experiment using stride.</span>
</span></span><span class=line><span class=cl>        <span class=c1># see discussion here: https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># decoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>   <span class=c1># for the final layer I used kernel size 2, as I found it empirically match the original input image size</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>Let’s define our model and see its architecture.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># define model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>ConvDenoiseAutoencoder</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># move to GPU device</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>ConvDenoiseAutoencoder(
  (encoder): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
  (decoder): Sequential(
    (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ReLU()
    (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2))
    (3): ReLU()
    (4): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2))
    (5): ReLU()
    (6): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(1, 1))
    (7): Sigmoid()
  )
)
</code></pre><h3 id=add-noise-to-images>Add Noise to Images<a hidden class=anchor aria-hidden=true href=#add-noise-to-images>#</a></h3><p>In order to make our model understand to clean the noise from images, we first need to introduce a random noise to the original images.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># get one batch from trainloader</span>
</span></span><span class=line><span class=cl><span class=n>features</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=nb>iter</span><span class=p>(</span><span class=n>trainloader</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># define random noise and how much its influence</span>
</span></span><span class=line><span class=cl><span class=n>noise_factor</span> <span class=o>=</span> <span class=mf>.5</span>
</span></span><span class=line><span class=cl><span class=n>noise</span> <span class=o>=</span> <span class=n>noise_factor</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=o>*</span><span class=n>features</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>noisy_img</span> <span class=o>=</span> <span class=n>features</span> <span class=o>+</span> <span class=n>noise</span>
</span></span><span class=line><span class=cl><span class=c1># keep image colors stay in scale between 0 to 1</span>
</span></span><span class=line><span class=cl><span class=n>features</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>noisy_img</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span>
</span></span></code></pre></div><p><input type=checkbox id=zoomCheck-e93c4 hidden>
<label for=zoomCheck-e93c4><img class=zoomCheck loading=lazy decoding=async src=images/ori_vs_noisy.png#center alt="ori vs noisy.png"></label></p><p>Note that the denoising autoencoder learn the representation by measuring the distance between the generated image and the original one. The generated image here is the attempt output of denoising autoencoder network.</p><p>By minimizing the distance between noisy and original one, it means the model will also learn to remove the noise or at least reduce it.</p><h3 id=noise-probability>Noise Probability<a hidden class=anchor aria-hidden=true href=#noise-probability>#</a></h3><p>During training phase, I found its challenging for our model to learn image representation if the whole training data are corrupted. Therefore, as part of my experiment, I tried to add threshold probability to determine whether batch of images need to be corrupted or not.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># add noise to image</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&gt;</span> <span class=n>p</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>noise_factor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>noise</span> <span class=o>=</span> <span class=n>noise_factor</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=o>*</span><span class=n>features</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>noisy_img</span> <span class=o>=</span> <span class=n>features</span> <span class=o>+</span> <span class=n>noise</span>
</span></span><span class=line><span class=cl>        <span class=n>features</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>noisy_img</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span>
</span></span></code></pre></div><p>By adding threshold probability, we let some non-corrupted dataset to be learned as well. And by doing so, we hope the model will be able to learn underlying images representation and have a little insight on difference between the corrupted and non-corrupted images.</p><h2 id=training-model>Training Model<a hidden class=anchor aria-hidden=true href=#training-model>#</a></h2><p>Let’s now define our training loop. Here we would like to wrap our code inside a function, so we can reproduce easily later and the code itself become cleaner.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>noise_factor</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=n>p_threshold</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># loss and optimizer</span>
</span></span><span class=line><span class=cl>    <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>learning_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># loss logging</span>
</span></span><span class=line><span class=cl>    <span class=n>history</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;train_loss&#39;</span><span class=p>:</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># progressbar</span>
</span></span><span class=line><span class=cl>    <span class=n>num_train_steps</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>trainloader</span><span class=p>)</span> <span class=o>*</span> <span class=n>config</span><span class=o>.</span><span class=n>epochs</span>
</span></span><span class=line><span class=cl>    <span class=n>progressbar</span> <span class=o>=</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_train_steps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>epochtime</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>trainloss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>batchtime</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>batch</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>trainloader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># unpack data</span>
</span></span><span class=line><span class=cl>            <span class=n>features</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>batch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>						<span class=o>**</span><span class=c1># add noise to image</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&gt;</span> <span class=n>p</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>noise_factor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>noise</span> <span class=o>=</span> <span class=n>noise_factor</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=o>*</span><span class=n>features</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>noisy_img</span> <span class=o>=</span> <span class=n>features</span> <span class=o>+</span> <span class=n>noise</span>
</span></span><span class=line><span class=cl>                    <span class=n>features</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>noisy_img</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span><span class=o>**</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>features</span> <span class=o>=</span> <span class=n>features</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># clear gradient</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># forward pass</span>
</span></span><span class=line><span class=cl>            <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># calculate loss</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># optimize</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1>#  update running training loss</span>
</span></span><span class=line><span class=cl>            <span class=n>trainloss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># update progressbar</span>
</span></span><span class=line><span class=cl>            <span class=n>progressbar</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>progressbar</span><span class=o>.</span><span class=n>set_postfix_str</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># log step</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>idx</span> <span class=o>%</span> <span class=n>config</span><span class=o>.</span><span class=n>log_step</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Epoch: </span><span class=si>%03d</span><span class=s2>/</span><span class=si>%03d</span><span class=s2> | Batch: </span><span class=si>%04d</span><span class=s2>/</span><span class=si>%04d</span><span class=s2> | Loss: </span><span class=si>%.4f</span><span class=s2>&#34;</span> \
</span></span><span class=line><span class=cl>                      <span class=o>%</span> <span class=p>((</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=p>),</span> <span class=n>config</span><span class=o>.</span><span class=n>epochs</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> \
</span></span><span class=line><span class=cl>                         <span class=nb>len</span><span class=p>(</span><span class=n>trainloader</span><span class=p>),</span> <span class=n>trainloss</span> <span class=o>/</span> <span class=p>(</span><span class=n>idx</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># log epoch</span>
</span></span><span class=line><span class=cl>        <span class=n>history</span><span class=p>[</span><span class=s1>&#39;train_loss&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>trainloss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>trainloader</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;***Epoch: </span><span class=si>%03d</span><span class=s2>/</span><span class=si>%03d</span><span class=s2> | Loss: </span><span class=si>%.3f</span><span class=s2>&#34;</span> \
</span></span><span class=line><span class=cl>              <span class=o>%</span> <span class=p>((</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=p>),</span> <span class=n>config</span><span class=o>.</span><span class=n>epochs</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># log time</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Time elapsed: </span><span class=si>%.2f</span><span class=s1> min&#39;</span> <span class=o>%</span> <span class=p>((</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>batchtime</span><span class=p>)</span> <span class=o>/</span> <span class=mi>60</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Total Training Time: </span><span class=si>%.2f</span><span class=s1> min&#39;</span> <span class=o>%</span> <span class=p>((</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>epochtime</span><span class=p>)</span> <span class=o>/</span> <span class=mi>60</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span><span class=p>,</span> <span class=n>history</span>
</span></span></code></pre></div><p>And the function call itself.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=p>,</span> <span class=n>history</span> <span class=o>=</span> <span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>noise_factor</span><span class=o>=</span><span class=mf>0.15</span><span class=p>,</span> <span class=n>p_threshold</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span></span></code></pre></div><p>Here we are training our model with probability of data corruption 0.5 and the noise factor itself 0.15.</p><p>After training for a while, let’s now plot our train loss history.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>7</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;train_loss&#39;</span><span class=p>])),</span> <span class=n>history</span><span class=p>[</span><span class=s1>&#39;train_loss&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Train Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epochs&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;MSE Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><input type=checkbox id=zoomCheck-8bd18 hidden>
<label for=zoomCheck-8bd18><img class=zoomCheck loading=lazy decoding=async src=images/stride_w_noise_proba_losses.png#center alt="stride w noise proba losses.png"></label></p><h2 id=testing-time>Testing time!<a hidden class=anchor aria-hidden=true href=#testing-time>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>testloss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>testtime</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>noise_factor</span> <span class=o>=</span> <span class=mf>0.25</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=n>testloader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># unpack data</span>
</span></span><span class=line><span class=cl>    <span class=n>test_feats</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>batch</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=c1># add noise to image</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>noise_factor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>noise</span> <span class=o>=</span> <span class=n>noise_factor</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=o>*</span><span class=n>test_feats</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>noisy_img</span> <span class=o>=</span> <span class=n>test_feats</span> <span class=o>+</span> <span class=n>noise</span>
</span></span><span class=line><span class=cl>        <span class=n>test_feats</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>noisy_img</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span><span class=o>**</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>test_feats</span> <span class=o>=</span> <span class=n>test_feats</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># forward pass</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>test_out</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>test_feats</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># compute loss</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>test_out</span><span class=p>,</span> <span class=n>test_feats</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>testloss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Test Loss: </span><span class=si>%.4f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=n>testloss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>testloader</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Total Testing Time: </span><span class=si>%.2f</span><span class=s1> min&#39;</span> <span class=o>%</span> <span class=p>((</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>testtime</span><span class=p>)</span> <span class=o>/</span> <span class=mi>60</span><span class=p>))</span>
</span></span></code></pre></div><p><input type=checkbox id=zoomCheck-a3189 hidden>
<label for=zoomCheck-a3189><img class=zoomCheck loading=lazy decoding=async src=images/Untitled.png#center alt=Untitled></label></p><h3 id=visualize-our-test-result>Visualize our Test Result<a hidden class=anchor aria-hidden=true href=#visualize-our-test-result>#</a></h3><p>Let’s take the first 5 original and reconstructed image from latest batch in test set.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>orig</span> <span class=o>=</span> <span class=n>test_feats</span><span class=p>[:</span><span class=mi>5</span><span class=p>]</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>recon</span> <span class=o>=</span> <span class=n>test_out</span><span class=p>[:</span><span class=mi>5</span><span class=p>]</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=n>sharex</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>sharey</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>row</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>([</span><span class=n>orig</span><span class=p>,</span> <span class=n>recon</span><span class=p>],</span> <span class=n>axes</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>img</span><span class=p>,</span> <span class=n>ax</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>row</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>img</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>get_xaxis</span><span class=p>()</span><span class=o>.</span><span class=n>set_visible</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>get_yaxis</span><span class=p>()</span><span class=o>.</span><span class=n>set_visible</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></span></code></pre></div><p><input type=checkbox id=zoomCheck-3e6c3 hidden>
<label for=zoomCheck-3e6c3><img class=zoomCheck loading=lazy decoding=async src=images/stride_w_noise_p_proba_inference.png alt="stride w noise p proba inference.png"></label></p><p>Great! Based on the result, although the image not completely denoised, our model seems to be able reduce it a little bit. You may also try to inference your denoised image several time and see if the noise will be more reduced.</p><h2 id=further-exploration>Further Exploration<a hidden class=anchor aria-hidden=true href=#further-exploration>#</a></h2><h3 id=experiment-with-different-model-architecture>Experiment with Different Model Architecture<a hidden class=anchor aria-hidden=true href=#experiment-with-different-model-architecture>#</a></h3><p>To fulfill my curiosity, I tried incorporate the Max Pooling layer. So I changed the model architecture to be like this.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ConvDenoiseAutoencoderV3</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>ConvDenoiseAutoencoderV3</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># encoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># decoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>   <span class=c1># for the final layer I used kernel size 2, as I found it empirically match the original input image size</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><pre tabindex=0><code>ConvDenoiseAutoencoderV3(
  (encoder): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): ReLU()
    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (decoder): Sequential(
    (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
    (1): ReLU()
    (2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))
    (3): ReLU()
    (4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    (5): ReLU()
    (6): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))
    (7): ReLU()
    (8): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): Sigmoid()
  )
)
</code></pre><h3 id=experimental-result>Experimental Result<a hidden class=anchor aria-hidden=true href=#experimental-result>#</a></h3><p>We keep the training setup same to our previous setup. The only difference is now we are setting noise probability to 0 and noise_factor to 0.5, ensuring that all images are quite badly corrupted.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=p>,</span> <span class=n>history</span> <span class=o>=</span> <span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>noise_factor</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>p_threshold</span><span class=o>=</span><span class=mf>0.0</span><span class=p>)</span>
</span></span></code></pre></div><p>With that settings, here is our training loss looks like.</p><p><input type=checkbox id=zoomCheck-9033d hidden>
<label for=zoomCheck-9033d><img class=zoomCheck loading=lazy decoding=async src=images/maxpool_losses.png#center alt="maxpool losses.png"></label></p><p>And finally, this is our denoised images using such architecture.</p><p><input type=checkbox id=zoomCheck-a1992 hidden>
<label for=zoomCheck-a1992><img class=zoomCheck loading=lazy decoding=async src=images/maxpool_inference.png#center alt="maxpool inference.png"></label></p><p>From the output we could tell that the model might be able to remove noise better than our previous model architecture. However, the generated result seems to be blurred.</p><p>Again, our model architecture is very simple compared to the latest image reconstruction architecture. Of course the performance and reconstruction quality is differs by large margin. You may try to improve the model architecture or implement published paper’s architecture.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>Now we know more advanced Autoencoder architecture using Convolutional layers. The key difference is how our image will be processed through each layer. It works by reducing the dimension of images and projected to specific size of dimension.</p><p>We also discovered how to corrupt image simply by creating random noise and add it to our data. By learning the distance between corrupted images and the original one, it is expected that our model is able to remove (or at least reduce) the noise from image, leaving better image quality.</p><p>If you have any inquiries, comments, suggestions, or critics please don’t hesitate to reach me out:</p><ul><li>Mail: <a href=mailto:affahrizain@gmail.com>affahrizain@gmail.com</a></li><li>LinkedIn: <a href=https://www.linkedin.com/in/fahrizainn/>https://www.linkedin.com/in/fahrizainn/</a></li><li>GitHub: <a href=https://github.com/fhrzn>https://github.com/fhrzn</a></li></ul><p>Until next time! 👋</p><hr><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ol><li><a href="https://www.youtube.com/watch?v=345wRyqKkQ0&amp;list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&amp;index=139">Intro to Deep Learning and Generative Models Course</a> by <a href=https://www.youtube.com/@SebastianRaschka>Sebastian Raschka</a>.</li><li><a href=https://github.com/udacity/deep-learning-v2-pytorch/blob/master/autoencoder/denoising-autoencoder/Denoising_Autoencoder_Solution.ipynb>Udacity Intro to Deep Learning with Pytorch Github repos</a></li><li><a href=https://cs231n.github.io/convolutional-networks/#layers>CS231n Convolutional Neural Networks for Visual Recognition</a> by Stanford</li><li><a href=https://stats.stackexchange.com/questions/387482/pooling-vs-stride-for-downsampling>Pooling vs Stride for downsampling</a></li><li><a href=https://www.v7labs.com/blog/autoencoders-guide>Autoencoders in Deep Learning</a> by V7</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://fhrzn.github.io/tags/autoencoder/>Autoencoder</a></li><li><a href=https://fhrzn.github.io/tags/deeplearning/>Deeplearning</a></li></ul><nav class=paginav><a class=prev href=https://fhrzn.github.io/posts/pain-free-python-fastapi-rmq-integration/><span class=title>« Prev</span><br><span>Pain-free Python Fastapi RabbitMQ Integration</span>
</a><a class=next href=https://fhrzn.github.io/posts/autoencoders-your-first-step-into-generative-ai/><span class=title>Next »</span><br><span>Autoencoders: Your First Step into Generative AI</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=fhrzn/fhrzn.github.io data-repo-id=R_kgDOK-oOOw data-category=Q&A data-category-id=DIC_kwDOK-oOO84CcI9D data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://fhrzn.github.io/>fahrizain</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>