[{"content":"","permalink":"http://localhost:1313/hugo-PaperMod/posts/on-building-voice-agent-for-fashion-recommendation/","summary":"","title":"On Building Voice Agent for Fashion Recommendation"},{"content":"Background We have an MySQL table that holds 600 million data since 2023.\n","permalink":"http://localhost:1313/hugo-PaperMod/posts/migrating-600-million-data-using-airflow-and-pyspark/","summary":"I was asked to migrate 600M+ data from our internal MySQL DB to BigQuery. Here is how I did it using incremental load strategy.","title":"Migrating 600 Million Data using Airflow and PySpark"},{"content":"TLDR; In this article, we will show how to parse PDF and webpage into Markdown format – which can preserve document structure for better LLM understanding.\nDocument Parsing In RAG, it is very common to upload our documents (i.e. PDF, spreadsheet, docs, or website) to the vector database so we can augment the LLM knowledge from it. One integral step is parsing the document itself, which the result must represent the document in the best way.\nThe easiest way to do that, is just read the file and get the contents. However, it is very possible to make the content losing it structures. Gratefully, we can mitigate this by parsing the document into structured format – Markdown. To the best of my experience, most LLMs are work best with Markdown format – especially OpenAI\u0026rsquo;s family.\nPDF To parse PDF file into Markdown, we can leverage the extended version of PyMuPDF library, called PyMuPDF4LLM.\nThis library is designed specifically to produce the best output that works well with LLM. Without further ado, let\u0026rsquo;s see how we can use it to parse PDF document.\nFirst, we need to install the library itself.\npip install pymupdf4llm We will use these sample PDFs from https://www.princexml.com/samples/. Optionally, you may also use your own PDF file.\nFortunately, it is very easy to convert it into markdown using pymupdf4llm. In this article we will only work with one of them. You can find the rest implementation on GitHub repo.\nimport pymupdf4llm # these files are downloaded from link above filepath = { \u0026#34;invoice1\u0026#34;: \u0026#34;assets/invoice.pdf\u0026#34;, \u0026#34;invoice2\u0026#34;: \u0026#34;assets/invoice2.pdf\u0026#34;, \u0026#34;brochure\u0026#34;: \u0026#34;assets/brochure.pdf\u0026#34;, \u0026#34;newsletter\u0026#34;: \u0026#34;assets/newsletter.pdf\u0026#34;, \u0026#34;textbook\u0026#34;: \u0026#34;assets/textbook.pdf\u0026#34;, } newsletter = pymupdf4llm.to_markdown(filepath[\u0026#34;newsletter\u0026#34;], show_progress=False) print(newsletter) Here is the parsed document output:\n# DrylabNews #### for investors \u0026amp; friends · May 2017 Welcome to our first newsletter of 2017! It\u0026#39;s been a while since the last one, and a lot has happened. We promise to keep them coming every two months hereafter, and permit ourselves to make this one rather long. The big news is the beginnings of our launch in the American market, but there are also interesting updates on sales, development, mentors and (of course) the investment round that closed in January. **New capital: The investment round was** successful. We raised 2.13 MNOK to match the 2.05 MNOK loan from Innovation Norway. Including the development agreement with Filmlance International, the total new capital is 5 MNOK, partly tied to the successful completion of milestones. All formalities associated with this process are now finalized. **New owners: We would especially like to** warmly welcome our new owners to the Drylab family: Unni Jacobsen, Torstein Jahr, Suzanne Bolstad, Eivind Bergene, Turid Brun, Vigdis Trondsen, Lea Blindheim, Kristine ## 34 ### meetingsmeetings NY · SFNY · SF LA · LLA · LVV Academy of Motion Picture Arts and Sciences · Alesha \u0026amp; Jamie Metzger · Amazon AWS · Apple · Caitlin Burns, PGA · Carlos Melcer · Chimney L.A. · Dado Valentic · Dave Stump · DIT WIT · ERA NYC · Facebook · Fancy Film · FilmLight · Geo Labelle · Google · IBM · Innovation Norway (NYC) · Innovation Norway (SF) · International Cinematographers Guild · NBC · Local 871 · Netflix · Pomfort · Radiant Images · Screening Room · Signiant · Moods of Norway · Tapad · Team Downey ----- Holmsen, Torstein Hansen, and Jostein Aanensen. We look forward to working with you! **Sales: Return customer rate is now 80%,** proving value and willingness to pay. Film Factory Montreal is our first customer in Canada. Lumiere Numeriques have started using us in France. We also have new customers in Norway, and high-profile users such as Gareth Unwin, producer of Oscar[winning The King\u0026#39;s Speech. Revenue for the](http://www.imdb.com/title/tt1504320/) first four months is 200 kNOK, compared to 339 kNOK for all of 2016. We are working on a partnership to safeguard sales in Norway while beginning to focus more on the US. ... (We trimmed the output) From the result above, we can see there is ----- token denoting different pages.\nAlthough, the parsed result is not perfect, the output\u0026rsquo;s structure is good enough as it also maintain separation of each document parts.\nAs comparison, we shall look into Langchain PyPDFLoader implementation.\n💡 Make sure you already install langchain on your machine.\nfrom langchain_community.document_loaders import PyPDFLoader lc_newsletter = \u0026#34;\\n==================\\n\u0026#34;.join(doc.page_content for doc in list(PyPDFLoader(filepath[\u0026#34;newsletter\u0026#34;]).lazy_load())) print(lc_newsletter) 💡 Note that langchain document loader\u0026rsquo;s implementation always return list of langchain Document object – each page represented by an object. Therefore we join them with separator token ================== to denote different pages.\nHere is the result of Langchain PyPDFLoader:\nDrylab Newsfor in vestors \u0026amp; friends · Ma y 2017 Welcome to our first newsletter of 2017! It\u0026#39;s been a while since the last one, and a lot has happened. W e promise to k eep them coming every two months hereafter , and permit ourselv es to mak e this one r ather long. The big news is the beginnings of our launch in the American mark et, but there are also interesting updates on sales, de velopment, mentors and ( of course ) the in vestment round that closed in January . New c apital: The in vestment round was successful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation Norwa y. Including the de velopment agreement with Filmlance International, the total new capital is 5 MNOK, partly tied to the successful completion of milestones. All formalities associated with this process are now finalized. New o wners: We would especially lik e to warmly welcome our new owners to the Drylab family: Unni Jacobsen, T orstein Jahr , Suzanne Bolstad, Eivind Bergene, T urid Brun, Vigdis T rondsen, L ea Blindheim, Kristine 34meetingsmeetings NY · SFNY · SF LA · LLA · L VVAcadem yofMotion Picture Arts and Sciences ·Alesha \u0026amp;Jamie Metzger ·Amazon AWS ·Apple ·Caitlin Burns, PGA ·Carlos Melcer ·Chimne yL.A.·Dado Valentic · DaveStump ·DIT WIT ·ERA NYC·Facebook ·Fancy Film ·FilmLight ·Geo Labelle · Google ·IBM ·Inno vation Norwa y(NY C)·Inno vation Norwa y(SF) ·International Cinematogr aphers Guild ·NBC ·Local 871 ·Netflix ·Pomfort ·Radiant Images · Screening Room · Signiant · Moods of Norwa y· Tapad · Team Downe y ================== Holmsen, T orstein Hansen, and Jostein Aanensen. W e look forward to working with you! Sales: Return customer r ate is now 80%, pro ving value and willingness to pa y. Film Factory Montreal is our first customer in Canada. Lumiere Numeriques ha ve started using us in F rance. W e also ha ve new customers in Norwa y, and high-profile users such as Gareth Un win, producer of Oscar- winning The King\u0026#39;s Speech . Re venue for the first four months is 200 kNOK, compared to 339 kNOK for all of 2016. W e are working on a partnership to safeguard sales in Norwa y while beginning to focus more on the US. Pay attention the output documents structure are not preserved, making its hard to identify which part originally belongs to. In addition, if we take a closer look, some captured words are strangely separated by random whitespace.\nAlthough the LLM may still can understand it, I believe giving better input representation will produce better output as well. Therefore, parsing into markdown format is a good choice to enhance LLM understanding of our document.\nNow we already know how to parse PDF into markdown format. What if I am telling you that you can do the same to webpages?\nLet\u0026rsquo;s look how to do it.\nWebpage To parse webpage into Markdown, we can utilize Jina AI Reader API.\nJina AI give an API key we can use for the first 1M token processed. Once it reaches the limit, we need to top up if we are intended to use the API key.\nAfraid not, we can still use their service (for now) even without the API key. Simply do the request with omitting the API key. In this example, we will try to omit the API key.\n🚧 Please note the rate limiter is very tight when we are not using the API key. Make sure we are doing graceful requests, otherwise we will get Error 429 – Too many requests\nTo use it, we only need to perform GET request to their endpoint.\nSkip it if you have installed it already\npip install requests We will try to parse one of my article Exploring Vision Transformers (ViT) with 🤗 Huggingface. You may also change it into any other websites.\nimport requests import os BASE_URL = \u0026#34;https://r.jina.ai\u0026#34; site = \u0026#34;https://fhrzn.github.io/posts/building-conversational-ai-context-aware-chatbot/\u0026#34; url = os.path.join(BASE_URL, site) resp = requests.get(url) print(resp.text) And here is the parsed result:\nTitle: Vision Transformers (ViT) with 🤗 Huggingface | Data Folks Indonesia URL Source: https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0 Published Time: 2022-10-14T11:00:46.983Z Markdown Content: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021) ----------------------------------------------------------------------------------------------------- [![Image 1: Affandy Fahrizain](https://miro.medium.com/v2/resize:fill:88:88/1*JCueIcAZjfbCE_ro4ZB8Og.jpeg)](https://medium.com/@fahrizain?source=post_page-----8cdda82920a0--------------------------------)[![Image 2: Data Folks Indonesia](https://miro.medium.com/v2/resize:fill:48:48/1*s8T4-0fscxMhh6V8adR4mQ.png)](https://medium.com/data-folks-indonesia?source=post_page-----8cdda82920a0--------------------------------) Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration! ![Image 3](https://miro.medium.com/v2/resize:fit:700/0*et8V-t6bjFm1w6ds) Photo by [Alex Litvin](https://unsplash.com/@alexlitvin?utm_source=medium\u0026amp;utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium\u0026amp;utm_medium=referral) Background ---------- As self-attention based model like Transformers has successfully become a _standard_ in NLP area, it triggers researchers to adapt attention-based models in Computer Vision too. There were different evidences, such as combine CNN with self-attention and completely replace Convolutions. While this selected paper belongs to the latter aproach. The application of attention mechanism in images requires each pixel attends to every other pixel, which indeed requires expensive computation. Hence, several techniques have been applied such as self-attention only in local neighborhoods \\[1\\], using local multihead dot product self-attention blocks to completely replace convolutions \\[2\\]\\[3\\]\\[4\\], postprocessing CNN outputs using self- attention \\[5\\]\\[6\\], etc. Although shown promising results, these techniques quite hard to be scaled and requires complex engineering to be implemented efficiently on hardware accelerators. On the other hand, Transformers model is based on MLP networks, it has more computational efficiency and scalability, making its possible to train big models with over 100B parameters. Methods ------- ![Image 4](https://miro.medium.com/v2/resize:fit:700/1*-HQPfbnebarylP543i58_Q.png) General architecture of ViT. Taken from the original paper (Dosovitskiy et al., 2021) The original Transformers model treat its input as sequences which very different approach with CNN, hence the inputted images need to be extracted into fixed-size patches and flattened. Similar to BERT \\[CLS\\] token, the so-called _classification token_ will be added into the beginning of the sequences, which will serve as image representation and later will be fed into classification head. Finally, to retain the positional information of the sequences, positional embedding will be added to each patch. The authors designed model following the original Transformers as close as possible. The proposed model then called as Vision Transfomers (ViT). ... (We trimmed the output) Please also pay attention, for some website this technique might not works very well. That probably caused by firewall or cloudflare protection. You may use the proxy to mitigate it.\nAlso, there are a lot more options provided by Jina AI Reader. You may find it out here https://jina.ai/reader/#apiform.\nConclusion Maintaining document structures can ensure the quality of LLM response when it getting asked about our documents. Therefore, choosing the right tools is essential. PyMuPDF4LLM ensure the parsed document output is given in markdown format, which is great in maintaining the document structures.\nOn the other side, Langchain implementation is easy to use. Unfortunately, it lack of ability to preserve document structure. One may extend Langchain API to create a PyMuPDF4LLM integration. So that can take benefit from both sides.\nShould you have other opinions or feedbacks, please never hesitate to comment below!\nLet’s get Connected 🙌 If you have any inquiries, comments, suggestions, or critics please don’t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","permalink":"http://localhost:1313/hugo-PaperMod/posts/llm-document-parsing-tips/","summary":"TLDR; In this article, we will show how to parse PDF and webpage into Markdown format – which can preserve document structure for better LLM understanding.","title":"Enhance Your LLM’s Understanding: Document Parsing Tips You Need to Know"},{"content":"While the LLM is powerful already, it is not impossible to extend its capability to answer the question according to our private data (which never exposed to the LLM itself before). In this part, we will explore how to inject the LLM with our data and ask anything about it!\nIn this part, we will extend the code from the first part. So if you haven\u0026rsquo;t see it yet, please have a look here.\n⚡️ Additionally, if you want to jump directly to the code, you may access it here.\nLet\u0026rsquo;s gooo! 💪\nIntroduction to RAG Before we start, let\u0026rsquo;s first talk about what is RAG and why we should know about it.\nSo, RAG stands for Retrieval Augmented Generation is a method which we will use to inject additonal knowledge (our private data) to the LLM. Note that the LLM capability in answering our questions are relied on the knowledge it has been trained for.\nNow, take a look on the diagram below. There are two new components introduced here, namely embedding and vectordb which we will discuss more in the following sections.\nIn short, we retrieve the relevant documents from vectordb according to user input. Then, we add the output to the prompt to inject some knowledge from our private data. So the LLM will able to answer our question based on those data.\nNow, let\u0026rsquo;s discuss a bit further about Embedding and VectorDB.\nEmbedding If you are quite new to AI field, you may be wondering what is Embedding actually?\nSimply put, embedding is just another AI model. However, the key difference is it doesn\u0026rsquo;t predict labels or next tokens like the way LLM did. Instead, it only give us strange bunch of float numbers called a vector representation.\nHere is the example of embedding result.\nSee? The result is just float numbers with specific size. In the example above, I just show the example of encoded hello world! using OpenAI embedding model. The size generated by each embedding model itself vary, in this case OpenAI embedding model produced a vector sized 1536.\nBut are we really need this embedding?\nWell, in essence the LLM itself do embedding process to understand and generate sentences. Probably I will make another post about it later.\nBut in this case, we always need to encode our text to interact with VectorDB. As its literal name, a Vector Database store vector representation instead of tabular dataset or file-based data like we\u0026rsquo;ve known in SQL and NoSQL database.\nLet\u0026rsquo;s now discuss a little bit about VectorDB itself.\nVector Database Unlike the other databases we know so far, vector database require us to encode the data we want to store to be encoded in a vector representation. And it can be achieved by leveraging an Embedding model. Not just during ingestion process, the query process also require a vector representation as its input.\nYou may be curious, then how the query process works in VectorDB?\nSince we are interacting with vectors, the usage of similarity algorithm become the standard way to retrieve data. Remember that we need to encode our query sentence? The vectordb itself internally will find the closest document according to our query.\nThere are various similarity algorithm can be used in this case. However, the most popular are cosine similarity, L2/euclidean distance, and inner product.\nNow we have covered the RAG concept and its component as well. Let\u0026rsquo;s move to coding section!\nCoding Time! ☕️ We will have slight modification where we add vectordb collection options to choose which data we are going to ask to the chatbot. And we will add one more page for our data ingestion to the vectordb.\nLets get started!\nSetup VectorDB Here we will use Milvus lite which a lightweight version of open-source Milvus vector database.\nFirst thing first, install the library:\npip install \u0026#34;pymilvus\u0026gt;=2.4.2\u0026#34; 🚨 This lite version is good only for building demos and prototype. It is not recommended to use it in production environment. Instead, run Milvus locally with Docker or use the cloud version.\nNext, let\u0026rsquo;s put all vectordb logic into a file named knowledge.py. Let\u0026rsquo;s start by creating function for opening connection to vectordb.\n# knowledge.py from pymilvus import MilvusClient import logging logger = logging.getLogger(__file__) MILVUS = None def init_vectordb(path: Optional[str] = None): if not path: path = \u0026#34;./milvus.db\u0026#34; global MILVUS if not MILVUS: logger.info(\u0026#34;initiating vectordb\u0026#34;) MILVUS = MilvusClient(path) Then, create functions to upload and ingest data, get available collections in database, and perform query.\nfrom typing import Union, List, Optional from pymilvus import model import re from langchain_community.document_loaders import CSVLoader, PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_core.documents import Document import gradio as gr import time def upload_file(collection_name: str, file: Union[str, list[str], None]): start_upload = time.time() re_ptn = r\u0026#39;((\\w+?\\-)+)?\\w+\\.(csv|pdf)\u0026#39; filename = re.search(re_ptn, file).group() extension = filename.split(\u0026#39;.\u0026#39;)[-1] if extension == \u0026#39;csv\u0026#39;: loader = CSVLoader(file) data = loader.load() elif extension == \u0026#39;pdf\u0026#39;: loader = PyPDFLoader(file) data = loader.load() splitter = RecursiveCharacterTextSplitter() data = splitter.split_documents(data) else: raise NotImplementedError(f\u0026#34;Loader for {extension} not implemented yet.\u0026#34;) __encode_and_insert(MILVUS, data, collection_name) # re-retrieve all collection_name to update the dropdown collections = get_collections() logger.info(f\u0026#34;Time elapsed {time.time() - start_upload:.1f}s | Collection name: {collection_name}\u0026#34;) return [ gr.Textbox(value=None), gr.File(value=None), gr.Dropdown(choices=collections, interactive=True, value=collections[0]), gr.Tabs(selected=\u0026#34;chat\u0026#34;) ] def __encode_and_insert(client: MilvusClient, data: List[Document], collection_name: str): # extract content content = [item.page_content for item in data] # encode content to vector embedding_fn = model.DefaultEmbeddingFunction() vectors = embedding_fn.encode_documents(content) data = [{\u0026#34;id\u0026#34;: i, \u0026#34;vector\u0026#34;: vectors[i], **data[i].dict()} for i in range(len(vectors))] client.create_collection( collection_name=collection_name, dimension=embedding_fn.dim, # auto_id=True ) client.insert(collection_name=collection_name, data=data) Here we will only create a file parser for csv and pdf only using LangChain document loaders. Additionally, for pdf file we apply text splitter to split documents into smaller chunks. Other than that, we will throw an error.\nAfter documents parsed, we will encode them using an embedding model. Here we use the default embedding provided by Milvus which refer to all-MiniLM-L6-v2. However, you may use other embeddings such as openai, vertex, sentence-transformer, etc. In addition, you may want to visit Milvus page about embedding here.\nOnce the document encoded into vector representation, all we need to do is pack it with the text we parsed earlier in a list of dictionary. Finally, the create_collection() function will do the rest for us.\nNote that when creating a collection, we need to supply the dimension of vector representation. This value must match with the size of our embedding model.\nIf you are wondering why we are returning gradio components, dont worry about it now because the explanation is up ahead.\nFor now, lets move to functions for getting collections and perform query.\ndef get_collections(): init_vectordb() collections = MILVUS.list_collections() return collections def query(query: str, collection_name: str): query_str = query if isinstance(query, dict) and \u0026#34;input\u0026#34; in query: query_str = query[\u0026#34;input\u0026#34;] start_query = time.time() embedding_fn = model.DefaultEmbeddingFunction() query_vector = embedding_fn.encode_queries([query_str]) result = MILVUS.search( collection_name=collection_name, data=query_vector, output_fields=[ \u0026#34;page_content\u0026#34; ], limit=1000 ) logger.info(f\u0026#34;Time elapsed: {time.time() - start_query:.1f}s | Query: \\\u0026#34;{query_str}\\\u0026#34;\u0026#34;) context_str = \u0026#34;\u0026#34; for res in result: for r in res: context_str += r[\u0026#39;entity\u0026#39;][\u0026#39;page_content\u0026#39;] + \u0026#34;\\n\u0026#34; return context_str To retrieve available collections in the database, we use list_collections function from Milvus client. Additionally, we also call init_vectordb to make sure that the Milvus instance is created before getting the collections.\nThen, to perform query we need to put our string query and collection name. As we\u0026rsquo;ve discussed earlier, our query string needs to be converted into vector representation as well before we passed it to the search function.\nFinally, we concatenate the retrieved document into a single string which then will be passed to the LLM.\nHere is the full version of our knowledge.py file.\nfrom pymilvus import MilvusClient, model from typing import Union, List, Optional import logging import re from langchain_community.document_loaders import CSVLoader, PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_core.documents import Document import gradio as gr import os import time logger = logging.getLogger(__file__) MILVUS = None def init_vectordb(path: Optional[str] = None): if not path: path = os.getenv(\u0026#34;MILVUS_DB_PATH\u0026#34;) global MILVUS if not MILVUS: logger.info(\u0026#34;initiating vectordb\u0026#34;) MILVUS = MilvusClient(path) def close_vectordb(): if MILVUS: logger.info(\u0026#34;closing vectordb\u0026#34;) MILVUS.close() def upload_file(collection_name: str, file: Union[str, list[str], None]): start_upload = time.time() re_ptn = r\u0026#39;((\\w+?\\-)+)?\\w+\\.(csv|pdf|txt)\u0026#39; filename = re.search(re_ptn, file).group() extension = filename.split(\u0026#39;.\u0026#39;)[-1] if extension == \u0026#39;csv\u0026#39;: loader = CSVLoader(file) data = loader.load() elif extension == \u0026#39;pdf\u0026#39;: loader = PyPDFLoader(file) data = loader.load() splitter = RecursiveCharacterTextSplitter() data = splitter.split_documents(data) else: raise NotImplementedError(f\u0026#34;Loader for {extension} not implemented yet.\u0026#34;) __encode_and_insert(MILVUS, data, collection_name) # re-retrieve all collection_name to update the dropdown collections = get_collections() logger.info(f\u0026#34;Time elapsed {time.time() - start_upload:.1f}s | Collection name: {collection_name}\u0026#34;) return [ gr.Textbox(value=None), gr.File(value=None), gr.Dropdown(choices=collections, interactive=True, value=collections[0]), gr.Tabs(selected=\u0026#34;chat\u0026#34;) ] def __encode_and_insert(client: MilvusClient, data: List[Document], collection_name: str): # extract content content = [item.page_content for item in data] # encode content to vector embedding_fn = model.DefaultEmbeddingFunction() vectors = embedding_fn.encode_documents(content) data = [{\u0026#34;id\u0026#34;: i, \u0026#34;vector\u0026#34;: vectors[i], **data[i].dict()} for i in range(len(vectors))] client.create_collection( collection_name=collection_name, dimension=embedding_fn.dim, # auto_id=True ) client.insert(collection_name=collection_name, data=data) def get_collections(): init_vectordb() collections = MILVUS.list_collections() return collections def query(query: str, collection_name: str): query_str = query if isinstance(query, dict) and \u0026#34;input\u0026#34; in query: query_str = query[\u0026#34;input\u0026#34;] start_query = time.time() embedding_fn = model.DefaultEmbeddingFunction() query_vector = embedding_fn.encode_queries([query_str]) result = MILVUS.search( collection_name=collection_name, data=query_vector, output_fields=[ \u0026#34;page_content\u0026#34; ], limit=1000 ) logger.info(f\u0026#34;Time elapsed: {time.time() - start_query:.1f}s | Query: \\\u0026#34;{query_str}\\\u0026#34;\u0026#34;) context_str = \u0026#34;\u0026#34; for res in result: for r in res: context_str += r[\u0026#39;entity\u0026#39;][\u0026#39;page_content\u0026#39;] + \u0026#34;\\n\u0026#34; return context_str Great, now let\u0026rsquo;s move to the UI!\nUpdate the UI Let\u0026rsquo;s create a new section for uploading our file to vectordb, then wrap both chat interface and the new section in a tab interface.\n# main.py import knowledge with gr.Blocks(fill_height=True) as demo: with gr.Tabs() as tabs: with gr.TabItem(\u0026#34;Chat\u0026#34;, id=\u0026#34;chat\u0026#34;): models = get_free_models() collections = knowledge.get_collections() user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) with gr.Row(): model_choice = gr.Dropdown( choices=models, show_label=True, label=\u0026#34;Model Choice\u0026#34;, interactive=True, value=models[0], ) collection_list = gr.Dropdown( choices=collections, label=\u0026#34;VectorDB Collection\u0026#34;, interactive=True, value=collections[0] if collections else None ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1, height=600) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids, collection_list], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) with gr.TabItem(\u0026#34;Knowledge\u0026#34;, id=\u0026#34;knowledge\u0026#34;): collection_name = gr.Textbox(label=\u0026#34;Collection Name\u0026#34;) upfile = gr.File(label=\u0026#34;Upload File\u0026#34;) submit_file = gr.Button(\u0026#34;Submit Knowledge\u0026#34;, variant=\u0026#34;primary\u0026#34;) submit_file.click(knowledge.upload_file, inputs=[collection_name, upfile], outputs=[collection_name, upfile, collection_list, tabs]) In the chat interface part, there are slight changes where we get list of collections then supply it to a dropdown called collection_list.\nAlso, pay attention since we want to pass the collection name we are using for RAG, we need to pass the collection_list dropdown as additional inputs of ChatInterface.\nWhile on the new section (lets call it ingestion section) quite simple, there are only 3 components: a textbox for collection name, an upload file, and a submit button.\nTake a look on our submit button click action, we take both textbox for collection name and upload file as inputs. Then, we define 4 components as the output, there are both textbox collection_name and the upfile, the collection_list dropdown on the chat interface section, and finally the root of Tab interface itself called tabs.\nBriefly, after the upload file is done we want to switch to chat interface tab automatically and populate the latest list of collection_name in the dropdown. And to do that, we need to update components state by putting them as the output of button click action.\nLet\u0026rsquo;s recall a little bit to our code which responsible for handling uploaded files in knowledge.py earlier, where we returned 4 gradio components.\n# knowledge.py def upload_file(collection_name: str, file: Union[str, list[str], None]): ... __encode_and_insert(MILVUS, data, collection_name) # re-retrieve all collection_name to update the dropdown collections = get_collections() return [ gr.Textbox(value=None), gr.File(value=None), gr.Dropdown(choices=collections, interactive=True, value=collections[0]), gr.Tabs(selected=\u0026#34;chat\u0026#34;) ] Look that we always call get_collections() retrieve collections from vectordb after inserting the file to vectordb.\nThere we set both textbox and upload file component values as None, because we want to reset them to initial state i.e. no prefilled text and file.\nFinally, we populate the retrieved collections list to the dropdown and set the selected tabs to chat which is an id of chat interface tab.\nHere is the full code of our main.py\nimport gradio as gr from dotenv import load_dotenv import logging from chatbot import predict_chat import knowledge import httpx import uuid load_dotenv() logging.basicConfig(level=logging.INFO, format=\u0026#34;[%(levelname)s] %(filename)s:%(lineno)d - %(message)s\u0026#34;) logger = logging.getLogger(__name__) # MILVUS_CLIENT = None def get_free_models(): res = httpx.get(\u0026#34;https://openrouter.ai/api/v1/models\u0026#34;) if res: res = res.json() models = [item[\u0026#34;id\u0026#34;] for item in res[\u0026#34;data\u0026#34;] if \u0026#34;free\u0026#34; in item[\u0026#34;id\u0026#34;]] return sorted(models) with gr.Blocks(fill_height=True) as demo: with gr.Tabs() as tabs: with gr.TabItem(\u0026#34;Chat\u0026#34;, id=\u0026#34;chat\u0026#34;): models = get_free_models() collections = knowledge.get_collections() user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) with gr.Row(): model_choice = gr.Dropdown( choices=models, show_label=True, label=\u0026#34;Model Choice\u0026#34;, interactive=True, value=models[0], ) collection_list = gr.Dropdown( choices=collections, label=\u0026#34;VectorDB Collection\u0026#34;, interactive=True, value=collections[0] if collections else None ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1, height=600) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids, collection_list], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) with gr.TabItem(\u0026#34;Knowledge\u0026#34;, id=\u0026#34;knowledge\u0026#34;): collection_name = gr.Textbox(label=\u0026#34;Collection Name\u0026#34;) upfile = gr.File(label=\u0026#34;Upload File\u0026#34;) submit_file = gr.Button(\u0026#34;Submit Knowledge\u0026#34;, variant=\u0026#34;primary\u0026#34;) submit_file.click(knowledge.upload_file, inputs=[collection_name, upfile], outputs=[collection_name, upfile, collection_list, tabs]) demo.load(lambda: knowledge.get_collections()) if __name__ == \u0026#34;__main__\u0026#34;: demo.queue() demo.launch() Great! At this point, we should able to play around with upload file functionality. To test it out, you may use the example file I\u0026rsquo;ve provided here or you may want to use your own file.\nUpdate Chatbot Logic Cool, now just the main part left. Here we will adapt our previous code to perform RAG. Lets goo! 🔥\nFirst of all, let\u0026rsquo;s update our prompt.\nprompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;You are an AI assistant that capable to interact with users using friendly tone. \u0026#34; \u0026#34;Whenever you think it needed, add some emojis to your response. No need to use hashtags.\u0026#34; \u0026#34;\\n\\n\u0026#34; \u0026#34;Answer user\u0026#39;s query based on the following context:\\n\u0026#34; \u0026#34;{context}\u0026#34; \u0026#34;\\n---------------\\n\u0026#34; \u0026#34;Chat history:\\n\u0026#34;), MessagesPlaceholder(\u0026#34;history\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) The key difference is now we have a new templated variable in our prompt denoted by {context} which later will be filled with retrieved documents from vectordb.\nPlease also note since we need selected collection name from the ui, we need to add it as function parameter as well. Here, our predict_chat has a new parameter collection_name.\ndef predict_chat(message: str, history: list, model_name: str, user_id: str, collection_name: str): ... Then, we will use LangChain\u0026rsquo;s Runnable to wrap our retrieval function we created earlier.\nfrom langchain_core.runnables import RunnableLambda import knowledge from functools import partial # runnable for retrieving knowledge query_runnable = RunnableLambda(partial(knowledge.query, collection_name=collection_name)) Here RunnableLambda is useful to wrap the custom function that later will be executed within the chains. In this case, our custom function is the query function inside knowledge.py file.\nEssentially, it accept a function with a single parameter only. But since our query function consist of two parameters, we use partial here to prefill collection_name argument.\nIf you are curious what those each line did, you may debug by invoking each runnable like this:\n# debugging query_runnable = RunnableLambda(partial(knowledge.query, collection_name=collection_name)) _context = query_runnable.invoke(input=message) logger.info(f\u0026#34;context: {_context}\u0026#34;) \u0026gt;\u0026gt;\u0026gt; context: Product: St. Isaac Cathedral Ticket Category: Trips Qty (Kg): 1 Price (Ruble): 284 Total Price (Ruble): 284 Purchased at: August 1, 2022 2:51 PM (GMT+3) Store: Trips Source Account: Tinkoff Purchase Date: August 1, 2022 6:51 PM (GMT+7) ... As we can see, when we invoke the runnable it will perform query to vectordb and returning the concatenated relevant documents.\nThen, let\u0026rsquo;s put our runnable result to chain using RunnablePassThrough.\nchain = ( RunnablePassthrough.assign(context=query_runnable) | prompt | llm ) Shortly, we can think its there to inject our retrieved context from vectordb to the prompt. The name of argument we put there should match with the variable we want to inject in prompt template.\nFinally, the full code will look like this:\n# chatbot.py from langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.runnables.history import RunnableWithMessageHistory from langchain_community.chat_message_histories import SQLChatMessageHistory from langchain.callbacks.tracers import ConsoleCallbackHandler from typing import Optional import logging import os from langchain_core.runnables import RunnablePassthrough, RunnableLambda import knowledge from functools import partial logger = logging.getLogger(__name__) def get_chat_history(session_id: str, limit: Optional[int] = None, **kwargs): if isinstance(session_id, dict) and \u0026#34;session_id\u0026#34; in session_id: session_id = session_id[\u0026#34;session_id\u0026#34;] chat_history = SQLChatMessageHistory(session_id=session_id, connection_string=\u0026#34;sqlite:///memory.db\u0026#34;) if limit: chat_history.messages = chat_history.messages[-limit:] return chat_history def predict_chat(message: str, history: list, model_name: str, user_id: str, collection_name: str): prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;You are an AI assistant that capable to interact with users using friendly tone. \u0026#34; \u0026#34;Whenever you think it needed, add some emojis to your response. No need to use hashtags.\u0026#34; \u0026#34;\\n\\n\u0026#34; \u0026#34;Answer user\u0026#39;s query based on the following context:\\n\u0026#34; \u0026#34;{context}\u0026#34; \u0026#34;\\n---------------\\n\u0026#34; \u0026#34;Chat history:\\n\u0026#34;), MessagesPlaceholder(\u0026#34;history\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) # Optionally, you may use this format as well # prompt = ChatPromptTemplate.from_template( # \u0026#39;You are an AI assistant that capable to interact with users using friendly tone.\u0026#39; # \u0026#39;Whenever you think it needed, add some emojis to your response. No need to use hashtags.\u0026#39; # \u0026#39;\\n\\n\u0026#39; # \u0026#39;Answer user\\\u0026#39;s input based on the following context below. If the context doesn\\\u0026#39;t contains\u0026#39; # \u0026#39;the suitable answers, just say you dont know. Dont make up the answer!\\n\u0026#39; # \u0026#39;{context}\u0026#39; # \u0026#39;\\n---------------\\n\u0026#39; # \u0026#39;Chat history:\\n\u0026#39; # \u0026#39;{history}\u0026#39; # \u0026#39;\\n---------------\\n\u0026#39; # \u0026#39;User input: {input}\u0026#39; # ) llm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;), openai_api_base=os.getenv(\u0026#34;OPENROUTER_BASE\u0026#34;) ) # runnable for retrieving knowledge query_runnable = RunnableLambda(partial(knowledge.query, collection_name=collection_name)) # ##### FOR DEBUGGING ONLY ##### # _context = query_runnable.invoke(input=message) # logger.info(f\u0026#34;context: {_context}\u0026#34;) chain = ( RunnablePassthrough.assign(context=query_runnable) | prompt | llm ) history_runnable = RunnableWithMessageHistory( chain, get_session_history=get_chat_history, input_messages_key=\u0026#34;input\u0026#34;, history_messages_key=\u0026#34;history\u0026#34; ) ################ #### STREAM #### ################ partial_msg = \u0026#34;\u0026#34; for chunk in history_runnable.stream({\u0026#34;input\u0026#34;: message}, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: user_id}, \u0026#34;callbacks\u0026#34;: [ConsoleCallbackHandler()]}): partial_msg = partial_msg + chunk.content yield partial_msg ######################## ##### REGULAR CALL ##### ######################## # response = chain.invoke({\u0026#34;input\u0026#34;: message, \u0026#34;session_id\u0026#34;: user_id}, config={\u0026#34;callbacks\u0026#34;: [ConsoleCallbackHandler()]}) # yield response.content Moment of Truth! Let\u0026rsquo;s now test our chatbot!\nHere is subset of the file I\u0026rsquo;m going to ask it. Demo: And that\u0026rsquo;s it, yeay! 🥳\nFull Project https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter\nConclusion In this article, we start improving our chatbot from having context-aware conversation into having capability to answer question based on the given data, with still maintain its core function to aware with previous contexes.\nWe also discussed a bit about embeddings, vector database, and why both is needed in RAG system. Although it was only brief explanation and very simplified project, I hope it is useful especially for those who want to deep dive in recent trend of AI Engineering.\nIn the upcoming article there are interesting topics I want to discuss for example tracking our token usages and LLM responses, perform automatic evaluation, and also demonstrating how we can extract information from images using multimodal LLM.\nStay tune! 👋\nReferences How to invoke runnables in parallel Parallel: Formatting inputs and outputs Runnable Lambda: Run custom functions RunnableWithMessageHistory in RAG pipeline Milvus lite Run Milvus lite locally Let\u0026rsquo;s get Connected 🙌 If you like this article or it useful for you please let me know through the comment section below. Should be there any improvement and suggestions for me you can also reach me out here. Cheers! 🥂\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","permalink":"http://localhost:1313/hugo-PaperMod/posts/building-conversational-ai-chat-with-your-data/","summary":"LLM is powerful, but it cannot answer questions it doesn\u0026rsquo;t know before. Thanks to RAG we can inject some knowledge to extend LLM capability. Now let\u0026rsquo;s build our first RAG!","title":"Building Conversational AI with LangChain Part 2: Chat with Your Data"},{"content":"Since the rise of LLMs era such as ChatGPT, Bard, Gemini, Claude, etc. the development of AI based application has been drastically changed. We moved from the conventional in-house training (which require high machine spec) to ready-to-use APIs.\nFortunately, there are free APIs curated on OpenRouter platform. So we can build our simple chatbot without spending a penny!\nLet\u0026rsquo;s get started! 🔥\n⚡️ If you want to jump directly to the repository, you can access it here.\nProject Brief Before we start working, let\u0026rsquo;s take a look on high level concept below of how it will works. For those who are just started learning LLM, this brief information can help you to understand more how LLM-based application works.\nBasically, LLM works by taking user input and answer them based on its internal knowledge. If we want our LLM to do specific task such as brainstorming, making travel plan, calculating our expenses, or etc. we will need more advanced and structured user input. This is what we called Prompt.\nHere are very simple illustrations of the difference in user input structure when adding a prompt and not.\nHuman: \u0026lt;user query/input here\u0026gt; Assistant: \u0026lt;chatbot answer here\u0026gt; System Prompt: \u0026lt;system prompt here\u0026gt; Human: \u0026lt;user query/input here\u0026gt; Assistant: \u0026lt;chatbot answer here\u0026gt; At this point, our LLM should be able to do specific and more advanced task. However, the LLM doesn\u0026rsquo;t remember the prior conversation and every time we invoke LLM call it assume that current user input as initial conversation with the user. To solve this problem, we need to add our previous conversation to the prompt. So that everytime we send user input, the LLM has knowledge of prior conversation which makes it remember previous conversation contexes. At the end of conversation, right after the LLM given its responses, we need to save both user input and LLM response.\nHere is the simple illustration of our system prompt which incorporating previous conversation.\nYou are professional travel planner. You are able to convert different timezone to the desired timezone quick and precisely. ... ... --------------------- Chat history: {chat_history} Pay attention that we are giving clear separation between system prompt (act as basic command) and previous chat history (as additional knowledge/context). And we put chat_history variable in curly braces that intended to be replaced with our retrieved previous conversation later. We will talk about it more in technical implementation.\nIn short, our chatbot will combine both user input and previous chat history in a prompt. Then, it will passed to LLM as the input. The LLM then responsible to generate answer based on given input. Finally, we save current conversation (user input and LLM response) as chat history which will be consumed again later.\nSetup OpenRouter API Key Mostly, each LLM has different APIs to the others. That makes switching between models become less straightforward. OpenRouter lift those problem for us by providing a unified interface which allow us to change between models easily with very minimal code changes.\nNow, to get started make sure you already created an account in https://openrouter.ai/. Then, go to Keys page and create your API Key. Remember to save it somewhere save as it won\u0026rsquo;t show twice. Start Coding! 🔥 Setup Environment In python, it is advised to create individual virtualenv to isolate our libraries. This can reduce the possibility of error due to conflict on library versions. We will use default python\u0026rsquo;s virtualenv to make one.\nRun the commands below on terminal\nmkdir simple-rag-openrouter \u0026amp;\u0026amp; cd simple-rag-openrouter python -m venv venv source .venv/bin/activate After running the commands above, a new folder named venv should be appeared in our project directory. All of our installed library will be saved there.\nInstall Libraries Make sure you already activate the virtualenv. Then, run this command to install required libraries.\npip install langchain langchain_openai gradio langchain_community uuid httpx Create Project Environment Variables Now, create a new file named .env. We will store our openrouter api key and base url there. So we can ensure our published codes didn\u0026rsquo;t contains any confidential information.\nYou also may create the file using terminal\ntouch .env Then, open the .env file we just created and put our credentials there.\nOPENROUTER_BASE = \u0026#34;https://openrouter.ai/api/v1\u0026#34; OPENROUTER_API_KEY = \u0026#34;\u0026lt;your-openrouter-api-key\u0026gt;\u0026#34; To make our code reproducable by other person, let\u0026rsquo;s dump our installed python libraries to a file called requirements.txt.\npip freeze \u0026gt; requirements.txt Building our Chatbot Interface There are a various options to build chat UI, but here we will use gradio\u0026rsquo;s ChatInterface which very handy to use.\nLet\u0026rsquo;s create a python file called main.py and put the codes below.\nimport gradio as gr from dotenv import load_dotenv load_dotenv() def predict_chat(): # TODO: we will put our LLM call here later pass with gr.Blocks(fill_height=True) as demo: chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) if __name__ == \u0026#34;__main__\u0026#34;: demo.queue() demo.launch() To run our script, run the following command in terminal.\ngradio main.py And we can already see the chat interface provided by gradio. Now, since openrouter provide a lot of LLM models we can choose and switch between them every time we want to send message. Think of the LLM here as replacable module that we can change and set to any model we want. Let\u0026rsquo;s add all available openrouter models, but limited to the free version only as we dont want to spend any money. To get the full list of available models, we can perform API request to openrouter\u0026rsquo;s endpoint here https://openrouter.ai/api/v1/models. Then we can put the available models as dropdown options above the chat interface.\nFirst, create a new function to get all available free models.\nimport httpx def get_free_models(): res = httpx.get(\u0026#34;https://openrouter.ai/api/v1/models\u0026#34;) if res: res = res.json() # filter only free models models = [item[\u0026#34;id\u0026#34;] for item in res[\u0026#34;data\u0026#34;] if \u0026#34;free\u0026#34; in item[\u0026#34;id\u0026#34;]] return sorted(models) Then, inside add dropdown component and populate the model names.\nwith gr.Blocks(fill_height=True) as demo: models = get_free_models() # get model names model_choice = gr.Dropdown( choices=models, # populate model names show_label=True, label=\u0026#34;Model Choice\u0026#34;, interactive=True, value=models[0] ) Finally, add the newly added component as Chat Interface additional inputs.\nchat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) Our main.py file should be look like this.\nimport gradio as gr from dotenv import load_dotenv import httpx load_dotenv() def predict_chat(): # TODO: we will put our LLM call here later pass def get_free_models(): res = httpx.get(\u0026#34;https://openrouter.ai/api/v1/models\u0026#34;) if res: res = res.json() # filter only free models models = [item[\u0026#34;id\u0026#34;] for item in res[\u0026#34;data\u0026#34;] if \u0026#34;free\u0026#34; in item[\u0026#34;id\u0026#34;]] return sorted(models) with gr.Blocks(fill_height=True) as demo: models = get_free_models() model_choice = gr.Dropdown( choices=models, # populate model names show_label=True, label=\u0026#34;Model Choice\u0026#34;, interactive=True, value=models[0] ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) if __name__ == \u0026#34;__main__\u0026#34;: demo.queue() demo.launch() The code above should add dropdown list with free models name available in OpenRouter. Now we have all necessary components for our UI. However, we still can\u0026rsquo;t play around with the LLMs as we haven\u0026rsquo;t put logic to handle user-chatbot interactions yet.\nLet\u0026rsquo;s add some logic there!\nAdd Chatbot Logic Remember that we\u0026rsquo;ve created a function called predict_chat() earlier? Now, to make the code cleaner we will move it to a new file named chatbot.py. We will put everything related to chatbot logic there including manage the conversation history.\nWe will use the prompt below to give specific task to the LLM.\nSystem prompt:\nYou are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\nLet\u0026rsquo;s write some codes to file chatbot.py. Don\u0026rsquo;t forget to create one if you don\u0026rsquo;t have it yet.\nFirst, let\u0026rsquo;s create a prompt template to put our prompt and user input together.\nfrom langchain.prompts import ChatPromptTemplate def predict_chat(message: str, history: list, model_name: str): prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) Here, we are using ChatPromptTemplate as we want to format the prompt in the conversation style. There are only 3 roles available, system, human, and AI.\nThe predict_chat() function takes 3 input, namely message, history, and model_name. Both message and history is required by default as we used gradio\u0026rsquo;s ChatInterface. While the model_name came from the model names dropdown in the main.py file.\n💡 If you\u0026rsquo;re curious how in the world those dropdown input automatically required in the predict_chat function, it is because we put that component into additional_inputs in Chat Interface parameter.\nNext, let\u0026rsquo;s initiate our LLM instance. Then chain our prompt and LLM together.\nllm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;), openai_api_base=os.getenv(\u0026#34;OPENROUTER_BASE\u0026#34;) ) # chain prompt and LLM instance using LCEL chain = prompt | llm Notice that we chain prompt and LLM together using pipe (|) symbol. Thanks to LangChain Expression Language (LCEL) we can write this handy shorthand.\nFinally, we invoke the chain in stream mode so we can see the progressive output while it generates the full response.\npartial_msg = \u0026#34;\u0026#34; # for chunk in history_runnable.stream({\u0026#34;input\u0026#34;: message}, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: user_id}, \u0026#34;callbacks\u0026#34;: [ConsoleCallbackHandler()]}): for chunk in chain.stream({\u0026#34;input\u0026#34;: message}): partial_msg = partial_msg + chunk.content yield partial_msg Our chatbot.py should be look like this now.\nfrom langchain.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI import os def predict_chat(message: str, history: list, model_name: str): prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) llm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;), openai_api_base=os.getenv(\u0026#34;OPENROUTER_BASE\u0026#34;) ) chain = prompt | llm partial_msg = \u0026#34;\u0026#34; # for chunk in history_runnable.stream({\u0026#34;input\u0026#34;: message}, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: user_id}, \u0026#34;callbacks\u0026#34;: [ConsoleCallbackHandler()]}): for chunk in chain.stream({\u0026#34;input\u0026#34;: message}): partial_msg = partial_msg + chunk.content yield partial_msg Now, go back for a while to main.py and add this line on import section.\nfrom chatbot import predict_chat Cool! At this point, we can start playing around with our chatbot and it will respond to our chats!\nContext-Aware Response Generation Our chatbot can respond to our question already. However, it lack of previous conversation contexes. Take a look on the captured conversation below. The previous context was talking about travel plan, but when I tried to continue the conversation it gave me an answer that doesn\u0026rsquo;t have correlation with previous context. To work on this issue, we need to put the chat history to our prompt. Here we will use SQLite as our database to save the whole chat history. Since we will only have one database for all users, we need a session_id between each user conversation history to avoid retrieving wrong user\u0026rsquo;s conversation.\nWe will first add a hidden input in chat interface that generate a unique UUID which will act as our session_id. So, everytime we refresh the page, it will generate new session_id as well.\nimport uuid with gr.Blocks(fill_height=True) as demo: user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) Next, add the hidden input component as Chat Interface additional_inputs as well. So, now Chat Interface additional inputs should contains model_choice and user_ids. Otherwise, we cannot pass the value to the function predict_chat() behind.\nchat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) Our final main.py should be look like this.\nimport gradio as gr from dotenv import load_dotenv import httpx from chat import predict_chat import uuid load_dotenv() def get_free_models(): res = httpx.get(\u0026#34;https://openrouter.ai/api/v1/models\u0026#34;) if res: res = res.json() models = [item[\u0026#34;id\u0026#34;] for item in res[\u0026#34;data\u0026#34;] if \u0026#34;free\u0026#34; in item[\u0026#34;id\u0026#34;]] return sorted(models) with gr.Blocks(fill_height=True) as demo: user_ids = gr.Textbox(visible=False, value=uuid.uuid4()) models = get_free_models() model_choice = gr.Dropdown( choices=models, show_label=True, label=\u0026#34;Model Choice\u0026#34;, interactive=True, value=models[0] ) chat_window = gr.Chatbot(bubble_full_width=False, render=False, scale=1) chat = gr.ChatInterface( predict_chat, chatbot=chat_window, additional_inputs=[model_choice, user_ids], fill_height=True, retry_btn=None, undo_btn=None, clear_btn=None ) if __name__ == \u0026#34;__main__\u0026#34;: demo.queue() demo.launch() Now, we\u0026rsquo;re done with main.py. Let\u0026rsquo;s move further to chatbot.py file.\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory def get_chat_history(session_id: str): chat_history = SQLChatMessageHistory( session_id=session_id, connection_string=\u0026#34;sqlite:///memory.db\u0026#34;) return chat_history Then, we\u0026rsquo;ll a new variable named history in our prompt using MessagesPlaceholder. The rest of the prompt attribute stays the same. Also, don\u0026rsquo;t forget to add user_id to our predict_chat function parameter\ndef predict_chat(message: str, history: list, model_name: str, user_id: str): prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\u0026#34;), MessagesPlaceholder(\u0026#34;history\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) Next, instead of we call invoke directly on chained prompt and LLM instance, we will use a new instance called RunnableWithMessageHistory.\nfrom langchain_core.runnables.history import RunnableWithMessageHistory history_runnable = RunnableWithMessageHistory( chain, get_session_history=get_chat_history, input_messages_key=\u0026#34;input\u0026#34;, history_messages_key=\u0026#34;history\u0026#34; ) Remember that we should always save our current conversation to database so we can use it in future interaction with chatbot? Gratefully, all those logic already handled by RunnableWithMessageHistory so we don\u0026rsquo;t have to handle it by ourselves.\nNote that we also put input and history as input and history message key respectively. Keep in mind that this key should match with variables key that already defined in prompt.\nFinally, rather than calling stream from chain, we call it from history_runnable instead. So our streaming code will look like this.\npartial_msg = \u0026#34;\u0026#34; for chunk in history_runnable.stream({\u0026#34;input\u0026#34;: message}, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: user_id}}): partial_msg = partial_msg + chunk.content yield partial_msg Our final main.py should be look like this.\nfrom langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.runnables.history import RunnableWithMessageHistory from langchain_community.chat_message_histories import SQLChatMessageHistory def get_chat_history(session_id: str): chat_history = SQLChatMessageHistory(session_id=session_id, connection_string=\u0026#34;sqlite:///memory.db\u0026#34;) return chat_history def predict_chat(message: str, history: list, model_name: str, user_id: str): prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;You are an AI assistant that capable to interact with users using friendly tone. Whenever you think it needed, add some emojis to your response. No need to use hashtags.\u0026#34;), MessagesPlaceholder(\u0026#34;history\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) llm = ChatOpenAI( model=model_name, openai_api_key=os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;), openai_api_base=os.getenv(\u0026#34;OPENROUTER_BASE\u0026#34;) ) chain = prompt | llm history_runnable = RunnableWithMessageHistory( chain, get_session_history=get_chat_history, input_messages_key=\u0026#34;input\u0026#34;, history_messages_key=\u0026#34;history\u0026#34; ) partial_msg = \u0026#34;\u0026#34; for chunk in history_runnable.stream({\u0026#34;input\u0026#34;: message}, config={\u0026#34;configurable\u0026#34;: {\u0026#34;session_id\u0026#34;: user_id}}): partial_msg = partial_msg + chunk.content yield partial_msg That\u0026rsquo;s a wrap! Our chatbot now can understand previous conversation context. Super! ⚡️ Full Project https://github.com/fhrzn/study-archive/tree/master/simple-rag-openrouter\nConclusion Throughout this article, we already covered how to build a context-aware chatbot \u0026ndash; a chatbot that understand previous conversation contexes.\nIn the upcoming article I will demonstrate how we can extend this chatbot to be able interact with external files as well such as financial reports, product catalogs, or even company profile website.\nStay tune! 👋\nReferences LangChain getting started Add message history (memory) LCEL (LangChain Expression Language) OpenRouter docs Let\u0026rsquo;s get Connected 🙌 If you have any inquiries, comments, suggestions, or critics please don’t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn ","permalink":"http://localhost:1313/hugo-PaperMod/posts/building-conversational-ai-context-aware-chatbot/","summary":"We moved from in-house training model to hosted models and ready-to-use APIs. With the existence of free LLM APIs, let\u0026rsquo;s explore how to create our own free chatbot!","title":"Building Conversational AI with LangChain: Techniques for Context Retention in Chatbots"},{"content":"Background I was working on implementing RabbitMQ (RMQ) in FastAPI for doing background task. Initially, there was a long process in our system, and the execution time is not deterministic. Making the some request caught in the timeout error. To solve it, we come up with a message broker based solution.\nIn this case, our system has to communicate with itself to do the long process in the background. So the origin request won\u0026rsquo;t be expired. Optionally, once the process is done it can be asked to notify the user.\nThe illustration of our FastAPI interaction with RabbitMQ producer and consumer.\nPub/Sub Pattern The publisher/subscriber pattern using message broker is one of the common pattern. It also covered in the official documentation of RabbitMQ. However, the common pattern is to have publisher and consumer as different application because usually the communication is happened between two or more services. While in our case, we don\u0026rsquo;t want to create another service just for processing our task in background. Therefore, we need to have both publisher and subscriber in a single service.\nUsually, in more complex system there could be multiple publisher and subscriber for different events. But in this case, we will demonstrate it using only one publisher and subscriber which we will call it as producer and consumer. The simple illustration of single pub/sub.\nIntegration with FastAPI Initially, I did my experiments by running producer and consumer in dedicated threads; and using asyncio connection. The threads solution always gave me an error pika.exceptions.StreamLostError: Stream connection lost: BrokenPipeError(32, 'Broken pipe') after idle for several minutes. While the asyncio based one always blocking the same thread that used to serve endpoint routes. Meaning when the background process is running, any calls to the endpoints should be wait for it to be finished.\nFinally, I found the solution using aio-pika library, and the decided to built my solution on top of it. So, let\u0026rsquo;s start demonstrate it using simple FastAPI project.\nNote: I won\u0026rsquo;t explain basic terms and definitions of each RMQ part. Feel free to visit their official tutorial to better understand the terms and definitions\nRabbitMQ (Pika) documentation\naio-pika documentation\nFirst thing first, lets create a rmq.py file for our PikaClient which will be interact with RMQ.\nimport logging import aio_pika import asyncio logger = logging.getLogger(__name__) class PikaClient(): def __init__(self, queue_name: str, exchange_name: str, conn_str: str) -\u0026gt; None: self.queue_name = queue_name self.exchange_name = exchange_name self.conn_str = conn_str self.connection = None self.channel = None self.exchange = None self.queue = None async def start_connection(self): logger.info(\u0026#34;Starting a new connection\u0026#34;) self.connection = await aio_pika.connect_robust(url=self.conn_str) logger.info(\u0026#34;Opening a new channel\u0026#34;) self.channel = await self.connection.channel() logger.info(\u0026#34;Declaring an exchange: %s\u0026#34; % self.exchange_name) self.exchange = await self.channel.declare_exchange(name=self.exchange_name, type=aio_pika.ExchangeType.DIRECT) await self.setup_queue() async def setup_queue(self): logger.info(\u0026#34;Setup a queue: %s\u0026#34; % self.queue_name) self.queue = await self.channel.declare_queue(name=self.queue_name) logger.info(\u0026#34;Bind queue to exchange\u0026#34;) await self.queue.bind(self.exchange) async def disconnect(self): try: if not self.connection.is_closed: await self.connection.close() except Exception as _e: logger.error(_e) Here we just created the basic interface for interacting with RMQ. It consists of starting a new RMQ connection, opening a new channel, declaring an exchange, and setup a queue.\nThen, let\u0026rsquo;s add the following functions to enable run PikaClient as producer.\nasync def start_producer(self): await self.start_connection() logger.info(\u0026#34;Producer has been started\u0026#34;) return self async def publish_message(self, message): await self.exchange.publish( aio_pika.Message(body=message.encode()), routing_key=self.queue_name ) Once the client is connected and ran in producer mode, it can call publish_message() at any time to send the message to RMQ.\nNow, let\u0026rsquo;s continue add the following functions for enabling run as consumer.\nasync def start_consumer(self): await self.start_connection() await self.channel.set_qos(prefetch_count=1) logger.info(\u0026#34;Starting consumer\u0026#34;) await self.queue.consume(self.handle_message) logger.info(\u0026#34;Consumer has been started\u0026#34;) return self async def handle_message(self, message: aio_pika.abc.AbstractIncomingMessage): # simulating long process await asyncio.sleep(10) logger.info(\u0026#34;Consumer: Got message from producer: %s\u0026#34; % message.body.decode()) await message.ack() Here, we need to set few things up at the initial connection.\nThe channel.set_qos() define how many job allowed to be executed concurrently. The queue.consume(callback) trigger the consumer to subscribe/listen to the predefined queue, waiting for any new message.\nThe callback function (in this case handle_message()) is executed rightaway the message arrived in RMQ. Note that we simulating the long background process using asyncio.sleep(10). Later you will see that during long process simulation, the user still able to navigate through our endpoints without getting blcoked.\nFinally, message.ack() is used to mark that the message is received successfully.\nNow, let\u0026rsquo;s move to main.py and setup our FastAPI application.\nfrom fastapi import FastAPI, Request, Response import logging from rmq import PikaClient logging.basicConfig(level=logging.INFO, format=\u0026#34;%(asctime)s %(levelname)s: %(name)s - %(message)s\u0026#34;) logger = logging.getLogger(__name__) app = FastAPI() @app.on_event(\u0026#34;startup\u0026#34;) async def start_rmq(): pass @app.on_event(\u0026#34;shutdown\u0026#34;) async def shutdown_rmq(): pass @app.get(\u0026#34;/\u0026#34;) def root(response: Response): response.status_code = 200 logger.info(\u0026#34;hit root endpoint\u0026#34;) return {\u0026#34;status_code\u0026#34;: 200, \u0026#34;message\u0026#34;: \u0026#34;Hello!\u0026#34;} @app.get(\u0026#34;/send-message\u0026#34;) async def send_message(request: Request, response: Response): pass Here, we will have 2 endpoints for demonstrating our background jobs and regular endpoint for which user or another service can interact with our system. We also have both startup and shutdown FastAPI event which we will use it to start and stop our RMQ producer and consumer.\nLet\u0026rsquo;s start with setup the producer.\n@app.on_event(\u0026#34;startup\u0026#34;) async def start_rmq(): # start producer app.rmq_producer = PikaClient(queue_name=\u0026#34;test.queue\u0026#34;, exchange_name=\u0026#34;test.exchange\u0026#34;, conn_str=\u0026#34;amqp://root:root@127.0.0.1:5672\u0026#34;) await app.rmq_producer.start_producer() @app.on_event(\u0026#34;shutdown\u0026#34;) async def shutdown_rmq(): await app.rmq_producer.disconnect() Quite simple, once the application start it will create a PikaClient object and call start_producer() to run it as producer. And once the application is stopped it will call the disconnect() to stop the producer.\nNow, let\u0026rsquo;s implement the similar thing to consumer with slight difference. We will run the consumer in a different thread so it won\u0026rsquo;t block the thread that FastAPI used for serving endpoint routes.\ndef start_background_loop(loop: asyncio.AbstractEventLoop) -\u0026gt; None: # inspired from https://gist.github.com/dmfigol/3e7d5b84a16d076df02baa9f53271058 asyncio.set_event_loop(loop) loop.run_forever() @app.on_event(\u0026#34;startup\u0026#34;) async def start_rmq(): # start producer app.rmq_producer = PikaClient(queue_name=\u0026#34;test.queue\u0026#34;, exchange_name=\u0026#34;test.exchange\u0026#34;, conn_str=\u0026#34;amqp://root:root@127.0.0.1:5672\u0026#34;) await app.rmq_producer.start_producer() # start consumer in other thread app.rmq_consumer = PikaClient(queue_name=\u0026#34;test.queue\u0026#34;, exchange_name=\u0026#34;test.exchange\u0026#34;, conn_str=\u0026#34;amqp://root:root@127.0.0.1:5672\u0026#34;) app.consumer_loop = asyncio.new_event_loop() tloop = threading.Thread(target=start_background_loop, args=(app.consumer_loop,), daemon=True) tloop.start() _ = asyncio.run_coroutine_threadsafe(app.rmq_consumer.start_consumer(), app.consumer_loop) @app.on_event(\u0026#34;shutdown\u0026#34;) async def shutdown_rmq(): await app.rmq_producer.disconnect() await app.rmq_consumer.disconnect() app.consumer_loop.stop() Here, inside the startup event we initiated a new asyncio event loop. Then, we trigger that loop to be run from another thread. We then delegate the start_consumer() calls to the loop, so the consumer will executed in the newly created thread. Finally, inside the shutdown event, we also call disconnect() to stop consumer RMQ from subscribing the queue. And we also trigger our created asyncio event loop to stop.\nThe final codes would be like this.\nrmq.py\nimport logging import aio_pika import asyncio logger = logging.getLogger(__name__) class PikaClient(): def __init__(self, queue_name: str, exchange_name: str, conn_str: str) -\u0026gt; None: self.queue_name = queue_name self.exchange_name = exchange_name self.conn_str = conn_str self.connection = None self.channel = None self.exchange = None self.queue = None async def start_connection(self): logger.info(\u0026#34;Starting a new connection\u0026#34;) self.connection = await aio_pika.connect_robust(url=self.conn_str) logger.info(\u0026#34;Opening a new channel\u0026#34;) self.channel = await self.connection.channel() logger.info(\u0026#34;Declaring an exchange: %s\u0026#34; % self.exchange_name) self.exchange = await self.channel.declare_exchange(name=self.exchange_name, type=aio_pika.ExchangeType.DIRECT) await self.setup_queue() async def setup_queue(self): logger.info(\u0026#34;Setup a queue: %s\u0026#34; % self.queue_name) self.queue = await self.channel.declare_queue(name=self.queue_name) logger.info(\u0026#34;Bind queue to exchange\u0026#34;) await self.queue.bind(self.exchange) async def start_producer(self): await self.start_connection() logger.info(\u0026#34;Producer has been started\u0026#34;) return self async def publish_message(self, message): await self.exchange.publish( aio_pika.Message(body=message.encode()), routing_key=self.queue_name ) async def start_consumer(self): await self.start_connection() await self.channel.set_qos(prefetch_count=1) logger.info(\u0026#34;Starting consumer\u0026#34;) await self.queue.consume(self.handle_message) logger.info(\u0026#34;Consumer has been started\u0026#34;) return self async def handle_message(self, message: aio_pika.abc.AbstractIncomingMessage): # simulating long process await asyncio.sleep(10) logger.info(\u0026#34;Consumer: Got message from producer: %s\u0026#34; % message.body.decode()) await message.ack() async def disconnect(self): try: if not self.connection.is_closed: await self.connection.close() except Exception as _e: logger.error(_e) main.py\nfrom fastapi import FastAPI, Request, Response import asyncio import logging from rmq import PikaClient import threading logging.basicConfig(level=logging.INFO, format=\u0026#34;%(asctime)s %(levelname)s: %(name)s - %(message)s\u0026#34;) logger = logging.getLogger(__name__) app = FastAPI() def start_background_loop(loop: asyncio.AbstractEventLoop) -\u0026gt; None: # inspired from https://gist.github.com/dmfigol/3e7d5b84a16d076df02baa9f53271058 asyncio.set_event_loop(loop) loop.run_forever() @app.on_event(\u0026#34;startup\u0026#34;) async def start_rmq(): # start producer app.rmq_producer = PikaClient(queue_name=\u0026#34;test.queue\u0026#34;, exchange_name=\u0026#34;test.exchange\u0026#34;, conn_str=\u0026#34;amqp://root:root@127.0.0.1:5672\u0026#34;) await app.rmq_producer.start_producer() # start consumer in other thread app.rmq_consumer = PikaClient(queue_name=\u0026#34;test.queue\u0026#34;, exchange_name=\u0026#34;test.exchange\u0026#34;, conn_str=\u0026#34;amqp://root:root@127.0.0.1:5672\u0026#34;) app.consumer_loop = asyncio.new_event_loop() tloop = threading.Thread(target=start_background_loop, args=(app.consumer_loop,), daemon=True) tloop.start() _ = asyncio.run_coroutine_threadsafe(app.rmq_consumer.start_consumer(), app.consumer_loop) @app.on_event(\u0026#34;shutdown\u0026#34;) async def shutdown_rmq(): await app.rmq_producer.disconnect() await app.rmq_consumer.disconnect() app.consumer_loop.stop() @app.get(\u0026#34;/\u0026#34;) def root(response: Response): response.status_code = 200 logger.info(\u0026#34;hit root endpoint\u0026#34;) return {\u0026#34;status_code\u0026#34;: 200, \u0026#34;message\u0026#34;: \u0026#34;Hello!\u0026#34;} @app.get(\u0026#34;/send-message\u0026#34;) async def send_message(request: Request, response: Response): message = \u0026#34;Hello from RMQ producer!\u0026#34; response.status_code = 202 logger.info(\u0026#34;message sent\u0026#34;) await request.app.rmq_producer.publish_message(message) return {\u0026#34;status_code\u0026#34;: 202, \u0026#34;message\u0026#34;: \u0026#34;Your message has been sent.\u0026#34;} Running the Application When we start the server, shortly we will notice both producer and consumer has started. Now, lets try to hit the long process endpoint following by several hits to the ordinary endpoint. Let\u0026rsquo;s try several more then observe our RMQ dashboard. And we can see there are some messages incoming and being processed.\nConclusion Now we know how to integrate both RMQ based publisher and subscriber within single FastAPI app.\nIf you have any inquiries, comments, suggestions, or critics please don’t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn Until next time! 👋\nReferences https://www.rabbitmq.com/tutorials/tutorial-one-python.html https://aio-pika.readthedocs.io/en/latest/quick-start.html#asynchronous-message-processing RabbitMQ publisher and consumer with FastAPI by IT racer Gist python asyncio event loop in a separated thread by dmfigol ","permalink":"http://localhost:1313/hugo-PaperMod/posts/pain-free-python-fastapi-rmq-integration/","summary":"Despite of the powerfulness of FastAPI, I found it\u0026rsquo;s not easy to work with threads and RabbitMQ. Here I\u0026rsquo;ll share my findings on creating both RMQ based producer and consumer in single FastAPI app.","title":"Pain-free Python Fastapi RabbitMQ Integration"},{"content":" This is the next series of my notes while exploring autoencoders. You may also interested to read my other notes on Autoencoder network:\n1. Autoencoders: Your First Step into Generative AI\n2. Restore your Noisy Image using Autoencoders\nConvolutional Autoencoder (CAE) In the previous article, we implemented the image compression model using Linear Autoencoder. However, when it comes to larger image and having more colors (RGB, not grayscale or just black and white) it is a good idea to try incorporating Convolutional layer instead of simple Linear layer. You may do your mini research by yourself, but this time let’s just modify our network using Convolutional layer.\nBefore we deep dive to code, let’s cover some basic theory so we can completely understand what is happening in our model.\nConvolutional Network ","permalink":"http://localhost:1313/hugo-PaperMod/posts/restore-your-noisy-image-using-autoencoders/","summary":"Autoencoder network designed to learn data representation using its bottleneck network architecture. Now, we will discover autoencoder use case for image restoration.","title":"Restore your Noisy Image Using Autoencoders"},{"content":"Generative AI When we discuss about generative models, most of us might be quickly triggered to imagine the greatness of current Large Language Models (LLMs) such as ChatGPT, Bard, Gemini, LLaMA, Mixtral, etc. Or instead, the Text-to-Image models like Dall-e and stable diffusion.\nBasically, the generative model works by trying to produce or generate new data that similar into the sampled one. Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one.\nAutoencoder Network Autoencoder is a neural network architecture that learn representation (encode) the input data, then tries to reconstruct the original data as close as possible by leveraging the learned representation (encoded) data. It is useful for denoising image, reduce dimensionality, or detecting outlier.\nThis network mainly consist of three parts: Encoder network, Decoder network, and Latent Space; each of them play an important role to make the model works. Let’s breakdown each of them below.\nEncoder Network Encoder network is responsible to take an input data, then compress it into smaller representation of data (latent space). The layer size of this network usually shrinks as it closer to the last layer that produce the latent space. It is inline with our human intuition where compress means to make things smaller, and in this case, it is works by passing the input data to hidden layers which shrinks at each stage.\nLatent Space Latent space is the encoder outputs, which we can also call it learned representation. It is actually the collection of vectors just like the input data, but in smaller dimension. In my opinion, the latent space has smaller dimension as it is only keep the important parts of the input data. These important parts were selected and evaluated in every step of forward propagation in the encoder network, hence producing such smaller representation.\nThis smaller representation later will be consumed as input for the decoder network.\nDecoder Network In contrast to the encoder network, decoder network is responsible to reconstruct (decode) the learned representation to be as close as possible with the original input data. Instead of using the original input data, this network use the latent space as its input. It also means the decoder network forced to generate new data based on those representation (which hopefully representing the input data enough). Finally, in each training step the generated data will be compared to the original one until it is already close enough.\nLoss Function As we mentioned earlier, the generated data needs to be compared with the original one to ensure it closeness. To do that, we need to define specific loss function which can be different for each field (e.g. image, text, audio). In this case, we will use image data for our discussion.\nNow, to measure the closeness between generated and original images, we can employ MSE Loss below.\n$$ MSE = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 $$\nwhere:\n$N:$ is the total samples\n$y_i:$ is the original input data\n$\\hat{y_i}:$ is the reconstructed data\nApplication of Autoencoder Dimensionality Reduction: similar to PCA (Principal Component Analysis) which useful for visualization purpose. Feature Extraction: generate the compressed latent representation to be used for the downstream tasks such as classification or prediction. The base model of BERT, GPT2, and its family is the examples of this application. File Compression: by reducing the data dimensionality we are able to reconstruct data with smaller size (but also risk some data quality). Image Denoising: remove the noise of the image that might be produced by high ISO or corrupted image. In order to do that, we must train the model to learn the difference between the clean image and the noisy one. Once trained, it expected able to reconstruct image with less noise. Image Generator: able to generate or modify images that are similar to input data. It used the variant of autoencoder named Variational Autoencoder (VAE). It is also useful for data augmentation. Coding Time! In this article, we will try to make our very first autoencoder. We will start with the simple one using Linear layers to compress the EMNIST letter images.\nIf you are prefer to jump ahead into the notebook, please visit this link.\nDataset Preparation First of all, let’s import and create config for our training purpose.\n# data manipulation import numpy as np import matplotlib.pyplot as plt import pandas as pd # utils import os import gzip import string from tqdm.auto import tqdm import time # pytorch import torch from torch.utils.data import Dataset, DataLoader from torch import nn import torch.nn.functional as F from torchvision.transforms import transforms from torchvision import datasets class config: batch_size = 512 device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; epochs = 20 learning_rate = 1e-3 log_step = 100 seed=42 latent_dim = 32 inp_out_dim = 784 hidden_dim = 128 When I explored the dataset, I found the original images were flipped and rotated like this.\nTherefore, we need to fix it into correct direction. Fortunately, torchvision has very helpful utilities to perform data transformation.\n# transform data transform = transforms.Compose([ transforms.RandomHorizontalFlip(1), # we need to flip and rotate cz the transforms.RandomRotation((90,90)), # original img was flip and rotated. transforms.ToTensor() ]) There, we define 3 transformations.\nRandomHorizontalFlip: flip image horizontally. This function require a parameter of probability of an image being flipped. Here we need all images to be flipped, therefore we fixed the probability to 1. RandomRotation: rotate image by angle. This function require sequence number of angles, if we put only single number it will assume the rotation degree ranging from (-degrees, +degrees). Since we need all images to be rotated in the same direction, we fixed the degrees by feeding sequences (tuple) of 90. ToTensor: convert images to tensor and scale it to range (0, 1) at the same time. Now, lets download EMNIST data and put our defined transformation here. Don’t forget to set the splits into letters as we want to reconstruct letters data instead of numbers.\n# load EMNIST data train_data = datasets.EMNIST(root=\u0026#39;data\u0026#39;, train=True, download=True, transform=transform, split=\u0026#39;letters\u0026#39;) test_data = datasets.EMNIST(root=\u0026#39;data\u0026#39;, train=False, download=True, transform=transform, split=\u0026#39;letters\u0026#39;) With the transformation applied, now if we interpret our data it will be in the correct direction.\nNext, let’s prepare our dataloaders!\n# setup the dataloaders trainloader = DataLoader(train_data, shuffle=True, batch_size=config.batch_size) testloader = DataLoader(test_data, shuffle=True, batch_size=config.batch_size) Designing Model Architecture Now, we are close enough to the fun part. But before that, let’s build our model architecture first. Here we will use Linear Layer for both our Encoder and Decoder networks. Remember that our data is scaled within range (0,1). Therefore, we should put Sigmoid layer in the very last part of Decoder network.\nclass LinearAutoencoder(nn.Module): def __init__(self, inp_out_dim, hidden_dim, latent_dim): super(LinearAutoencoder, self).__init__() # encoder layer to latent space self.encoder = nn.Sequential( nn.Linear(inp_out_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, latent_dim) ) # latent space to decoder layer self.decoder = nn.Sequential( nn.Linear(latent_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, inp_out_dim), nn.Sigmoid() # we use sigmoid cz the input and output should be in range 0,1 ) def forward(self, x): # pass input to encoder and activate it with ReLU x = self.encoder(x) # pass latent space to decoder and scale it with Sigmoid x = self.decoder(x) return x Let’s define our model and interpret its architecture.\n# define model model = LinearAutoencoder(inp_out_dim=config.inp_out_dim, hidden_dim=config.hidden_dim, latent_dim=config.latent_dim) # move to GPU device model = model.to(config.device) print(model) # our model architecture LinearAutoencoder( (encoder): Sequential( (0): Linear(in_features=784, out_features=128, bias=True) (1): ReLU() (2): Linear(in_features=128, out_features=32, bias=True) ) (decoder): Sequential( (0): Linear(in_features=32, out_features=128, bias=True) (1): ReLU() (2): Linear(in_features=128, out_features=784, bias=True) (3): Sigmoid() ) ) Take a look for a while in our model architecture.\nInitially our model will accept the input image within the shape of (batch_size, 784). For those who are wondering why is it 784 and not other value, well, it is actually obtained from 28 * 28 which our original image size.\nA little explanation…\nFor better intuition, let me break down a little bit for this part.\nBy default, our data is arranged with the following shape format (batch_size, color_channel, height, width). If you take a batch from our trainloader, you will observe that our dataset having shape like this.\ntorch.Size([512, 1, 28, 28])\nThen, we need to flatten it into 2-d array. So, later we will have dataset within shape (512, 784) which then will fed to our model.\nNow, back to our model architecture.\nInstead of having single Linear Layer, we stack it with another hidden layer on each Encoder and Decoder network. You may try to modify the dimension of hidden layer by changing hidden_dim value in config.\nThen, from hidden layer, we produce a latent representation within size dimension of 32. You also may modify it by changing latent_dim in config. Finally, the latent space will be act as the input of Decoder network.\n💡 Note that the Encoder network should be shrinking to its end and the opposite for the Decoder network as our objective is to compress the images.\nTraining Model And we arrived to the most interesting part. Here we define our loss function (criterion), optimizer, and training function.\n# loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate) # logging history = { \u0026#39;train_loss\u0026#39;: [] } # progressbar num_train_steps = len(trainloader) * config.epochs progressbar = tqdm(range(num_train_steps)) # training function epochtime = time.time() for epoch in range(config.epochs): trainloss = 0 batchtime = time.time() for idx, batch in enumerate(trainloader): # unpack data features, _ = batch features = features.to(config.device) # reshape input data into (batch_size x 784) features = features.view(features.size(0), -1) # clear gradient optimizer.zero_grad() # forward pass output = model(features) # calculate loss loss = criterion(output, features) loss.backward() # optimize optimizer.step() # update running training loss trainloss += loss.item() # update progressbar progressbar.update(1) progressbar.set_postfix_str(f\u0026#34;Loss: {loss.item()}:.3f\u0026#34;) # log step if idx % config.log_step == 0: print(\u0026#34;Epoch: %03d/%03d | Batch: %04d/%04d | Loss: %.4f\u0026#34; \\ % ((epoch+1), config.epochs, idx, \\ len(trainloader), trainloss / (idx + 1))) # log epoch history[\u0026#39;train_loss\u0026#39;].append(trainloss / len(trainloader)) print(\u0026#34;***Epoch: %03d/%03d | Loss: %.3f\u0026#34; \\ % ((epoch+1), config.epochs, loss.item())) # log time print(\u0026#39;Time elapsed: %.2f min\u0026#39; % ((time.time() - batchtime) / 60)) print(\u0026#39;Total Training Time: %.2f min\u0026#39; % ((time.time() - epochtime) / 60)) Here we will train for 20 epochs in total, and we log our model performances to console for every 100 training steps.\nAdditionally, we can also plot our training history to get better understanding on model performance.\nplt.figure(figsize=(5, 7)) plt.plot(range(len(history[\u0026#39;train_loss\u0026#39;])), history[\u0026#39;train_loss\u0026#39;], label=\u0026#39;Train Loss\u0026#39;) plt.xlabel(\u0026#39;Epochs\u0026#39;) plt.ylabel(\u0026#39;MSE Loss\u0026#39;) plt.legend() plt.show() After training for several epochs, we then evaluate it on our test set. Don’t forget to turn off the gradient by putting torch.no_grad() during evaluation since we don’t need any backpropagation process.\n# evaluate model testloss = 0 testtime = time.time() for batch in tqdm(testloader): # unpack data test_feats, _ = batch # reshape image test_feats = test_feats.view(test_feats.size(0), -1).to(config.device) # forward pass with torch.no_grad(): test_out = model(test_feats) # compute loss loss = criterion(test_out, test_feats) testloss += loss.item() print(\u0026#39;Test Loss: %.4f\u0026#39; % (testloss / len(testloader))) print(\u0026#39;Total Testing Time: %.2f min\u0026#39; % ((time.time() - testtime) / 60)) Inference It’s time to use our human intuition to see how good our model compression’s result. Let’s take a batch from the test set and compress it with our model.\n# obtain one batch of test images test_feats, test_labels = next(iter(testloader)) original_img = test_feats.numpy() # reshape image test_feats = test_feats.view(test_feats.size(0), -1).to(config.device) # forward pass with torch.no_grad(): infer_output = model(test_feats).detach().cpu() # resize outputs back to batch of images reconstructed_img = infer_output.view(config.batch_size, 1, 28, 28).numpy() Finally, we will compare both original data and the compressed one.\n# plot the first ten input images and the reconstructed images fig, axes = plt.subplots(2, 10, sharex=True, sharey=True, figsize=(25, 4)) # input images on top, reconstruction on bottom for idx, (images, row) in enumerate(zip([original_img, reconstructed_img], axes)): for img, lbl, ax in zip(images, test_labels, row): ax.imshow(img.squeeze(), cmap=plt.cm.binary) if idx == 0: ax.set_title(f\u0026#34;Label: {alphabets[lbl-1]}\u0026#34;) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) Save Model Lastly, if we are satisfied already with our model performance, we can save it. So we can use it anytime later without needing to run through all the codes above.\ntorch.save(model.state_dict(), \u0026#39;emnist-linear-autoencoder.pt\u0026#39;) Conclusion So we already discussed the Autoencoder network which also a family of Generative AI. It consists of 3 main parts: Encoder network, Decoder network, and the Latent representation. We also covered the implementation of Autoencoder using simple stacks of Linear Layer.\nAlthough simple network, our model performs quite good on test set and able to compress and reconstruct letter images.\nIf you have any inquiries, comments, suggestions, or critics please don’t hesitate to reach me out:\nMail: affahrizain@gmail.com LinkedIn: https://www.linkedin.com/in/fahrizainn/ GitHub: https://github.com/fhrzn Cheers! 🥂\nReferences https://www.analyticsvidhya.com/blog/2021/06/autoencoders-a-gentle-introduction/ https://structilmy.com/blog/2020/03/17/pengenalan-autoencoder-neural-network-untuk-kompresi-data/ https://medium.com/@samuelsena/pengenalan-deep-learning-part-6-deep-autoencoder-40d79e9c7866 https://deepai.org/machine-learning-glossary-and-terms/autoencoder https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder/linear-autoencoder https://www.nist.gov/itl/products-and-services/emnist-dataset https://www.youtube.com/watch?v=345wRyqKkQ0\u0026amp;list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51\u0026amp;index=138 https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L16 ","permalink":"http://localhost:1313/hugo-PaperMod/posts/autoencoders-your-first-step-into-generative-ai/","summary":"Generally, there are two popular basic variant of Generative AI: Autoencoders network and Generative Adversarial Network (GAN). In this series, we will discover the former one and leave the latter in another one.","title":"Autoencoders: Your First Step into Generative AI"},{"content":"Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration!\nBackground As self-attention based model like Transformers has successfully become a standard in NLP area, it triggers researchers to adapt attention-based models in Computer Vision too. There were different evidences, such as combine CNN with self-attention and completely replace Convolutions. While this selected paper belongs to the latter aproach.\nThe application of attention mechanism in images requires each pixel attends to every other pixel, which indeed requires expensive computation. Hence, several techniques have been applied such as self-attention only in local neighborhoods [1], using local multihead dot product self-attention blocks to completely replace convolutions [2][3][4], postprocessing CNN outputs using self- attention [5][6], etc. Although shown promising results, these techniques quite hard to be scaled and requires complex engineering to be implemented efficiently on hardware accelerators.\nOn the other hand, Transformers model is based on MLP networks, it has more computational efficiency and scalability, making its possible to train big models with over 100B parameters.\nMethods General architecture of ViT. Taken from the original paper (Dosovitskiy et al., 2021)\nThe original Transformers model treat its input as sequences which very different approach with CNN, hence the inputted images need to be extracted into fixed-size patches and flattened. Similar to BERT [CLS] token, the so-called classification token will be added into the beginning of the sequences, which will serve as image representation and later will be fed into classification head. Finally, to retain the positional information of the sequences, positional embedding will be added to each patch.\nThe authors designed model following the original Transformers as close as possible. The proposed model then called as Vision Transfomers (ViT).\nExperiments The authors released 3 variants of ViT; ViT-Base, ViT-Large, and ViT-Huge with different number of layers, hidden layers, MLP size, attention heads, and number of params. All of these are pretrained on large dataset such as ImageNet, ImageNet-21k, and JFT.\nIn the original paper, the author compared ViT with ResNet based models like BiT. The result shows ViT outperform ResNet based models while taking less computational resources to pretrain.\nThe following section will become technical part where we will use 🤗 Huggingface implementation of ViT to finetune our selected dataset.\n🤗 Huggingface in Action Now, let’s do interesting part. Here we will finetune ViT-Base using Shoe vs Sandal vs Boot dataset publicly available in Kaggle and examine its performance.\nFirst, lets load the dataset using 🤗 Datasets.\nfrom torch.utils.data import DataLoader from datasets import load_datasetdatasets = load_dataset(\u0026#39;imagefolder\u0026#39;, data_dir=\u0026#39;../input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset\u0026#39;)datasets_split = datasets[\u0026#39;train\u0026#39;].train_test_split(test_size=.2, seed=42) datasets[\u0026#39;train\u0026#39;] = datasets_split[\u0026#39;train\u0026#39;] datasets[\u0026#39;validation\u0026#39;] = datasets_split[\u0026#39;test\u0026#39;] Lets examine some of our dataset\n# plot samples samples = datasets[\u0026#39;train\u0026#39;].select(range(6)) pointer = 0 fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(10,6))for i in range(2): for j in range(3): ax[i,j].imshow(samples[pointer][\u0026#39;image\u0026#39;]) ax[i,j].set_title(f\u0026#34;{labels[samples[pointer][\u0026#39;label\u0026#39;]]} ({samples[pointer][\u0026#39;label\u0026#39;]})\u0026#34;) ax[i,j].axis(\u0026#39;off\u0026#39;) pointer+=1 plt.show() Few of our dataset looks like\nNext, as we already know, we need to transform our images into fixed-size patches and flatten it. We also need to add positional encoding and the classification token. Here we will use 🤗 Huggingface Feature Extractor module which do all mechanism for us!\nThis Feature Extractor is just like Tokenizer in NLP. Let’s now import the pretrained ViT and use it as Feature Extractor, then we will examine the outputs of processed image. Here we will use pretrained ViT with patch_size=16 and pretrained on ImageNet21K dataset with resolution 224x224.\nmodel_ckpt = \u0026#39;google/vit-base-patch16-224-in21k\u0026#39; device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) extractor = ViTFeatureExtractor.from_pretrained(model_ckpt)extractor(samples[0][\u0026#39;image\u0026#39;], return_tensors=\u0026#39;pt\u0026#39;) Our extracted features looks like\nNote that our original image has white background, that’s why our extracted features having a lot of 1. value. Don’t worry, its normal, everything will be work :)\nLet’s proceed to the next step. Now we want to implement this feature extractor to the whole of our dataset. Generally, we could use .map() function from 🤗 Huggingface, but in this case it would be slow and time consuming. Instead, we will use .with_transform() function which will do transformation on the fly!\ndef batch_transform(examples): # take a list of PIL images and turn into pixel values inputs = extractor([x for x in examples[\u0026#39;image\u0026#39;]], return_tensors=\u0026#39;pt\u0026#39;) # add the labels in inputs[\u0026#39;label\u0026#39;] = examples[\u0026#39;label\u0026#39;] return inputstransformed_data = datasets.with_transform(batch_transform) OK, so far we’re good. Next, let’s define our data collator function and evaluation metrics.\n# data collator def collate_fn(examples): return { \u0026#39;pixel_values\u0026#39;: torch.stack([x[\u0026#39;pixel_values\u0026#39;] for x in examples]), \u0026#39;labels\u0026#39;: torch.tensor([x[\u0026#39;label\u0026#39;] for x in examples]) }# metrics metric = load_metric(\u0026#39;accuracy\u0026#39;) def compute_metrics(p): labels = p.label_ids preds = p.predictions.argmax(-1) acc = accuracy_score(labels, preds) f1 = f1_score(labels, preds, average=\u0026#39;weighted\u0026#39;) return { \u0026#39;accuracy\u0026#39;: acc, \u0026#39;f1\u0026#39;: f1 } Now, let’s load the model. Remember that we have 3 labels in our data, and we attach it as our model parameters, so we will have ViT with classification head output of 3.\nmodel = ViTForImageClassification.from_pretrained( model_ckpt, num_labels=len(labels), id2label={str(i): c for i, c in enumerate(labels)}, label2id={c: str(i) for i, c in enumerate(labels)} ) model = model.to(device) Let’s have some fun before we finetune our model! (This step is optional, if you want to jump into fine-tuning step, you can skip this section).\nI am quite interested to see ViT performance in zero-shot scenario. In case you are unfamiliar with zero-shot term, it just barely use pretrained model to predict our new images. Keep in mind that most of pretrained model are trained on large datasets, so in zero-shot scenario we want to take benefit from those large dataset for our model to identify features in another image that might haven’t see it before and then make a prediction. Let’s just see how it works in the code!\n# get our transformed dataset zero_loader = DataLoader(transformed_data[\u0026#39;test\u0026#39;], batch_size=16) zero_pred = []# zero-shot prediction for batch in tqdm(zero_loader): with torch.no_grad(): logits = model(batch[\u0026#39;pixel_values\u0026#39;].to(device)).logits pred = logits.argmax(-1).cpu().detach().tolist() zero_pred += [labels[i] for i in pred]zero_true = [labels[i] for i in datasets[\u0026#39;test\u0026#39;][\u0026#39;label\u0026#39;]]# plot confusion matrix cm = confusion_matrix(zero_true, zero_pred, labels=labels) disp = ConfusionMatrixDisplay(cm, display_labels=labels) disp.plot() plt.show()# metrics print(f\u0026#39;Acc: {accuracy_score(zero_true, zero_pred):.3f}\u0026#39;) print(f\u0026#39;F1: {f1_score(zero_true, zero_pred, average=\u0026#34;weighted\u0026#34;):.3f}\u0026#39;) In short, we put our transformed data in DataLoader which going to be transformed on the fly. Then, for every batch, we pass our transformed data into our pretrained model. Next, we take the logits only from the model output. Remember that we have classification head with number of output 3. So, for each inferred image we will have 3 logits score. Among these 3 score, we will take the maximum one and return its index using .argmax(). Finally, we plot our confusion matrix and print the accuracy and F1 score.\nViT confusion matrix on zero-shot scenario\nSurprisingly, we got a unsatisfied metrics score with Accuracy: 0.329 and F1-Score: 0.307. OK, next let’s fine-tune our model for 3 epochs and test the performance again. Here, I used Kaggle environment to train model.\nbatch_size = 16 logging_steps = len(transformed_data[\u0026#39;train\u0026#39;]) // batch_sizetraining_args = TrainingArguments( output_dir=\u0026#39;./kaggle/working/\u0026#39;, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, evaluation_strategy=\u0026#39;epoch\u0026#39;, save_strategy=\u0026#39;epoch\u0026#39;, num_train_epochs=3, fp16=True if torch.cuda.is_available() else False, logging_steps=logging_steps, learning_rate=1e-5, save_total_limit=2, remove_unused_columns=False, push_to_hub=False, load_best_model_at_end=True)trainer = Trainer( model=model, args=training_args, data_collator=collate_fn, compute_metrics=compute_metrics, train_dataset=transformed_data[\u0026#39;train\u0026#39;], eval_dataset=transformed_data[\u0026#39;validation\u0026#39;], tokenizer=extractor)trainer.train() The code above was responsible to train our model. Note that we used 🤗 Huggingface Trainer instead of write our own training loop. Next, lets examine our Loss, Accuracy, and F1 Score for each epochs. You can also specify WandB or Tensorboard in Trainer parameter report_to for better logging interface. (Honestly, here I am using wandb for logging purpose. But for simplicity, I skipped the explanation of wandb part)\nModel performances on each epochs\nImpressive, isn’t it? Our ViT model already got very high performance since the first epoch, and changing quite steadily! Finally, let’s test again on the test data and later we plot our model prediction on few of our test data.\n# inference on test data predictions = trainer.predict(transformed_data[\u0026#39;test\u0026#39;]) predictions.metrics # plot samples samples = datasets[\u0026#39;test\u0026#39;].select(range(6)) pointer = 0fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(10,6)) for i in range(2): for j in range(3): ax[i,j].imshow(samples[pointer][\u0026#39;image\u0026#39;]) ax[i,j].set_title(f\u0026#34;A: {labels[samples[pointer][\u0026#39;label\u0026#39;]]}nP: {labels[predictions.label_ids[pointer]]}\u0026#34;) ax[i,j].axis(\u0026#39;off\u0026#39;) pointer+=1plt.show() Here is our prediction scores on test data. Our finetuned model now has a very good performances compared to the one in zero-shot scenario. And among of 6 sampled test images, our model correctly predict all of them. Super! ✨\n{\u0026#39;test_loss\u0026#39;: 0.04060511291027069, \u0026#39;test_accuracy\u0026#39;: 0.994, \u0026#39;test_f1\u0026#39;: 0.9939998484491527, \u0026#39;test_runtime\u0026#39;: 30.7084, \u0026#39;test_samples_per_second\u0026#39;: 97.693, \u0026#39;test_steps_per_second\u0026#39;: 6.122} Prediction result\nConclusion Finally, we reached the end of the article. To recap, we did quick review of the original paper of Vision Transformers (ViT). We also perform zero-shot and finetuning scenario to our pretrained model using publicly available Kaggle Shoe vs Sandals vs Boots dataset containing ~15K images. We examined that ViT performance on zero-shot scenario wasn’t so good, while after finetuning the performance boost up since the first epoch and changing steadily.\nIf you found this article is useful, please don’t forget to clap and follow me for more Data Science / Machine Learning contents. Also, if you found something wrong or interesting, please feel free to drop it in the comment or reach me out at Twitter or Linkedin.\nIn case you are interested to read more, follow our medium Data Folks Indonesia and don’t forget join us Jakarata AI Research on Discord!\nFull codes are available on my Github repository, feel free to check it 🤗.\nNB: If you are looking for deeper explanation especially if you want to reproduce the paper by yourself, you can check this amazing article by Aman Arora.\nReferences Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., \u0026amp; Houlsby, N. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, 2018. Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In ICCV, 2019. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. In NeurIPS, 2019. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In CVPR, 2018. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. Let’s Get in Touch! Linkedin Twitter Github ","permalink":"http://localhost:1313/hugo-PaperMod/posts/exploring-visual-transformers-vit-with-huggingface/","summary":"Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration!","title":"Exploring Vision Transformers (ViT) with 🤗 Huggingface"},{"content":"Sometimes when we are applying for jobs, they ask us to send our portfolio. But instead of sharing your Jupyter Notebook — which they might won’t read it as it requires more effort to open — what if I tell you there are some way to simply export your beloved notebook to PDFs?\nHere we go!\nUsing online converter Pretty straightforward, of course!\nvertopal.com Site: vertopal.com\nI found this tool is very helpful and easy to use. It basically can convert picture, document, spreadsheet, presentation, and many more. Just go straight to their website vertopal.com, choose IPYNB as source input and PDF as target output. Then drop your file — or you can choose from Google Drive, Dropbox, or OneDrive but it will requires you to log in. Once it uploaded successfully, we can just click continue to convert. It might take a while, and finally we can download it as pdf version!\nPDF output generated byVertopal\nIt translate our markdown and notebook cells as well correctly! Super ✨\nUsing nbconvert Unsatisfied with the result? Don’t worry, nbconvert come to rescue!\nnbconvert\nInstall nbconvert, Pandoc, and LaTeX First of all, we need to install nbconvert package to our system using either pip or conda. Note that nbconvert use LaTeX to convert our notebook to pdf, so it is mandatory to install it as mentioned in Kessie’s Article.\nIn my findings, I need to install Pandoc and LaTeX. There are different OS-specific distribution for installing LaTeX, since I am using MacOS I need to install MacTeX (Windows user will need MikTex). If you are wondering am I on the right path downloading this large file — yes, you are on the right path. The file size is around 4.7GB that might take a while to download.\nOnce your download is complete, the next step is just to install it — as it format already in .pkg we can just double click on it — super easy!\nInstall template As mentioned in Kessie’s Article too, the default output of nbconvert cut off the side of the page. Therefore, we need to install additional template. Here are the steps I did:\nRun pip install``nb_pdf_template Create an empty folder named latex inside /usr/local/lib/python3.9/site-packages/nbconvert/templates. Run python -m nb_pdf_template.install Open latex folder and make sure there are 2 files named classic.tplx and classicm.tplx. Time to convert! Finally, run this command on your terminal.\njupyter nbconvert --to pdf \u0026lt;your\\_filename\\_here\u0026gt; PDF output generated by nbconvert\nIn my opinion, now our code is more readable than Vertopal’s pdf version — thanks to Tyler Makaro as creator of the nbconvert template. Personally, I like this version more than the first one. Super! ✨\nConclusion We covered 2 ways to convert Jupyter Notebook to PDFs. Each produce different styles of pdf. The advantages of using online converter is you dont have to install anything on your machine, but sometimes there are some limitations on the result. In the other hand, you can get more flexibility (sometimes) by installing some stuffs on your machine like nbconvert. Although, maybe some of you doesn’t think nbconvert is a good way as it will takes our 4.7GB space just for installing LaTeX. But yeah, the decision is on your hand)\nIf you found this article is useful, please don’t forget to clap and follow me for more Data Science / Machine Learning contents. In case you are interested to read more, follow our medium Data Folks Indonesia and don’t forget join us Jakarata AI Research on Discord!\nReference https://nbconvert.readthedocs.io/en/latest/ https://towardsdatascience.com/how-to-convert-jupyter-notebooks-into-pdf-5accaef3758 ","permalink":"http://localhost:1313/hugo-PaperMod/posts/quick-export-your-jupyter-notebook-to-pdf/","summary":"Sometimes when we are applying for jobs, they ask us to send our portfolio. But instead of sharing your Jupyter Notebook — which they might won’t read it as it requires more effort to open — what if I tell you there are some way to simply export your beloved notebook to PDFs?","title":"Quick Export your Jupyter Notebook to PDF"},{"content":" Ketika kita berhubungan dengan data teks seperti klasifikasi teks misalnya, kita tentunya harus melakukan transformasi data teks menjadi sekumpulan angka (vektor) terlebih dahulu sebelum melakukan modelling. Nah, 2 metode yang cukup populer diantaranya adalah Bag of Words dan TF-IDF. Mari kita bahas bagaimana mereka bekerja serta apa perbedaannya!\nThe Story Bayangkan saja kita adalah pemilik restoran. Setiap pengunjung selesai makan, kita meminta mereka untuk menuliskan review dari segi apapun sebagai bahan evaluasi restoran. Dan setiap akhir bulan kita melakukan evaluasi berdasarkan review pengunjung. Kebetulan, bulan ini kita mendapat 3 review yang isinya seperti berikut:\nReview 1: Makanan disini gurih dan enak!\nReview 2: Makanan disini biasa saja.\nReview 3: Makanan disini hambar dan tidak enak!\nSebagai pemilik restoran yang melek IT, kita ingin seluruh review nantinya diproses menggunakan komputer. Sayangnya oh sayangnya, komputer tidak mengerti bahasa manusia. Mereka hanya memahami angka. Oleh karena itu, kita perlu melakukan transformasi terhadap data kita dari teks menjadi sekumpulan angka yang biasa disebut vektor. Yuk, mari kita lakukan!\nBag of Words Bag of Words (BoW) merupakan salah satu metode paling sederhana dalam mengubah data teks menjadi vektor yang dapat dipahami oleh komputer. Metode ini sejatinya hanya menghitung frekuensi kemunculan kata pada seluruh dokumen.\nMari kita ingat kembali contoh yang sudah kita baca sebelumnya.\nReview 1: Makanan disini gurih dan enak!\nReview 2: Makanan disini biasa saja.\nReview 3: Makanan disini hambar dan tidak enak!\nPertama, kita abaikan tanda baca serta huruf kapital dari ketiga review tersebut. Kemudian kita bisa membentuk sebuah korpus / kamus kata seperti berikut.\n“makanan”\n“disini”\n“gurih”\n“dan”\n“enak”\n“biasa”\n“saja”\n“hambar”\n“tidak”\nPerlu diperhatikan sebelumnya, bahwa dalam membentuk korpus, kita hanya menghitung kata secara unik. Artinya, setiap kata yang berulang hanya akan ditulis sekali.\nBerikutnya, mari kita hitung frekuensi kemunculan kata di korpus tersebut kepada ketiga review sebelumnya. Kita beri nilai 1 jika kata tersebut muncul pada sebuah review dan 0 jika tidak muncul.\nAgar lebih mudah dalam memahminya, mari kita perhatikan tabel berikut. Perhitungan Bag of Words (BoW)\nDari tabel tersebut, akhirnya kita dapatkan vektor dari setiap review seperti berikut.\nVektor Review 1 = [1, 1, 1, 1, 1, 0, 0, 0, 0]\nVektor Review 2 = [1, 1, 0, 0, 0, 1, 1, 0, 0]\nVektor Review 3 = [1, 1, 0, 1, 1, 0, 0, 1, 1]\nItulah konsep dari Bag of Words, cukup mudah bukan? Namun, meski demikian metode ini ternyata memiliki beberapa kekurangan. Yuk mari kita ulas.\nKekurangan Bag of Words Ukuran korpus Bag of Words mengikuti jumlah kata unik dari seluruh dokumen. Artinya, jika nantinya terdapat berbagai kata unik baru maka ukuran korpus juga akan semakin membesar. Tentunya hal ini akan berpengaruh pada komputasi yang dibutuhkan pada saat kita melatih model machine learning. Seperti yang kita lihat pada tabel diatas, ada banyak angka 0 dalam vektor kita. Kondisi ini biasa juga disebut dengan sparse matrix. Hal tersebut harusnya kita hindari karena model harus menemukan informasi yang sedikit dalam ukuran data yang besar, yang tentunya juga akan membutuhkan proses komputasi lebih tinggi. Bag of Words menghilangkan konteks kalimat akibat tidak memperhatikan urutan kata. TF-IDF TF-IDF merupakan singkatan dari Term Frequency — Inverse Document Frequency. Sejatinya, TF-IDF merupakan gabungan dari 2 proses yaitu Term Frequency (TF) dan Inverse Document Frequency (IDF).\nTF-IDF biasa digunakan ketika kita ingin mengubah data teks menjadi vektor namun dengan memperhatikan apakah sebuah kata tersebut cukup informatif atau tidak. Mudahnya, TF-IDF membuat kata yang sering muncul memiliki nilai yang cenderung kecil, sedangkan untuk kata yang semakin jarang muncul akan memiliki nilai yang cenderung besar. Kata yang sering muncul disebut juga Stopwords biasanya dianggap kurang penting, salah satu contohnya adalah kata hubung (yang, di, akan, dengan, dll).\nSekarang, mari kita coba aplikasikan TF-IDF terhadap 3 review yang telah kita miliki sebelumnya.\nTerm Frequency (TF) Term Frequency (TF) menghitung frekuensi jumlah kemunculan kata pada sebuah dokumen. Karena panjang dari setiap dokumen bisa berbeda-beda, maka umumnya nilai TF ini dibagi dengan panjang dokumen (jumlah seluruh kata pada dokumen).\nRumus Term Frequency (TF)\nKeterangan\ntf = frekuensi kemunculan kata pada sebuah dokumen\nMari kita ambil contoh kalimat Review 1 untuk dihitung nilai TF nya.\nReview 1: Makanan disini gurih dan enak!\nKorpus = [“makanan”, “disini”, “gurih”, “dan”, “enak”] Panjang kalimat = 5 Sehingga perhitungan untuk nilai TF nya menjadi:\nTF(“makanan”) = 1/5 ≈ 0.2 TF(“disini”) = 1/5 ≈ 0.2 TF(“gurih”) = 1/5 ≈ 0.2 TF(“dan”) = 1/5 ≈ 0.2 TF(“enak”) = 1/5 ≈ 0.2 Berikutnya, mari kita coba terapkan pada seluruh review dan kita formulasikan ke dalam bentuk tabel seperti berikut.\nPerhitungan Term Frequency (TF)\nR1, R2, R3 merupakan notasi untuk setiap Review 1, Review 2, dan Review 3. Sedangkan TF1, TF2, TF3 merupakan notasi untuk nilai Term Frequency setiap Review.\nInverse Document Frequency (IDF) Setelah kita berhasil menghitung nilai Term Frequency, selanjutnya kita hitung nilai Inverse Document Frequency (IDF), yang merupakan nilai untuk mengukur seberapa penting sebuah kata. IDF akan menilai kata yang sering muncul sebagai kata yang kurang penting berdasarkan kemunculan kata tersebut pada seluruh dokumen. Semakin kecil nilai IDF maka akan dianggap semakin tidak penting kata tersebut, begitu pula sebaliknya.\nRumus Inverse Document Frequency (IDF)\nSetiap review yang diberikan oleh pelanggan merupakan sebuah dokumen. Karena pada tulisan ini kita mempunyai 3 review, maka artinya kita mempunyai 3 dokumen.\nMari kita coba hitung nilai IDF untuk masing-masing kata pada Review 1.\nReview 1: Makanan disini gurih dan enak!\nKorpus = [“makanan”, “disini”, “gurih”, “dan”, “enak”] Jumlah dokumen = 3 Sehingga perhitungan untuk nilai IDF nya menjadi:\nIDF(“makanan”) = $log(\\frac{3} {3})$ ≈ 0 IDF(“disini”) = $log(\\frac{3} {3})$ ≈ 0 IDF(“gurih”) = $log(\\frac{3} {1})$ ≈ 0.48 IDF(“dan”) = $log(\\frac{3} {2})$ ≈ 0.18 IDF(“enak”) = $log(\\frac{3} {2})$ ≈ 0.18 Sekarang, mari kita coba terapkan pada seluruh kata dan kita lengkapi tabel TF sebelumnya seperti berikut.\nPerhitungan Inverse Document Frequency (IDF)\nTerm Frequency — Inverse Document Frequency (TF-IDF) Setelah kita punya TF dan IDF, berikutnya kita bisa menghitung nilai TF-IDF yang merupakan hasil perkalian dari TF dan IDF.\nRumus TF-IDF\nKarena kita sudah memiliki nilai TF dan IDF untuk setiap kata, maka mari kita coba hitung nilai TF-IDF untuk setiap kata pada Review 1.\nReview 1: Makanan disini gurih dan enak!\nmakanan\nTF(“makanan”) = 1/5 ≈ 0.2 IDF(“makanan”) = $log(\\frac{3} {3})$ ≈ 0 TFIDF(“makanan”) = $0.2 \\times0=0$ disini\nTF(“disini”) = 1/5 ≈ 0.2 IDF(“disini”) = $log(\\frac{3} {3})$ ≈ 0 TFIDF(“disini”) = $0.2 \\times0=0$ gurih\nTF(“gurih”) = 1/5 ≈ 0.2 IDF(“gurih”) = $log(\\frac{3} {1})$ ≈ 0.48 TFIDF(“gurih”) = $0.2 \\times0.48=0.095$ dan\nTF(“dan”) = 1/5 ≈ 0.2 IDF(“dan”) = $log(\\frac{3} {2})$ ≈ 0.18 TFIDF(“makanan”) = $0.2 \\times0.18=0.035$ enak\nTF(“enak”) = 1/5 ≈ 0.2 IDF(“enak”) = $log(\\frac{3} {2})$ ≈ 0.18 TFIDF(“makanan”) = $0.2 \\times0.18=0.035$ Sekarang, mari kita coba lengkapi tabel sebelumnya dengan nilai TF-IDF pada seluruh kata seperti berikut.\nPerhitungan Term Frequency — Inverse Document Frequency (TF-IDF)\nNote: Mungkin untuk sebagian perhitungan, angkanya tidak presisi dikarenakan tools yang saya gunakan. Semoga bisa dimaklumi dan tetap bisa diambil konsepnya 🙂\nDari tabel tersebut, akhirnya kita dapatkan vektor dari setiap review yang dinotasikan oleh TFIDF1, TFIDF2, dan TFIDF3 seperti berikut.\nVektor Review 1 = [0, 0, 0.095, 0.035, 0.035, 0, 0, 0, 0]\nVektor Review 2 = [0, 0, 0, 0, 0, 0.119, 0.119, 0, 0]\nVektor Review 3 = [0, 0, 0, 0.0293, 0.0293, 0, 0, 0.080, 0.080]\nKekurangan TF-IDF TF-IDF sejatinya berdasar pada Bag of Words (BoW), sehingga TF-IDF pun tidak bisa menangkap posisi teks dan semantiknya. TF-IDF hanya berguna sebagai fitur di level leksikal. So, itulah perbedaan antara Bag of Words (BoW) dan TF-IDF sebagai metode untuk transformasi teks menjadi vektor. Jika ada pertanyaan, diskusi, sanggahan, kritik, maupun saran jangan pernah ragu untuk menuliskannya di kolom komentar 🙂\nSekian tulisan saya kali ini, mohon maaf apabila ada kekurangan dan salah kata, semoga bermanfaat. Terima kasih!\nYuk, belajar dan diskusi lebih lanjut tentang seputar Data Science, Artificial Intelligence, dan Machine Learning dengan gabung di discord Jakarta AI Research. Dan jangan lupa follow medium Data Folks Indonesia biar nggak ketinggalan update terbaru dari kami.\nReferensi https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/ https://machinelearningmastery.com/gentle-introduction-bag-words-model/ http://www.tfidf.com/ https://www.quora.com/What-are-the-advantages-and-disadvantages-of-TF-IDF ","permalink":"http://localhost:1313/hugo-PaperMod/posts/bag-of-words-vs-tf-idf-penjelasan-dan-perbedaannya/","summary":"Ketika kita berhubungan dengan data teks seperti klasifikasi teks misalnya, kita tentunya harus melakukan transformasi data teks menjadi sekumpulan angka (vektor) terlebih dahulu sebelum melakukan modelling. Nah, 2 metode yang cukup populer diantaranya adalah Bag of Words dan TF-IDF. Mari kita bahas bagaimana mereka bekerja serta apa perbedaannya!","title":"Bag of Words vs TF-IDF — Penjelasan dan Perbedaannya"},{"content":"","permalink":"http://localhost:1313/hugo-PaperMod/profile/","summary":"","title":"Profile"}]